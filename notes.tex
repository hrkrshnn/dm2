% Created 2019-02-16 Sat 18:08
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[left=1cm, right=1cm, bottom=2cm, top=1cm]{geometry}
\usepackage{parskip}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{amssymb}
\def\R{\mathbb{R}}
\def\Z{\mathbb{Z}}
\def\N{\mathbb{N}}
\def\max{\operatorname{max}}
\def\P{\textup{P}}
\def\NP{\textup{NP}}
\def\coNP{\textup{co-NP}}
\def\min{\operatorname{min}}
\def\dist{\operatorname{dist}}
\def\prev{\operatorname{prev}}
\def\val{\operatorname{val}}
\usepackage[T1]{fontenc}
\author{Hari}
\date{\today}
\title{Discrete Math II}
\hypersetup{
 pdfauthor={Hari},
 pdftitle={Discrete Math II},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.1 (Org mode 9.1.13)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents


\section{Lecture 2\textit{<2018-10-17 Wed>}}
\label{sec:orga48464c}
\subsection{Review}
\label{sec:org416422a}
\subsubsection{Random stuff about algorithms}
\label{sec:org5dea3c4}
Algorithm A

Input class I

\(T(A, I)\): time algorithm \(A\), input \(I\).

The worst case time complexity of \(A\) on \(I\) is \(\max_{I\in \mathbb{I}} (A, I) = T_(n)\).

The worst case complexity of \(I = \min_{A} T_A(n) = T_{I}(n)\). A function that grows with \(n\).

Average case complexity \(\mathbb{E}(T(A, I))\).
\subsubsection{Sorting}
\label{sec:orgc4a63b2}
Sorting algorithm \(A\), \(T(A, \pi) =\) number of comparisons that \(A\) makes to find \$\(\pi\).\$

Yesterday, we proved that for insertion sort (the stupid algorithm), the
worst case running time was \(\binom{n}{2}\).

Binary insertion:  \(T(n) \le n log_2{n}\).

Merges sort: \$T\(_{\text{M}}\)(n) =le n\(\log_{\text{2}}\)\{n\}.\$
\subsubsection{Theorem about lower-bound of sorting}
\label{sec:org0d31f69}
If \(A\) is a sorting algorithm (correctly sort \(n\) numbers)

Then \(T_A(n) \ge \log_2(n!)\).

In particular, using the stirling's formula, \(T_A(n) \ge n\log_n\).
\begin{enumerate}
\item Proof
\label{sec:orga8b03dc}
The algorithm runs on the permutations on the set of \(n\) numbers. Define
\(\forall \kappa \in \N\) blah blah.

\(S_n \subset S_n(\alpha, \kappa) =\) \(\{\pi \in S_n \colon \textup{where A
     receives input} \pi\) \(\textup{ there, it receives } \alpha \textup{ as the sequence of
     the list of first k answerers}\}\)
\item Observation
\label{sec:orgf973a25}
Everything is determined if we run it.

\(S^n_{\alpha, k} \cap S^n_{\beta, k} = \emptyset \forall \alpha \neq \beta\)

\(\cap S^{n}_{\alpha, k} = S_n\)

Suppose, \(T_A(n) < \log_2(n!)\).

\(S_n = \cap S^n_{\alpha, k}\) partition into \(2^k\) sets.

\(2^k < n!\) implies that there exist \(\beta\) such that there exist \(\pi_1
     \neq \pi_2 \in S_{\beta, r}\).

Run \(A\) on \(\pi_1\), we receive the answers \(\beta\) implies that \(A\) outputs
\(\pi^{*} \in S_n\)

\begin{itemize}
\item Case 1: \(\pi^* \neq \pi_1\), a contradiction to the correctness of \(A\).

\item Case 2: \(\pi = \pi_1\). Run \(A\) on \(\pi_2\) implies \(\beta\) is the answer.
\(A\) outputs \(\pi_{*}\), this is a contradiction that \(\pi_{*} \neq \pi_2\).
Contradicts the correctness of \(A\).

Called the \textbf{information theoretical lower bound}.
\end{itemize}
\end{enumerate}
\subsection{Graph algorithms}
\label{sec:org2c491e3}
\subsubsection{Connectedness of graph}
\label{sec:org4e4d11e}
A graph \(G\) is connected if for every two vertices \(u\) and \(v\), there exists
a \(uv\) path in \(G\).

This induces an \textbf{equivalence} relation where \(u\) is equivalent to \(v\) if
there exists a path that connects \(u\) and \(v\). Create equivalence classes,
which are called the \textbf{connected components} of \(G\).

Observation: There is no edge between different connected components.
\subsubsection{How do you decide whether a graph is connected?}
\label{sec:org9548d47}
Take a vertex \(v\). We maintain a list of all vertices that are visited. We
redo the same thing for every other vertex. We repeat this until we saw all
the vertices in the graph.

Algorithm (Comp (V))
\begin{verse}
Initialize: Queue Q = v; and W = empty\\
for all \(i \ge 1\)\\
step i: v\(_{\text{i}}\) first vertex in Queue.\\
remove v\(_{\text{i}}\) from the queue  and put it in W\\
put all of \(N(v_i) \setminus W\) into \(Q\).\\
\vspace*{1em}
IF Q = \(\empty\), STOP and return \(W\) as the connected component of \(v\).\\
else go to step i+1\\
\end{verse}
\subsubsection{Theorem: Comp(v) returns \(C_G(v)\)}
\label{sec:org17cd490}
Suppose \(u \in C_G(v) \cap W_{out}\).

Let \(P\) be a vu path in \(G\).

Comp(v) puts a vertex into \(w\) only if all its neighbours are put into \(Q\).
We stop only if \(Q\) is empty. Also \(v_f\) was in \(Q\) at some point \(A\). \(v_f\)
had to be moved to \(w\). This is a contradiction.

Other direction: \(u\in W_{out}\). Before \(u\) became part of \(W\), \(u\) was in
\(Q\). Why? Because there is a \(u_1 \in Q\), \(u \in N(u_1) \setminus W\).
(More things, I skipped.)
\subsubsection{Spanning tree}
\label{sec:org285f4e7}
Suppose we run Comp(v) on a connected graph, where a vertex \(w\) is put into
\(Q\), then there is a unique edge coming with it that attaches it to \(v\).
(the vertex that is moved from \(Q\) to \(W\) at the same time.)
\subsubsection{Theorem about spanning tree}
\label{sec:org2273f7b}
The following are equivalent: for an \(n\) vertex graph.

\begin{enumerate}
\item T is a tree (connected, acyclic.)
\item T is connected and has \(n-1\) edges.
\item T is acyclic and has \(n-1\) edges.
\item For every pair of vertices \(u\) and \(v\) in \(V(T)\), there is a unique \(uv\)
path.
\end{enumerate}
\begin{enumerate}
\item Definition (spanning tree)
\label{sec:org0da1782}
\(T \subset G\) is a spanning tree if \(T\) is a tree and \(V(T) = V(G)\).
\end{enumerate}
\subsubsection{Special spanning trees}
\label{sec:org3ba77af}
Let \(G\) be connected and run Comp(v) (don't forget the edges.)

\emph{What if} we always put \(N(v_i) \setminus W\) to the top of \(Q\). (We call
this the \textbf{depth first search} tree.) This is going to create a tree which is
long (?)

\emph{What if} if we put it to the bottom of the tree, this will create a
\textbf{breadth first search}. You will create which is short.

A diagram that I ignored.
\subsection{Minimal spanning tree}
\label{sec:org986a7ee}
Given a graph \(G\). (can be a complete or arbitrary graph.)

We have a weight function that is assumed on the edge set to \(\mathbb{R}\).
What we want is a spanning tree \(T\subset G\) such that the cost of the sum of
weights on the edges is minimum (i.e., for any other spanning tree, the sum
of the weights on the edges would be more than the current one.)
\subsubsection{Naive algorithm}
\label{sec:orgadbde87}
There is at most \(n^{n-2}\) (Cayley's theorem.) spanning trees on \(n\) vertices. Let's look at all
of them and calculate the weights and output the minimum.
\subsubsection{Kruskal's algorithm}
\label{sec:org7031cc2}
\begin{enumerate}
\item Step 1
\label{sec:orgc566f2e}
Sort edges in increasing order of weights \(e_1, \cdots e_m\) such that
\(w(e_1) \le w(e_n) \le \cdots, \le w(e_n)\).

Start with an empty forest \(E(F) \neq \emptyset\) for all \(v \in V\), \(c_v = v\).
\item Step 2
\label{sec:org4f27b17}
For each edge \(e_i = uv\). For \(\forall i \ge 1\), if the forest plus the new
edge has a cycle, then \(C_v\) remains the same.

If there is no cycle, we have a new forest, i.e., the bigger forest with
the extra edge added to it.
\item The end
\label{sec:orgad399e5}
Output \(F\).
\end{enumerate}
\subsubsection{Theorem: Kruskal's algorithm returns the min-weight spanning tree.}
\label{sec:orge8b1e65}
Proved in discrete Math 1.
\subsubsection{Running time of Kruskal}
\label{sec:org8ced960}
The first step involves sorting. This can be done in \(O(|E| \log|E|)\).

There is \(O(m)\) and \(O(n^2)\).

If \(G\) is dense, then \(O(m\log m)\) and if \(G\) is sparse, then \(O(n^2)\).
\section{Lecture 3 \textit{<2018-10-23 Tue>}}
\label{sec:org74155fe}
\subsection{Spanning trees}
\label{sec:org8fbe1f4}
Another perspective: get to one place to another in the fastest way possible.
Versus the minimum spanning tree. \footnote{MST would be city-side and the fastest possible way would be consumer side.}
\subsection{Problem}
\label{sec:org98ce372}
Given graph \(G=(V, E)\), a distance for \(d\colon E \rightarrow \mathbb{R}_{\ge 0}\).

\textbf{Goal}: Given a vertex \(u\in V\), find the shortest path to any vertex \(v \in V\).

The brute force way is to find all the path and find the minimu.
\subsection{Idea}
\label{sec:orgb145370}
Maintain a set of vertices to where a shortest path from \(u\) was found. And
in each step we add one vertex to \(W\).

\textbf{Key observation}: If \(P\) is a shortest \(uv\) path, then for every \(w\) on this
 path, \(P[u, w]\), this is also the shortest path. (\(P[u, w]\) represents the
 path from \(u\) to \(w\) through \(P\).)
\subsection{Dijkstra's algorithm}
\label{sec:org8cce0cc}
\textbf{Input} is a graph \(G = (V, E)\) which is connected. \footnote{Otherwise you can explore the components.} We have a distance
\(d\colon E \rightarrow \mathbb{R}_{\ge 0}\).

\textbf{Output}: For every vertex \(u \in V\), the distance from \(u\) and also a
shortest path.
\subsubsection{Algorithm}
\label{sec:org96d14bd}
\textbf{Initialization}: dist[u] = 0

For every other vertex \(v\), I set \(d[v] = \infty\). \(prev[v] =
    \textup{null}\). Maintain the set \(W = \emptyset\).

\textbf{Iteration}: Choose a vertex \(v_0 = \min\{\dist[v]\colon v  \in V \setminus W\}\)

Update \(W = W \cap \{v_0\}\).

\(\forall v \in V \setminus W\) if \(\dist[v] > \dist[v_0] + d(v_0, v)\)
then \(\dist[v] = \dist[v_0] + d(v_0, v)\) and \(\prev[v] = v_0\).

\textbf{Termination}: If \(W = V\), then STOP and output \(\dist[v]\) search head of
\(\prev\) for a \(uv\) path.

An example was done. \href{https://en.wikipedia.org/wiki/Dijkstra\%27s\_algorithm}{Wikipedia}.
\subsubsection{Analysis}
\label{sec:org4ecdc34}
\begin{enumerate}
\item Correctness
\label{sec:orgaf08902}
\textbf{Claim}: At the time \(v_0\) is put into \(W\), \(\dist[v_0]\) is the distance of
 \(v_0\) to \(u\).

(This would prove the correctness, because \(dist\) does not change after
vertex is in \(W\).)

Proof: Induction on \(\vert W\vert\).

Because \(\vert w \vert = 0\) \(u\) is put into \(W\), \(\dist[u] = 0 = d(uu)\).

Suppose \(\vert W \vert \ge 1\), we put \(v_0\) into \(W\). If this is the case,
then \(\dist[v_0] = \min\{\dist[v_0]\colon v \in V \setminus W\}\).

Suppose \(\dist[v_0] > s(uv_0)\). (here \(s\) is the shortest path going
from one vertex to another.)

Take the shortest \(uv_0\) path \(P\). There will be a first vertex on \(P\) not
in \(W\), call it \(v_f\) and \(v_p\) be its predecessor. \(\dist[v_0] > s(uv_0)
      = s(uv_f) + s(v_fv_0) \ge s(uv_f) = s(uv_p) + s(v_pv_f) = dist[v_p] +
      d(v_pv_f)\). (By our observation from before, both these paths are the
shortest.)

When we are updating after putting \(v_p\) into \(W\), we consider \(v_f\) and
we will put it in \(W\). This is a contradiction.
\item Termination
\label{sec:org81e9f79}
In each iterating step, one vertex is put into \(W\) and stays there and then
in \(n\) iterations, we are done.
\item Cost
\label{sec:org3392ed8}
Finding \(v_0\), then \(O(\vert V \vert)\).

Adding \(v_0\) to \(W\) is \(O(1)\)

Updating \(\dist\),  \(O(\vert V\vert)\).

With better data structure \(O(\vert E\vert + \vert V \vert log \vert V \vert)\).
\end{enumerate}
\subsection{Euro 2020 or Travelling Salesman Problem}
\label{sec:org3612ccc}
Watch a game in every one of \(13\) cities. We want to visit all \(13\) but as
cheap as possible. The English football fans cannot return to the same
country. A \(13\) vertex graph, between any two vertices, there is a price of
the air ticket.

We are looking for a Hamilton cycle.

Given graph \(G = (V, E)\) and \(w\colon E \rightarrow \R_{\ge 0}\). A cycle
that does not repeat.
\subsection{Complexity classes}
\label{sec:org2cfe726}
\(\P\), polynomial time running problem.

\begin{center}
\begin{tabular}{rlllll}
\(n\) & \(1000n\) & \(1000n\log n\) & \(10n^2\) & \(2^n\) & \(n!\)\\
\hline
10 & 0.01 sec & 0.0002 sec & 0.001 & 0.0000001 sec & 0.003 sec\\
100 & 0.1 sec & 0.001 sec & 000001 sec & 400000 years & \(>10^100\) years\\
100000 & 17 min & 20 sec & 2450 min & \(>10^100\) years & \\
\end{tabular}
\end{center}
\section{Lecture 4 \textit{<2018-10-24 Wed>}}
\label{sec:org1b50a08}
\subsection{Decision problems}
\label{sec:org899a7e8}
Problems that output yes or no
\subsubsection{Example}
\label{sec:orgd0d001b}
\begin{itemize}
\item Is there a spanning tree of weight \(\le 42\). (Kruskal algorithm.)
\item Is there a path of weight \(\le 405\) from \(u\) to \(v\)? (Djistra's algorithm.)
\end{itemize}
\subsection{Class P}
\label{sec:org745f53b}
The set of all decision problems with a polynomial time algorithm.
\subsection{Traveling salesman problem}
\label{sec:org2f9387b}
We don't know if the problem is in \(\P\).

As a decision problem: There is a graph \(G = (V, E)\) and \(w\colon E
   \rightarrow \R_{\ge 0}\)., You ask what is the smallest weight Hamiltonian cycle. \footnote{"and the decision problem version ("given the costs and a number x, decide
whether there is a round-trip route cheaper than x") is NP-complete."-Wikipedia}
\subsubsection{Approximation algorithm}
\label{sec:org692a9ea}
\textbf{Definition}: An \(\alpha\) approximation of TSP is an algorithm that turns a
 Hamiltonian cycle whose weight is within \(\alpha\) fraction of the min
 weight Hamiltonian cycle.\footnote{In general we don't know how to approximate the TSP, but we can do it with some extra conditions}
\subsubsection{Extra conditions}
\label{sec:org6cc0686}
Triangle inequality: the weight function satisfies the triangle inequality
if every two vertices of the graph, the weight \(w(xy) \le w(xz) + w(zy)\).

Examples: The usual Euclidean distance satisfies this.
A non-example is Airfare cost.
\subsection{Approximation algorithm for TSP}
\label{sec:org0d89ed8}
\subsubsection{Algorithm}
\label{sec:org5bf387d}
\textbf{Input}: a weight function \(w\colon E(K_n) \rightarrow \R_{\ge 0}\) with
triangle inequality. (We assume that it is a complete graph.)

\textbf{Output}: Hamiltonian cycle \(C\).

\textbf{Algorithm}:
\begin{enumerate}
\item Find the minimum weight spanning tree (Kruskal algorithm.)
\item From the spanning tree, we create a closed walk spanning all vertices by
traversing each edge of \(T\) twice in both directions.
\item Traverse \(W\), when hitting a vertex that was used before, we do a short
cut. (Go instead to next vertex \(W\)) Do this iteratively.
\end{enumerate}
4 \textbf{Termination}: when all vertices are traversed, output \(C\).

We know that \(w(W) = 2w(T)\) and \(w(C) \ge w(W)\).

\(C^{*}\) is a minimum weight Hamilton cycle. How does this compare to the
weight of the spanning tree. We know that \(w(C^{*}) \ge w(T)\). and thus
\(w(C) \le 2 w(C^{*})\).
\subsubsection{Running time}
\label{sec:org2a1de6a}
\begin{enumerate}
\item Kruskal: \(O(n^2\log n)\)
\item Closed walk \(W\), \(O(n)\).
\item short cutting: \(O(n)\).
\end{enumerate}
\subsection{Hall's theorem}
\label{sec:orgaf112e4}
If \(G = (A \cap B, E)\) a bipartite graph, then \(G\) has a matching \(A\) if and
only if for every subset \(S \subset A\), \(\vert N(S) \vert \ge \vert S \vert\).

The non-trivial direction implies that when there is no matching saturating
\(A\), then there is an \(S \subset A\), \(\vert N(S) \vert < \vert S \vert\).
\subsection{Class \(\NP\)}
\label{sec:org3445df8}
A decision problem is in class \(\NP\) if the YES answer can be verified
efficiently (within time that is polynomial in variable size.) (In other
words, there is a polynomial size certificate.)

The perfect matching problem is in NP. \footnote{Input is a graph \(G\) and the question is whether there is a perfect matching.}

Opposite of perfect matching: Does \(G\) has a \(PM\)? We can use Hall's
condition as a certificate. Hence the problem is in NP.
\subsection{Class \(\coNP\)}
\label{sec:orgfca3286}
Means that the problem is in \(NP\) and the negation of the problem is also in
\(NP\).
\subsection{About Hamilton path}
\label{sec:orgc9c23b4}
The Hamilton path problem is in \(NP\).

But the negation of the HAM is not known to be in \(NP\). In other words, we
don't know if HAM is \(\coNP\).\footnote{The belief is that this is not true. This is one of the Millennium problems.}
\subsection{Problem reduction}
\label{sec:orgdec1f78}
Maximum weight spanning tree problem can be reduced to a minimum weight
spanning tree. (You can solve the minimum weight spanning tree problem by
inverting the sign of the edges.) Furthermore, it is a polynomial time
reduction.

A problem is called \(\NP\) hard if any problem in \(\NP\) class can be reduced
by the problem.

If furthermore, the problem is in \(\NP\), then we call it \(\NP\) complete.

Example: 3-SAT is \(\NP\) hard and also \(\NP\) complete.

Karp came up with \(21\) natural \(\NP\) complete problems, all of them are \(\NP\)
complete.
\section{Lecture 5 \textit{<2018-10-30 Tue>}}
\label{sec:org606c6fb}
\subsection{NP class}
\label{sec:orgc890626}
A yes/no problem is in class NP if the answer yes can be verified
efficiently.
\subsubsection{Examples}
\label{sec:org607ef21}
\begin{enumerate}
\item Does the bipartite graph have a perfect matching.
\item Does the bipartite graph have no perfect matching.
\item Does the graph have a Hamiltonian-cycle?
\item \textbf{Don't know} Whether a problem have no hamiltonian cycle.
\end{enumerate}
\subsection{P class}
\label{sec:org32d274d}
A yes/no decision problem is in P if the answer can be found in polynomial
time. It is obviously true that \(P \subset NP\).
\subsection{Co-NP}
\label{sec:org7e1a5be}
A yes/no problem is in the class Co-NP if the no-answer can be verified
efficiently. Again trivially, \(P \subset NP \cap no-NP\).
\subsubsection{Example of NP intersection co-NP}
\label{sec:org6401484}
\begin{enumerate}
\item Perfect matching problem in bipartite graph is in the intersection.
\item Is this graph 2-colorable.
\item Is this graph Eulerian?\footnote{I think it's about going through each edge once.} Verify that the degree of each vertex is even.
(polynomial time algorithm.) Another answer: The yes answer is the list
of edges in an Eulerian edges. For the NO answer, we will be given a
vertex of odd-degree.
\end{enumerate}
\subsection{Conjecture P \(\neq\) NP}
\label{sec:org661870d}
\subsection{Stronger conjecture of \(P \neq\) \(NP\) intersection \(co-NP\)}
\label{sec:org8ef0415}
Is there a factor of \(n < k\). This problem is in the intersection of NP and
co-NP.

Is \(n\) a prime. This was also a problem. But in 2002, it was proven to be
true. (The input size is in \(\log n\).)

A problem in \(NP\) and co-NP and then trying to find a good characterization
and then solving the problem.
\subsection{NP completeness}
\label{sec:orgc70ea6a}
Subtle difference between easy and hard problem.
\begin{enumerate}
\item The graph is 2-colorable? is in P\footnote{Apparently there is a characterization that a graph is 2 colorable if and
only if it has no cycle.}
\item Is the graph 3-colorable? is in NP-complete.
\item Is this planar graph 3-colorable? is in NP-complete.
\item Is this planar graph 4-colorable? is in P. (The is in complexity class TRIVIAL)
The answer is always yes.
\end{enumerate}
\subsection{Hall's theorem}
\label{sec:orgd8caf6f}
If you have a graph \(G\) that is bipartite, then \(G\) has a perfect matching if
and only if for every \(S\) inside \(A\), the \(\vert N(S) \vert \ge \vert S
   \vert\) and for every \(S \subset B\).
\subsection{Necessary conditions for Hamiltonianity}
\label{sec:org053f5d1}
Dirac's theorem \(d(G) \ge n/2 \implies G\) is hamiltonian. \href{https://en.wikipedia.org/wiki/Hamiltonian\_path\#Bondy\%E2\%80\%93Chv\%C3\%A1tal\_theorem}{Wikipedia} (This is
a sufficient condition.) For a cycle, this fails.

Proposition: If a graph \(G\) is hamiltonian then \(\forall S \subset V(G)\),
\(C(G\setminus S) \subset \vert S \vert\). (This is a necessary condition.)\footnote{Here \(C\) is the number of connected components in the graph.} \textsuperscript{,}\,\footnote{Peterson graphs can be used to make a lot of counter examples. This was
taught in discrete math 1.}

A simple example is an edge. It's probably also true for Peterson graph.

We can try to frame something like if \(t C(G\setminus S) \subset \vert S
   \vert\). For peterson graph \(t = 4/3\). There is a conjecture on if we can talk
about a value of \(t\) and do stuff.
\subsection{Does a graph have a perfect matching? Tutte's theorem}
\label{sec:org41f972e}
The question is whether this is in NP intersection co-NP.

The hall's theorem was for bipartite graph.

Consider \(K_{2k+1}\). It has all the edges, but has no perfect matching. Odd
(vertices) graphs are bad obviously.

There was something about applying the necessary condition for Hamiltonian
cycle to the matching problem and arriving at a necessary condition (and sufficient condition.)

\(G\) has a perfect matching \(\implies\) \(\forall S \subset V(S)\), \(o(G
   \setminus S) \subset |S|\).\footnote{Here \(o\) is the number of components of odd size.}

\textbf{Proof}: Let \(M\) be a perfect matching in \(G\). In each odd component, there
is at least one edge \(e_L \in M\) which has one vertex in \(b\) and the other in
\(S\). These edge \(e_L\) are disjoint \(\implies\) \footnote{My condition} \textsuperscript{,}\,\footnote{\href{https://en.wikipedia.org/wiki/Tutte\_theorem}{Tutte's theorem}}
\subsection{Proof of Tutte's theorem}
\label{sec:org20f2bd6}
Let \(G\) be a counter example with maximum number of edges.\footnote{Why can we do this? We fix the number of vertices, so this it actually
makes sense.}

What is a counter example? It should satisfy the following properties:
\begin{enumerate}
\item \(G\) has no perfect matching
\item \(\forall S \subset V(G)\), \(o(G \setminus S) \le \vert S \vert\)
\end{enumerate}

Add \(xy\) to \(G\) and \(G+xy\) is not a counter example. We claim that \(\forall S
   \subset V(G)\), \(o((G+xy)\setminus S) \le \vert S \vert\). \footnote{Here \(xy\) is an edge that is not already in \(G\).}

I know that \(o(G\setminus S) \le \vert S \vert\).
\begin{itemize}
\item If \(xy \in S = \emptyset \implies \vert S \vert\) does not decrease.
\item If \(xy\) goes between even components, then nothing changes.
\item If \(xy\) goes to an odd components, the number of odd components decreases.
Basically do a case analysis and it checks out.
\end{itemize}

\(U = \{v \in V(G) \colon d(v) = n- 1\}\)

Case 1. \(G \setminus U\) is the disjoint union of cliques. There are even
cliques and odd cliques. Even cliques can be matching within themselves. In
odd cliques, you match everything but one, but we can match the extra vertex
to \(U\). Now what happens with the vertices inside \(U\) that doesn't get a pair
in \(U\). If that part is odd, then the whole thing is not odd. But it is not
odd, because we have a contradiction when we put \(S = \emptyset\). So after
everything, the number of unmatched vertices is even (otherwise we have a
contradiction.)

Case 2. \(G \setminus U\) is not a disjoint union of cliques. The idea is from
two almost perfect matching of \(G\), create a perfect matching of \(G\) and two
more edges, create a perfect matching. This leads to a contradiction.

Claim: In \(G, \exists x, u, v, w\) such that \(xu, xv, \in E\), \(uv, vw \notin
   V(G)\). \(w\) is anything that is not in the neighbourhod of \(x\) which is non
empty.\footnote{I think I missed some parts to the explanation.}

\begin{verbatim}

                   x .--------------------------- w
                    /.
                   /  -\
                 -/     \
                /        -\
               /           -\
             -/              \
            /                 -\
           /                    \
          /                      -\
        -/                         \
       /                            -\
      /                               -\
    -/                                  \
   /                                     -\
u .----------------------------------------. v


\end{verbatim}
\section{Lecture 6 \textit{<2018-10-31 Wed>}}
\label{sec:orgd55a568}
\subsection{{\bfseries\sffamily TODO} Tutte's theorem proof}
\label{sec:orgda17a44}
\(\Leftarrow\): G\$, a counter example with maximum number of edges.

\textbf{Claim}: \(G+xy\) has a p.m. \(xy\in E(G)\), \(G\) has no p.m., \(\forall S \in
    V(G)\), \(o(G \setminus S) \le o(\vert S \vert)\)

\(U = \{v \colon deg(v) = n-1\}\) and \(n=\vert V(G)\vert\).

Case 1: \(G \setminus U\) is the union of cliques. We are done, we use Tutte's
condition for empty set.

Case 2: Otherwise, there exists the diagram that I already drew. Our claim
implies that there exists a perfect matching \(M_1\) in \(G + xw\) and also
there is a perfect matching in \(G\) if one adds \(uv\). Our goal is to find a
perfect matching in \(M_1 \cap M_2\). Our goal is to find a perfect matching
in \(M_1 \cap M_2 \setminus \{xw, uv\} \cap \{ux, xv\} \subset E(G)\).

\(M_1 \cap M_2\) is the disjoint union of \$K\(_{\text{2}}\)\$s and even cycles\footnote{Is the \(K_2\) here an edge?}. The degree
of each vertex in the union is either \(1\) or \(2\), because the matching is
perfect because there are two of them. If there is one, then the vertex
participates in the same edge with () matching \(\implies\) \(K_2\) component.

If it is \(2\) \(\implies\) vertex participates in a cycle component.

(cycle is even since edges of the matchings alternate.)

There was a diagram and the proof involved doing stuff on the diagrams. I
don't understand what he did.

The proof in the class was from bondy and murthy. \href{http://www.zib.de/groetschel/teaching/WS1314/BondyMurtyGTWA.pdf}{Bondy and murthy} page 76.

The wikipedia link seems to have the same proof.
\subsection{Perfect matching is in NP intersection co-NP}
\label{sec:orga7e6b49}
Tutte's theorem tells us that the problem is in the intersection of NP and
co-NP. The certificate for the yes case is a matching and for the No case is
a case where the Tutte's theorem is false.

The problem is also in P.
\subsection{Corollary to hall theorem (Theorem of Frobenius)}
\label{sec:orgc63500e}
A \(k\) regular bipartite graph has perfect matching.\footnote{What is regular graph? Every vertex has the same number of neighbours.} (1-factor)

A \(k\) factor is a spanning \(k\) regular subgraph.

This is not true for general graphs. Example: odd cycles, they are \(2\)
regular and 1-factor. Are there examples with even number of vertices.
(3-regular graph with no \(1\) factor.)


\subsection{Theorem (peterson)}
\label{sec:org7c27478}
A \(2k\) regular subgraph has a \(2\) factor.
\subsection{Theorem (another peterson theorem)}
\label{sec:org0656287}
Every \(3\) regular graph without cut edges\footnote{What is a cut edge? A cut edge should mean that you remove an edge and
the graph gets disconnected.} has a perfect matching. (Theorem in
Bondy and Murthy) \footnote{An example of such a graph is Peterson graph.}
\subsubsection{Proof}
\label{sec:org2d3463f}
The proof is component wise. Now we assume that \(G\) is connected.

We will check that Tutte's condition holds. Then Tutte's theorem tells us
that \(G\) has a perfect matching.

\(S\) be an arbitrary subset.

Consider the number of edges between odd components and \(S\).

Claim: For every odd component, there is at least three edges going to \(S\)
from \(C\).

Proof:
\begin{enumerate}
\item \(0\) edges is not possible because connected.
\item \(1\) edge is
not possible, because it would be a cut edge.
\item \(2\) edges are not possible
because the sum of the degrees of the vertices inside the component -2,
\(\sum d(v) - 2 = 2 \cdot e(C)\). Now this is just a handshake lemma.\footnote{What is a handshake lemma?}
\end{enumerate}

The number of edges between odd components and \(S\). The number of edges
going is at least \(3\) times the odd components. On the other hand, the
number of components cannot be more than \(3 \vert S \vert\).

$$ 3\cdot o(G \setminus S) \le \textup{number of edges between odd components and S } \le 3 \cdot \vert S \vert$$

Thus \(\cdot o(G \setminus S) \le \vert S \vert\)
\subsection{Maximum matching problem}
\label{sec:orga4a97aa}
In the decision problem formulation. Is there a matching of size \(k\) in the
graph on \(n\) vertices.

Is this problem in NP intersection co-NP? The problem is obviously in NP.

For Bipartite graphs, we can provide the other verification by Konig's
theorem.
\subsection{Konig's theorem}
\label{sec:orgfe75c50}
\(G\) is bipartite, then \(\alpha(G)=\beta(G)\). Here \(\alpha\) is the size of the
largest matching and \(\beta\) is the size of smallest vertex cover.

\(C \subset V(G)\), the vertex cover if \(\forall e \in E(G)\), \(e\cap C \neq
   \emptyset\).
\subsection{Konigs on Maximal matching problem}
\label{sec:orge6d88d7}
Suppose \(\alpha(G) = 88\), then konig gives a certificate to show that there
exists a vertex cover of size \(88\). So this means that there are no matching
of size more than \(89\).
\subsection{Homework: a corollary of Tutt due to Berge}
\label{sec:org5c9997a}
If \(G\) is a arbitrary graph, then it is true that \(2\alpha'(G)\) is equal to
the minimum of the following sum of quantities: \(\min \{ n - o(G\setminus
   S) + \vert S \vert \colon S \subset V(G) \}\).\footnote{If the graph satisfies tutt's condition, then \(\alpha\) should evaluate
to \(n/2\).}

It is easy to show one direction. But this is the maximum size, which is the homework.

This example would put the problem of maximum matching into the intersection.
\subsection{How to find maximum matchings in polynomial time?}
\label{sec:orgba88883}
\subsection{Proposition about maximum matching}
\label{sec:org0949bcd}
IF \(M \subset E(G)\) is a maximum matching of \(G\), \(\iff\) there is no
\(M\) augmenting path.

\(M\) augmenting path: It's a path in which non-edges and edges follow each
other alternatively. One direction is easy. If \(M\) is a maximum matching,
then there is no \(M\) augmenting path.
\subsection{A M alternating path}
\label{sec:orgf8bc88e}
A path of \(G\) where edges of \(M\) alternate with non-edges of \(m\).

An \(M\) alternating path that starts and ends in an unsaturated vertex is
called \(M\) augmenting. \href{https://en.wikipedia.org/wiki/Saturation\_(graph\_theory)}{Wikipedia}
\subsection{Using the characterization for Bipartite graph}
\label{sec:org37815ae}
\footnote{Apparently the problem for general graph is also in P. The algorithm for
this graph was what lead to the definition of \(P\).} You have a matchin and then unsaturated vertices. The idea is to
somehow extend the matching to the unsaturated edges.

\begin{verbatim}
-------------------------------------------------------------\------------------------------\---------------
-                                                                                                           \------
(                                                                                                                  )
\              |           |              |              .                                                  /------
\              |           |              |                            .                    /---------------
\              |           |              |                  /------------------------------
---------------------------+--------------+------------------
                |          |              |
                |           \              \
                |           |              |
                 \          |            --+----------------------------------------------------------------------------------------------------------------------
       ----------+----------+-----------/  |                                                                                                                      \---------------------------------
------/          |          |             .|                 .           .     .       .                                                                                                            \------------
(                           |                                                                                                                                                                                    )
------\                                                                                                                                                                                             /------------
       ---------------------------------\                                                                                                                         /---------------------------------
                                         -------------------------------------------------------------------------------------------------------------------------



\end{verbatim}
\section{Lecture 7 \textit{<2018-11-06 Tue>}}
\label{sec:orgfade5cd}
\subsection{Maximum matching is in P}
\label{sec:orga711466}
\subsubsection{Proposition}
\label{sec:orgb930bd0}
\(M \subset E(G)\) is a matching in \(G\).

\(M\) is a maximum \(\iff\) there is no \(M\) augmenting path in \(G\). (is
\(M\) alternating if it starts and ends at an unsaturated vertex.)
\subsubsection{Augmenting path algorithm}
\label{sec:org837f08a}
Input: Bipartite graph \(G = (X \cap Y, E)\), a matching \(M \subset E\).
Output: Either an \(M\) augmenting path or a cover of size \(\vert M \vert\).

Initialization: \(S = U\), \(Q = \emptyset\), \(T = \emptyset\)

Iteration: If \(Q = S\) STOP and return \(M\) (as maximal matching), \(TU(x
    \setminus S)\) (as min cover of size \(\vert M \vert\)) Else select \(x \in S
    \setminus Q\), \(\forall y \in N(x)\) with \(xy\in M\), DO if \(y\) is unsaturated,
then stop return a \(M\) augmenting path from \(U\) to \(y\). Else \(\exists w \in
    X\), \(yw \in M\) update, \(T = T \cap \{y\}\) and \(S = S \cup \{w\}\). Update \(Q
    = Q \cup \{x\}\). \footnote{I missed a lot of details here.}
\subsubsection{Proposition}
\label{sec:org393a5a6}
\(G\) graph, \(M \subset E(G)\) is a matching. \(C \subset V(G) \implies \vert M
    \vert \le \vert C \vert\). Here \(C\) is the cover.

The idea is that every cover has to be bigger than the matching.
\subsubsection{{\bfseries\sffamily TODO} Proof of correctness}
\label{sec:org7bb5fa0}
Stopping the algorithm: The algorithm can either stop with a \(M\) augmenting
path or it can stop with a maximum matching or a cover.

Proof of correctness: If the algorithm terminates with a matching \(M\) and a
cover \(T \cap (X \setminus S)\), we terminate at \(Q = S\), which means that we
have explored all the neighbours of \(S\) and they are all in \(T\). We want to
conclude that there exists no edge between \(S\) and \(Y-T\). (Because if there
is no edge between \(T\) and \(Y-T\), then \(T\) together \(S-T\) is a cover.)

If there is an edge from \(S\) to an unsaturated vertex \(y \in T\), then we
would have immediately put this vertex into \(T\). These two cases are not
possible.
\subsubsection{Comments}
\label{sec:org66db888}
\(\vert M \vert = \vert T \vert + \vert X \setminus S\vert\). By the selection
of \(S\) and \(T\), vertices of \(T\) are put into \(T\) where their \(M\) partner is
put into \(S\).

\(\implies\), \(S = U \cap M\) partners of vertices in \(T\).
\subsubsection{Internet reference}
\label{sec:org4c330dd}
\url{http://www.columbia.edu/\~cs2035/courses/ieor8100.F12/lec4.pdf}
\subsection{Theorem}
\label{sec:orge2b867e}
Repeatedly applying APA to bipartite graph produces a maximal matching and
minimal cover. The running time is \(O(V(G) \cdot e(G))\)

If we repeat APA \(\le n/2\) times. One running of APA considers each edge
\(\le 1\), implies \(O(e(G))\).
\subsection{Matching with weights}
\label{sec:org69091be}
We have a weight function on the edges. \(w\colon E(K_{n, n}) \rightarrow \R\).

The goal is to find a perfect matching \(M\) such that the weight of the
matching which is the sum \(\sum w(e)\) is maximum.\footnote{Practical example: There is a company which has factories and corn
farms, the edges represent the profit the factory makes by producing corn from a
certain farm. There is more to this story. But I missed it.}

In general, we say that the weighted cover \(W\) is \(u_0, \cdots, u_n, v_1,
   \cdot, v_n\) such that \(u_i+v_j \ge w_{ij}\) for all \(i, j = 1, \cdots, n\). The
cost of \((u, v)\), \(c(u, v) = \sum u_i + \sum v_j\). (\textbf{The minimum weighted
cover problem} is to find a cover of minimum weight.)

The interesting part is that these two problems can be solved together.
\subsection{Duality lemma}
\label{sec:org3541e86}
For all perfect matching \(M\) and cover \((u, v)\) in a weighted bipartite graph
\(G\), \(C(u, v) \ge w(M)\) (\textbf{Home work})

\subsubsection{Corollary}
\label{sec:org1773088}
If \(C(u, v) = w(M)\), then \((u, v)\) is a min-cost cover and \(M\) is a maximum
weight matching.
\subsection{Algorithm for Maximal weighted matching}
\label{sec:orgd261344}
Equality: Subgraph \(G_{u, v} \subset K_{n, n}\), a spanning subgraph which has
the same vertex set and the edges at those \(x\) and \(y\) where \(w_{i, j} =
   u_i+u_j\).
\subsection{Hungarian algorithm}
\label{sec:org2a67efe}
Input: A matrix \(w_{i, j}\) of weights of the edges of \(K_{n, n}\) with points
\(X = \{x_1, \cdots, x_n\}\), \(Y=\{y_1, \cdots, y_n\}\).

Idea: Iteratively adjust a cover \((u, v)\) until \(G_{u, v}\) has a perfect
matching.

if \(G_{u, v}\) has a perfect matching, then \((u, v)\) and \(M\) are both optimal.
Initial \(u_i = \max \{W_{i, j}\colon j=\{1, \cdots, n\}\}\) and \(v_i =0\). Note
that this is a cover and \(u_i + v_j \ge w_{i, j}\) for all \(i, j\).

Iteration: Create \(G_{u, v}\), using APA and find a maximal matching \(M\) and a
minimum vertex cover \(Q\) and \(Q\) will be equal to \(T \cup R\) (where \(T = Y
   \cap Q\) and \(R = X \cup Q\))

If \(M\) is a perfect matching, then we are done. (By corollary of the duality
lemma.)

Else \(\varepsilon = \min\{u_i + v_i -w_{i, j}\colon x_i \in X \setminus R,
   y_j \colon Y\setminus T\}\) (all elements are positive here.)

We update as follows: \(u_i = u_i - \varepsilon\) if \(x \in X \setminus R\) and
\(V_j = v_j+\varepsilon\) if \(y \in T\). Now you iterate.

Why is the update \((u, v)\) still a cover? It involves 4 cases depending of
where the pair \((i, j)\) goes to.

\begin{enumerate}
\item If \(x_i \in R\) and \(y_j \in Y \setminus T\).
\(u_i, v_j\) are unchanged.
\item \(x_i \in R, y_j \in T\) implies that \(u_i + v_j\) grew by \(\varepsilon\) which
is okay.
\item \(x_i \in X \setminus R\), \(y_j \in T\), \(u_i - \varepsilon, v_j +
      \varepsilon = u_i + v_j\)
\item x\(_{\text{i}}\) \(\in\) X \(\setminus\) R, y\(_{\text{j}}\) \(\in\) Y \(\setminus\) T\$. So \(u_i + v_j \ge w_{i,
      j}\).
\end{enumerate}
\section{Lecture 8 \textit{<2018-10-31 Wed>}}
\label{sec:org58cb5df}
\textbf{Input}: \((w_{i, j})_{i, j =1}^n\) weights or \(E(K_{n, n})\), \(X = \{x, \cdots,
   y_n\}\), \(Y = \{y, \cdots, y_n\}\)

\textbf{Initialization}: \(u_i = \max_\{w_{i, j}, j\cdots n\}\), \(v_j = 0\)

\textbf{Iteration}: For \(m\), \(G_{u, v}\), \(V(G_{u, v} = V(K_{n, n}), E(G_{u, v}) =
    \{x_iy_j \colon w_ij = u_i + v_j\}\)

Find a maximal matching \(M \subset G_{u, v}\) and min vertex case \(Q = T \cup
    R\).

If \(M\) is perfect matching, then return (as max weighted is perfect
matching.), \(R = X \cap Q, T = Y \cap Q\). \((u, v)\) as a minimum cost cover.

Else \(\epsilon = \min\{u_i + v_j - w_{ij}\colon x_i \in X \setminus R, y_j
    \in Y \setminus T \}\) Update \(u_i = u_i - \epsilon\) if \(x_i \in X \setminus
    R\) and \(v_j = v_j + \epsilon\) if \(y_j \in T\).

\textbf{Remark}: \(G\) is bipartite, define \(w_{i, j} \iff w_{i, j} = 1 \iff x_iy_j
     \in E(G)\) vertex cover \(G\) implies \(u, v\) characteristic \((011)\) vectors of
 \(C\), implies cover of \(w_{ij}\), \(w_{i, j} \le u_i + v_j\). True since if
 \(w_{i, j} = 1\) then \(x_iy_j \in E(G)\), then \(x_i\) or \(y_j \in C\), then
 \(u_i\) or \(v_j =1\).

\begin{center}
\begin{tabular}{rrrrrr}
x & 0 & 0 & 0 & 0 & 0\\
\hline
5 & 1 & 2 & 3 & 4 & 5\\
8 & 6 & 7 & 8 & 7 & 2\\
5 & 1 & 3 & 4 & 4 & 5\\
8 & 3 & 6 & 2 & 8 & 7\\
5 & 4 & 1 & 3 & 5 & 4\\
\end{tabular}
\end{center}

Excess matrix
\begin{center}
\begin{tabular}{rrrrrr}
x & 0 & 0 & 0 & 0 & 0\\
5 & 4 & 3 & 2 & 1 & 0\\
8 & 2 & 1 & 0 & 1 & 6\\
5 & 4 & 2 & 1 & 1 & 0\\
8 & 5 & 2 & 6 & 0 & 1\\
5 & 1 & 4 & 2 & 0 & 1\\
\end{tabular}
\end{center}

Now we form the graph with \(0\) edges and find a perfect matching.

\begin{center}
\begin{tabular}{rrrrrr}
x & 0 & 0 & 1 & 1 & 1\\
\hline
4 & 3 & 2 & 2 & 1 & 0\\
7 & 1 & 0 & 0 & 1 & 6\\
4 & 3 & 1 & 1 & 1 & 0\\
7 & 4 & 1 & 6 & 0 & 1\\
4 & 0 & 3 & 2 & 0 & 1\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{rrrrrr}
x & 1 & 0 & 1 & 2 & 2\\
\hline
3 & 3 & 1 & 1 & 1 & 0*\\
7 & 2 & 0* & 0 & 2 & 7\\
3 & 3 & 0 & 0* & 1 & 0\\
6 & 4 & 0 & 5 & 0* & 1\\
3 & 0* & 2 & 1 & 0 & 1\\
\end{tabular}
\end{center}

Now we end up with a perfect matching in the equality subgraph. (The ones
labelled *)

\begin{center}
\begin{tabular}{lrrrrr}
x & 0 & 0 & 0 & 0 & 0\\
\hline
 & 1 & 2 & 3 & 4 & 5*\\
 & 6 & 7* & 8 & 7 & 2\\
 & 1 & 3 & 4* & 4 & 5\\
 & 3 & 6 & 2 & 8* & 7\\
 & 4* & 1 & 3 & 5 & 4\\
\end{tabular}
\end{center}

The above table represents the cover in the original graph.

Perfect matching of weight \(5 + 7 + 8 + 4= 28\) and \(C(u, v) = 3 + 7 + 3 + 6 +
   3+ 1 + 1 + 2 + 2 = 28\).
\subsection{Proof}
\label{sec:orga669e81}
If we add edges at every step in the graph from which we get the matching, we
would be done. But we are not exactly doing it.

Observations: \(\vert Q \vert = \vert M \vert\), no \(M\) edge is covered twice by \(Q\).

\(T = \{y \in Y \colon \exists M\) alternating \((U, y)\) path \(\}\).

\(R = \{x \in X \colon \nexists M\) alternating \((U, Y)\) path \(\}\).

where \(U = \{x \in X \colon x\) is \(M\) unsaturated \(\}\).

For termination of the Hungarian algorithm, count for \textbf{the number of vertices
that are reached from \(U\) on an \(M\) alternating path}. This quantity grows in
each iteration. (or \(M\) augmenting path, which implies that there is a larger
matching.)

The edges of \(M\) alternating path starting at \(U\) remain in \(G_{u, v}\). Edges
can be lost only between \(T\) and \(R\). But these edges are not participating
in the alternating path. In \(M\) alternating path, vertices of \(T\) are only
connected to vertices in \(S = X - R\).

By the choice of \(\varepsilon\), there is at least one pair \(x_i y_j\) such
that \(x_i \in X \setminus R\), \(y_j \in T\),such that \(x_i y_j\) is a new edge
in the equality sub-graph.

This means that after \(\le \frac{n}{2}\) iterations, or \(M\) unsaturated \(y\in
   Y\) is reached via a \(M\) alternating path, which means that the matching is
growing, and the matching can grow at most \(n/2\). Thus after \(\frac{n^2}{4}\)
iterations.
\subsection{Connectivity problem}
\label{sec:orgdae11f4}
\subsubsection{Definition (Vertex cut)}
\label{sec:orgba8b37c}
A vertex cut of a graph \(G\) is a set of vertices such that \(G-S\) is
disconnected.\footnote{In a good connected graph, you don't want a vertex set to be small.}
\subsubsection{Definition (Connectivity of G)}
\label{sec:orgec8f8dd}
Connectivity of \(G\), denoted by \(\kappa\), is the minimum size of the vertex
cut. If your graph is disconnected to begin with, then this is zero.

By definition, for a clique, \(K(K_n) = n-1\). The empty graph is "considered"
to be disconnected.
\subsubsection{Examples}
\label{sec:orgcfad167}
\(K(K_{n, m})= \min\{n, m\}\). Proof: \(\le\), Given a vertex cut of size
\(\min\{m, n\}\), smaller side

\(\ge\) Now we remove \(\{m, n\} - 1\), vertices of \(S\), \(K_\{n, m\} - S\), and
through them everybody else can be reached.
\subsubsection{Proposition}
\label{sec:org890a56a}
For all \(G\), \(K(G) \le n-1\). The clique is \(K(G) = n-1 \iff G = K_n\).

\(K(G) \le \delta(G)\), here \(\delta\) is the min degree. This is kinda clear,
because we can pick a vertex with the minimum degree and remove all it's
neighbours.

\(K(Q_d)\), the one skeleton of the \(d\) dimensional cube.

\(E(Q_d) = \{uv \colon\) \(u\) and \(v\) differ in exactly one coordinate \(\}\).
From the proposition, the minimum degree is \(d\).

I didn't write the rest of the argument. But doesn't look so hard. The proof
was by induction.
\section{Lecture 9 \textit{<2018-11-07 Wed>}}
\label{sec:orgeb81a67}
\subsection{Connectivity of graph}
\label{sec:org3664248}
\(G \neq K_n\), then \(K(G) = \min \{ \vert S \vert \colon G - S \textup{ is not
   connected } \}\).

\(G = K_n\), \(K(G) = n-1\).
\subsection{Proposition}
\label{sec:orgc90cdf8}
\(K(G) \le v(G) - 1\) and \(K(G) \le \delta(G)\).
\subsection{Extremal questions}
\label{sec:orgf7da676}
Given \(n\) and \(k\), what is the smallest number \(e\) of edges that there exists
as \(n\) vertex \(k\) connected  graph with \(e\) edges.
\subsection{Proposition}
\label{sec:orgd9c6c7c}
For all \(k \ge 2\), there exists a \(k\) connected graph on \(n\) vertices with
the ceil of \((n-k)/2\) edges.
\subsection{Theorem (Chvatal-Erdos)}
\label{sec:org8d7f078}
If \(G \neq K_2\) and its connectivity \(K(G) \ge \alpha(G)\). \(\alpha(G)\) is the
size of the largest independent set of vertices. And \(\alpha'\) is the size of
the largest matching \footnote{We use the convention that the prime is the corresponding to edges.}

Then \(G\) is hamiltonian.
\subsubsection{Proof}
\label{sec:orgc3e7892}
Take a cycle in \(C \subset G\) which is the longest. If the length of \(C\) is
\(n\), we are done. So we can assume the length of \(C\) is less than \(n\).

Let \(H\) be a component of \(G -C\).

\(k \le \delta(G)\). The length of the longest cycle is at least \(\delta\).
Take the largest path in \(G\) and the end points of \(P\) only have neighbours
on \(P\), otherwise the path could be extended. The farthest neighbour of \(w\)
of \(x\) on \(P\) has distance at least \(\delta(C)\).

This means that the segment \(P[x, w] + xw\) forms a cycle of length \(\ge
    \delta(G) + 1\).

Back to \(H\). Let name the vertices which have an edge to \(H\) as \(v_i\). There
exist at least \(k\) vertices on \(C\) which have an edge to \(H\) (this is
because of \(k\) connectedness; otherwise we could separate \(C\) from \(H\) by
the deletion of strictly less than \(k\) vertices, which is a contradiction.)

Notice that two adjacent vertices in \(C\) cannot have an edge to \(H\),
otherwise, we can extend \(C\) by entering \(H\) and coming back. We call
\(v_{i}^{+}\) denote the vertices that are the vertices that follows \(v_i\) on
the cycle. We now know that all off \(v_{i}^{+}\) is distinct from \(v_i\).

We can see that there cannot be two edges between \(v_{i}^{+}\) and
\(v_{j}^{+}\), one can see this by drawing an easy diagram, for one can form a
bigger cycle than \(C\), which is a contradiction.

This implies that \(\{v_i^+, \cdots, v_k^+\}\) is an independent set of size
\(k\). This union with any vertex in \(H\) is an independent set. This is a
contradiction because \(\alpha(G) \ge k+1\), which is not what we assumed.
\subsection{Definition (Disconnecting set of edges)}
\label{sec:org6af0331}
\(F\) is a disconnecting set of edges if its removal makes the graph
disconnected, i.e., \(G-F\) is disconnected.

\(\kappa'(G) = \min\{\vert F \vert \colon F \subset E(G) \textup{ is a
   disconnecting of edges}\}\)

This is the \textbf{edge connectivity} of \(G\). \(G\) is called \(k\) edge connected,
\(k'(G) \ge k\).
\subsection{Definition}
\label{sec:orgcc991ec}
A subset \(S \subset V(G)\), then an edge-cut of the Multi-Graph is an edge-set
of the form \([S, \bar{S}] = \{xy \colon x = S, y \in \bar{S}\}\). For some
subset \(S, \emptyset \neq S \neq V(G)\). Here \(\bar{S} = V(G)\setminus S\), the
set theoretic complement.
\subsection{Observation}
\label{sec:orga62bb61}
\(K'(G) = \min\{[S, \bar{S}] colon S\subset V(G), S\neq \emptyset, S \neq V(G)
   \}\)
\subsubsection{Proof}
\label{sec:org6291927}
If \(F \subset E(G)\) is a disconnecting set of edges with \(\vert F \vert
    \subset K'(G)\), then let \(S\) be a components of \(G - F\), then \([S, \bar{S}]
    \subset F\). Since \(F\) is minimal, this implies that \(F = [S, \bar{S}]\).
\subsection{Bounds}
\label{sec:org0cce5b7}
\begin{enumerate}
\item Initial bound, \(k'(G) \le \delta(G)\).
\item \(\kappa'(G) > \kappa(G)\) is true for simple graphs.

\textbf{Proof}: The proof is easy. Try to show that there is a vertex cut of size
\(\kappa'(G)\) whose removal disconnects \(G\).

Take an edge cut such that the size \(\vert[S, \bar{S}]\vert = K'(G)\).

Case 1: \(G\) contains \(K_{\vert S \vert, \vert \bar{S} \vert}\) on \(S\) and
\(\bar{S}\). Then the statement is trivially true, since \(K'(G) = \vert S,
      \bar{S}\vert = \vert S \vert \vert \bar{S}\vert = \vert S \vert (n - \vert
      S \vert) \ge n - 1\ge \kappa(G)\).

Case 2: We don't have a complete bipartite graph, so we can identify
vertices that we will disconnect, \(x\in S\), \(y\in \bar{S}\) such that \(xy
      \notin E(G)\).

\(T_1\) is the set of neighbours \(N(x) \cap \bar{S}\) on the other side. \(T_2
      = \{w \in S \setminus x \cap S \colon N(w) \cap \bar{S}\} \neq \emptyset\).
The removal of \(T_1\) and \(T_2\) separate \(x\) and \(y\). Now \(\vert T_1 \cap
      T_2 \vert \le \vert[S, \bar{S}]\). Because there is an injection from the
left set to the other one, given by if \(w\in T_1, take xw\). \(w\in T_2
      \mapsto wu_w\).

Note \(S\) is just one side that is separate from the other side.
\end{enumerate}
\subsection{Is graph \(G\) \(k\) connected?}
\label{sec:orgf67c817}
It's definitely in co-NP. If it's not \(k\) connected, we get a set a vertices,
and, we'll be able find it in polynomial time.

But is it in \(NP\)?

\(1\) connectedness if in \(P\).

But \(2\) connectedness is in \(P\). There are only a \(n^2\) number of vertices.
Hence polynomial.

But we want to know about \(k\) connectivity, when \(k\) is the function of \(n\).
\subsection{Theorem (A characterization of \(2\) connectivity)}
\label{sec:org69e1be6}
\(G\) is \(2\) connected if and only if for all \(x, y \in V(G)\), they are on a
cycle.\footnote{It is true that for \(k\) connectedness, any \(k\) vertices are on a cycle,
but the other direction is not true.}

\(G\) is \(2\) connected if and only if for all \(x, y \in V(G)\) if there exist
\(2\) internally disjoint \(x, y\) paths. \footnote{This can be generalized in a very straightforward way.}
\section{Lecture 10 \textit{<2018-11-14 Wed>}}
\label{sec:org4bdeb67}
\subsection{Menger's theorem (VMT)}
\label{sec:orga265129}
A graph \(G\) is \(k\) connected if and only if for all pairs of vertices \(x, y
   \in V(G)\), there exist \(K\) pairwise internally disjoint paths.
\subsubsection{Remarks}
\label{sec:orgba0cd21}
\begin{enumerate}
\item This implies that \(k\) connectivity is in \(NP\) intersection co-NP. It
takes \(O(kn)\) to check that they are internally disjoint. There are
\(O(n^2)\) pairs to check
\item For \(k=2\), the VMT says that \(G\) is \(2\) connected if and only if for
every \(x, y \in U(G)\) is a cycle.
\end{enumerate}
\subsubsection{Proof of (\(\Leftarrow\)) VMT}
\label{sec:org9d144a9}
Suppose for a contradiction that there are \(k\) internally disjoint vertices
between every vertices, and \(\kappa(G) \le k-1\). Let \(S\) be a such a
separating set.

Let \(S\) be such a separating set which separates \(x\) from \(y\), then every
\(xy\) path passes through \(S\), and pigeon-hole implies that two halfs share a
vertex of \(S\). This is a contradiction.
\subsubsection{Proof of \(\implies\) of VMT for \(k=2\)}
\label{sec:orgf443bff}
Assume \(K(G) \ge 2\), need a cycle containing \(x, y \forall\) pairs \(x, y \in
    V(G)\).

Induct on \(d(x, y) = \{\vert e(P)\vert \colon P \textup{ a xy path }\}\).

\textbf{Base case}: \(d(x, y) = 1\), \(\kappa'(G) \ge \kappa(G) \ge 2\). Implies \(G
    \setminus \{xy\}\) is still connected \(\implies xy\) path \(P\) in \(G\setminus
    e \implies P \cap \{e\}\) is a cycle which contains \(x, y\).

\textbf{Induction step}: \(d(x, y) = d\), let \(P\) be a path of length \(d\). Let \(v\) be
 the last vertex before \(y\) on \(P\). Induction says that some \(C\) containing
 \(x\) and \(v\).

If \(y\in C\), we are done. If not, there exist a \(xy\) path \(Q\) in
\(G\setminus \{v\}\) as \(G\) is \(2\) connected. Now, it's easy to construct a
cycle from \(x\) to \(y\).

Let \(z\) be the last \(v_x\) of \(C\) on \(Q\) if it exist. Let \(D\) be \(C\setminus
     [z, v]\) such that \(x \in D\). Now \(C' = D \cup Q\vert[x, y] \cup \{xy\}\)
cycle containing \(x\) and \(y\).

If no such \(z\) exists, then it's straightforward to construct a cycle.
\subsection{Edge Menger's theorem}
\label{sec:orgc0f6f6b}
A graph \(G\) is \(k\) connected if \(\kappa'(G) \ge k\) if and only if for all \(x,
   y \in V(G)\), there exists pairwise edge disjoint \(x-y\) paths.
\subsection{Local version of Menger's theorem}
\label{sec:org3708e16}
\subsubsection{Separating set}
\label{sec:org5a540da}
\begin{enumerate}
\item A set \(S \subset V(G)\) is an \(xy\) separating set if \(x\) and \(y\) are in
different connected components of \(G\setminus S\).
\item \(\kappa(x, y) = \min\{\vert s \vert \colon S \textup{ is a xy separating set}\}\)
\item \(\lambda(x,y) = \max\) size of a family of pairwise disjoint \(xy\) paths.
\end{enumerate}
\subsubsection{Local version of theorem}
\label{sec:orgfb96ea4}
For all graphs \(G\) and \(x, y \in G\), we require that \(x\) and \(y\) are not
adjacent edges.

For all \(x, y\), \(\kappa'(x, y) = \lambda'(x, y) = \max\) size of family of
pid \(xy\) paths. Here \(\kappa'\) is the minimum \(\vert F \vert\) such that \(x\)
and \(y\) are in different components of \(G\setminus F\).
\subsubsection{Local theorem implies Global}
\label{sec:org7835857}
What we need to do is that for all \(x, y \in G\), \(\exists \ge k = \kappa(G)\)
pid paths.
\begin{enumerate}
\item \(x\) and \(y\) are not adjacent \((xy \neq E(G))\) implies \(\kappa(G) \le
       \kappa(x, y)\le \lambda(x, y)\). The local theorem says that the last term
is at least \(\lambda(x, y)\), implies \(\ge k\) pid \(xy\) paths.
\item For \(xy = e\), \(\kappa(G-e) \ge K(G) - 1\). Suppose for a contradiction
that \(S\) separates \(G-e\) and \(\vert S \vert \le \kappa - 2 \le n-3\). We
know as \(\vert s \vert < K\), \(G\setminus S\) is connected implies that
\(G\setminus S\) must have a "bridge" \(e\). Meaning we have two disconnected
sets only connected by \(e\) between \(T\) and \(T'\).

Without loss of generality \(x \in T\), and \(\vert T \vert \ge 2\). But then
\(S \cap \{x\}\) is a separating set of \(G\) of size \(\le \kappa - 1\).
\item If \(xy\) is an edge in \(E\). Removing it means that \(\kappa(G-e) \ge k-1\),
then 1 implies that \(k-1\) pid \(xy\) paths in \(G-e\) and \(e=xy\) adds another
path.
\end{enumerate}
\subsection{Flow networks}
\label{sec:orgd66b7b2}
\subsubsection{Example}
\label{sec:org9ea5dea}
A graph was drawn.
\subsubsection{Definition}
\label{sec:org60d0f7f}
A network is a quadruple \((D, s, t, c)\) where \(D\) is a directed (multi)
graph. \(s \in V(D)\) is the source vertex and \(t\) is the sink vertex. \(c
    \colon E(D) \rightarrow \R^{+}\) is the capacity.

A flow is \textbf{feasible} if
\begin{enumerate}
\item For all \(v \neq s, t\), the net flow in is equal to the net flow out, i.e,
for all \(v\neq s, t\), \(\sum f(uv) =f^{-}(v) = f^{+}(v) = \sum(vu)\). This
is called the \emph{conservation constraints}. \footnote{I think there is a typo in the last \(\sigma\) part.}
\item For all directed edges, we require that the flow on the edge is
non-negative and at most the capacity. This is called \emph{capacity
constraint}
\end{enumerate}

The \textbf{value} of of a flow is the net flow into the sink, \(f^{-1}(t) -
    f^{+}(t) = val(f)\).

A \textbf{max flow} is a feasible flow with maximum \(\val(f)\).
\subsubsection{Problem}
\label{sec:orgcdbb553}
Given a network flow, we need to find a max value and if possible find a max
flow.
\subsubsection{Homework}
\label{sec:org78b94fb}
For any \(Q \subset V(D)\), with \(s\in Q\) and \(t\in \bar{Q} = V(D) \setminus
    Q\), \(\val(f) = \sum_{e\in [Q, \bar{Q}]} f(e) - \sum_{e\in [\bar{Q}, Q]}
    f(e)\)
\subsubsection{Definition}
\label{sec:orgcc7828a}
Given \(Q \subset V(D)\), \(\bar{Q} = V \setminus Q\), with \(s\in Q, t\in
    \bar{Q}\), the capacity of the cut is capacity \(\cap[Q, \bar{Q}] = \sum_{e\in
    [Q, \bar{Q}] c(e)}\)
\subsubsection{Lemma}
\label{sec:org5bcbeee}
Weak duality: If \(f\) is a feasible flow and \([Q, \bar{Q}]\) a source/sink
cut, then the value of the flow \(\val(f) \le \cap([Q, \bar{Q}])\)
\begin{enumerate}
\item Proof
\label{sec:orge7e7e50}
Fix \(f\) and \([Q, \bar{Q}]\). \(\val(f) = \sum_{[Q, \bar{Q}]} f(e) -
     \sum_{[\bar{Q}, Q]} f(e) \le \sum_{[Q, \bar{Q}]} c(e) - 0 = \cap([Q,
     \bar{Q}])\)
\end{enumerate}
\subsection{Theorem (Ford-Fulkerson) (Max-flow min-cut theorem)}
\label{sec:org15fccf1}
Let \(f\) be a flow of max value and \([Q, \bar{Q}]\) a source-sink cut of
minimum capacity, then the \(\val(f) = \operatorname{cap}([Q, \bar{Q}])\).
\section{Lecture 11 \textit{<2018-11-20 Tue>}}
\label{sec:orge8564c0}
\subsection{Network}
\label{sec:orgbadee98}
\(D\) is a directed multigraph. \(s = V(D)\), the source vertex and \(t \in V(D)\)
the sink vertex and \(c\colon E(D) \rightarrow \R_{\ge 0}\) capacity
function.

\textbf{Flow} is a function \(f\colon E(D) \rightarrow \R\)

Flow is function \(E(D) \rightarrow \R\). For feasible flow
\begin{enumerate}
\item For every \(v\) that is not \(s\) and \(t\), \(\sum_{(v,u) \in E} f(v, u) =
      f^{+}(v)\)
\item \(\forall e \in E\), \(0 \le f(e) \le c(e)\). The value of the flow is
whatever goes into the sink \(\val(f) = f^{-}(t) - f^{+}(t)\).
\end{enumerate}

The \textbf{maximum flow} is a feasible flow of maximum value.
\subsection{Weak duality}
\label{sec:org5c6869f}
For every feasible flow \(f\) and source sink cut \([S, \bar{S}]\), the value of
the \(f \le \operatorname{cap}(S, \bar{S})\), in particular, the equality happens if \(f\) is a
maximum flow and \([S, \bar{S}]\) is a min-cut.

\([S, \bar{S}] = \{(u, v) \in E(D) \colon u \in S, v \in \bar{S}\}\) is a
source sink cut if \(s\in S\) and \(t\in \bar{S}\).

\([S, \bar{S}]\) is a min cut if its capacity \(\operatorname{cap}(S, \bar{S}) = \sum_{u \in
   S, v \in \bar{S}} c(u, v)\) is minimum among source/sink cut.
\subsection{Max Flow Min cut theorem (Ford-Fulkerson, 1956)}
\label{sec:org230104f}
Let \((D, s, t, c)\) be a network. Let \(f_{\max}\) be a max flow on \(D\) and
\([S_{\min}, \bar{S}_{\min}]\) a min cut, then \(\val(f) = \operatorname{cap} (S_{\min},
   \bar{S}_{\min})\).

\href{https://en.wikipedia.org/wiki/Ford\%E2\%80\%93Fulkerson\_algorithm}{Wikipedia}
\subsubsection{Proof}
\label{sec:org34a337d}
\(\le\) direction is just the weak duality.

\(\ge\) direction. We will find a source sink cut such that the capacity of
the source sink cut is equal to the value of the maximal flow.

That is, \(\operatorname{cap}(S, \bar{S}) = \val(f_\max)\).

\(S = \{v \in V(D) \colon \exist f \textup{augmenting path from s to v}\}\)

Observation \(t \in \bar{S}\) and \(s \in S\). This is because if there is a \(f\)
augmenting path from \(s\) to \(t\), then \(f\) is not maximum, but we know that
\(f\) is maximum, a contradiction.\footnote{An \(f\) augmenting path by definition means from source to sink. Our
definition here means the same definition except that the end vertex need not be
the sink.}

\(t\in \bar{S}\), \(s\in S\),
\begin{enumerate}
\item \([S, \bar{S}]\) is a source/sink cut.
\item \((u, v) \in [S, \bar{S}]\), \(f(u,v) = c(u, v)\). \((u, v) \in [\bar{S}, S]\)
implies that \(f(u, v) = 0\). Because if not, we can add them to \(S\).
\end{enumerate}

Now we see that the capacity of the cut \(\operatorname{cap}(S, \bar{S}) = \sum c(u, v) =
    \sum_{(u, v) \in [S, \bar{S}] f(u, v) - \sum_{(u, v) \in [\bar{S}, S]} f(u,
    v) = \val(f_\max)\) the last statement is a homework problem.
\subsection{Augmenting path}
\label{sec:org80da31a}
An \(s, t\) path \(s = v_0e_1v_1e_2v_2 \cdots v_{k-1}e_k v_{k} = t\) in the
underlying undirected graph \(G\) of a network \(D\). This is called an
\$f\$-augmenting path if for every \(i\),
\begin{enumerate}
\item \(f(e_i) < c(e_i)\) is true if \(e_i\) is a "forward" edge.
\item or \(f(e_i) > 0\) if \(e_i\) is a backward edge.
\end{enumerate}

The tolerance of a \(f\) augmenting path \(P\) is just the minimum of the values
\(\min \{E(e) \colon e \in E(P)\}\) where \(E(e) =\), \(c(e) - f(e)\) if \(e\) is
forwards and \(f(e)\) when \(e\) is backward.

If we define a \(f\) augmenting path, we can improve the value of the path. You
can improve the value of the path by the tolerance of the path.
\subsection{Lemma}
\label{sec:org111569d}
Take a feasible flow and an \(f\) augmenting path with tolerance \(z\), then we
define a new flow \(f'\) on the edges such that \(f'(e) = f(e) + z\) if \(e\) is
forward in \(P\) and \(f(e) - z\) if \(e\) is a backward edge. And if the edge is
not on the path, you do nothing.

Then \(f'\) is feasible and the \(\val(f') = \val(f) + z\).
\subsubsection{Proof}
\label{sec:org4ce1727}
Capacity constraints hold by the definition of \(z\).

Conservation constraints holds because: if we have a vertex \(v\in V(P)\),
then.

We had four cases and four diagrams. Basically given a vertex, there is one
vertex leaves the vertex and one vertex that comes to \(z\). We can argue for
each of these cases.
\subsection{Corollary}
\label{sec:org3b7e9aa}
If there exist an \(f\) augmenting path implies that \(f\) is not maximum.
\subsection{Local Vertex Menger's theorem}
\label{sec:org1d072f2}
For all \(x, y \in V(G)\), \(xy\notin E(G)\), \(\kappa(x, y) = \lambda(x, y) =
   \max\{\vert P \vert \colon P\) is a set of pairwise internally disjoint \(x, y\)
path \(\}\).
\subsubsection{Proof}
\label{sec:org8df2dc0}
The idea is to build a network so that the Ford-Fulkerson theorem implies
it.

We choose the following network: \((D, x^{+}, y^{-}, c)\),

\(V(D) = \{v^{-1}, v^{+}, v\in V(G)\}\),

\(E(D) =\{(u^{+}, v^{-} \colon uv \in E(G)\} \operatorname{cap} \{(v^{-1}, v^{+} \colon v
    \in V(G)\}\).

\(c(v^{-}, v^{+}) = 1\) for all \(v \in V(G)\), and \(c(u^{+}, v^{-}) = \infty\).

If there is an \(xy\) separating set in \(G\), this corresponds to a source sink
cut in \(D\). If \(C\) is an \(xy\) separating set, i.e., \(C\) contains \(x\) and
\(\bar{C}\) contains \(y\). We'll first show that the value of minimum cut is
exactly equal to \(\kappa(x, y)\).

\(s = \{v^-, v^{+}, v\in Q\} \operatorname{cap} \{u^{-1}\colon u \in C\} \subset V(D)\) and
\(\operatorname{cap} (S, \bar{S}) = \vert C \vert\). If \(v \in S\), there exist no
\(v^{+}w^{-}\) edge to \(w \notin S \operatorname{cap} Q\).

Minimum cut \([S, \bar{S}] \le K(x, y)\)

\(\ge\) take a minimum cut in \(D\). There does not exist an edge \(v^{+}u^{-}
    \in [S, \bar{S}]\).

The set \(C = \{u \in V(G) \colon (u^{-1}, u^{+}) \in [S, \bar{S}]\}\) is an
\(x, y\) separating set in \(G\) and the size \(\vert C \vert = \operatorname{cap}(S,
    \bar{S})\).

\(\lambda(x, y) \le \operatorname{maxval}(f) = \mincap(S, \bar{S}) =
    \kappa(x, y)\) If you have a family of internally disjoint, this in the
network correspond to several flows in the network. You definitely could
create a flow.

Actually this part is kinda obvious. We need to come up with an algorithm
that produces internally disjoint paths for max capacity. We can get this
from the Ford-Fulkerson algorithm.
\section{Lecture 12 \textit{<2018-11-21 Wed>}}
\label{sec:orgc9fc261}
\subsection{LMVT proof}
\label{sec:org058234e}
\(V(D) = \{u^{+}, u^{-}, u \in V(D)\}\)

\(E(D) = \{u^{+}, v^{-1}, uv \in E(G)\} \cup \{(v^{-1}, v^{+}\colon v\in
   V(G)\}\)

\(c(v^{-}, v^{+} =1\) for all \(v \in V(D)\).

\(c(u^+, v^-) = \infty\) for all \(uv \in E(D)\)

\((D, x^+, y^-, c)\)

The maximum value of a feasible flow is the minimum capacity \((S, \bar{S}) =
   \kappa(x, y)\).
\subsection{Ford Fulkerson algorithm}
\label{sec:orgb9f9933}
\textbf{Initialization}: Network \((D, s, t, c)\). Choose an initial flow \(f \eq 0\).

\textbf{Iteration}: Look for augmenting paths to improve the algorithm. Explore
 network for \(f\) augmenting paths starting from \(s\) (can be done using BFS.)
 Collect vertices reached in set \(S\).

Once you're done we have two cases. You didn't have an augmenting path which
implies that we have reached a maximum. If there is \(t\in S\), return
augmenting path and improve the flow.

If \(t\in S\), return augmenting path and an improved flow.

else return \(f\) as max flow and \([S, \bar{S}]\) as min cut with \(\val(t) =
    \operatorname{cap}(S, \bar{S}\)

\subsubsection{Integrability theorem}
\label{sec:orgdca4f77}
If \(c\) is a function from the edges to the natural numbers, then the
Ford-Fulkerson algorithm terminates with a flow with integer values.
\begin{enumerate}
\item Proof
\label{sec:org2643eb2}
Proof is by induction on the number of times the flow has been improved.
And noting that the tolerance of any \(f\) augmenting path is integer since
both are integers and hence the improved flow will also be an integer.

The algorithm hence terminates in at most maximum of the capacity steps.
\end{enumerate}
\subsubsection{Corollary}
\label{sec:orga21eb9e}
If the capacities are integers, then there exists a maximum flow that can be
represented as the flow of \(\val(f)\) many unit flows \(f = g_1 + g_2 +
    \cdots + g_\val(f)\) with \(\val g_i = 1\).
\subsubsection{Continuation}
\label{sec:org723dfe4}
By the IT theorem there exist flows \(g_1, \cdots, g_k\) from \(x^+\) to \(y^-\)
each of these are the form \(x^+v_{i,1}^-v_{i,1}^+v_{i, 2}^+, \cdots,
    v_{i1,l_1}^-v_{i1,l_1}^+y^-\)

Hence the \(xy\) path \(xv_{i, 1}v_{i, 2} \cdots v_{i, e_1}y\) in \(G\) are
pairwise internally disjoint paths.
\subsection{Partitioning triples}
\label{sec:orga3654a0}
We want to partition \(\binom{n}{3}\) triples into "perfect matchings" that are
pairwise disjoint. You need to have \(3 \vert n\)

This number will turn out to be \(\frac{n}{3} \cdot \frac{(n-1)(n-3)}{2}\).

For \(6\) vertices, we could take the set and the complement. Which is just
\(\binom63\).\footnote{I don't understand this. Shouldn't we multiply this by some factorial?
\textbf{Answer} the key is pairwise disjoint.}
\subsection{General version of Partitioning}
\label{sec:org8709329}
Let \(k, n\) integers and \(k\vert n\). Is it possible to partition the family
into \(\binom{n}{k}\) sets into pairwise disjoint perfect matchings. Perfect
matching is a set of \(n/k\) pairwise disjoint edges.

The answer is yes for every \(k\).
\subsubsection{Definition}
\label{sec:org886e914}
\(m = n/k\) is the number of sets in the perfect matching.

\(M = \binom{n}{k}/(n/k) = \binom{n-1}{k-1}\) The number of perfect matchings needed

Given an integer \(1\le l \le n\) a \(m\) partition of \([l]\) is a multi-set \(r
    = \{A_1, \cdots, A_m\}\) consisting of pairwise disjoint sets whose union is
\([l]\). (\footnote{Repetition is allowed only for \(\emptyset\). Because sets are disjoint.})
\subsubsection{Remark}
\label{sec:org8a76acf}
Restriction of a perfect matching to \([l]\) is a \(m\) partition.
\subsubsection{Proposition}
\label{sec:org71f1bbb}
For every \(l\) there exists \(M\), \(m\) partitions of \([l]\), called \(A_1,
    \cdots, A_2, \cdots, A_M\) such that for every subset of \([l]\), occurs in
exactly \(\binom{n-l}{k-\vert S\vert}\) of the \(m\) partitions. (The empty set is always
counted with multiplicity)
\subsubsection{Remarks}
\label{sec:org04a2445}
\begin{enumerate}
\item If \(\vert S \vert > k\), the statement says it occurs \(0\) times. Only \(\vert
       S \vert \le k\) occurs.
\item \(l=n\), There exists \(M\), \(m\) partitions \(=0\) and \(1\) when \(k = \vert S
       \vert\). \(M\), \(m\) partitions in which every \(k\) occurs exactly once and
nothing else. This means that there exists a partition \(A_1, \cdots, A_M\)
are perfect matchings consisting of \(k\) sets that partition
\(\binom{n}{k}\). Thus the theorem is proved.
\end{enumerate}
\subsubsection{Proof of Proposition}
\label{sec:orgc279dbe}
When \(l = 1\), \(A_i = \{\{1\}, \emptyset, \emptyset, \cdots, \emptyset\}\)
(the empty set occurs for \(m-1\) number of times.)

If \(s = \{1\}\), then \(s\) occurs \(M\) times. \(\binom{n-l}{k-s} =
    \binom{n-1}{k-1} = M\)

When \(s\) is the empty set. Then \(S\) should occur \(\binom{n-1}{k-0} =
    \binom{n-1}(k}\). Count: \(M(m-1) = \binom{n-1}{k-1}(n/k - 1)\) this is true as
well. I missed the rest of the proof.
\section{Lecture 13 \textit{<2018-11-27 Tue>}}
\label{sec:orgceef4ed}
\subsection{Colorings of graphs}
\label{sec:org4ca822b}
\subsubsection{Example}
\label{sec:orgf316676}
100 employees and \(6\) projects. They have meetings. The company tries to
schedule the meetings. Some employees are in different projects. So you
cannot schedule them at the same time.

The projects: \(A_1, A_2, \cdots, A_6 \subset S\)

Schedule: \(A_i \cap A_j \neq \emptyset \implies\), meeting times are
different from \(A_j\). Minimize the number of hours your teams spend with
meetings.

We form a graph with \(A_i\) and form edges when \(A_i \cap A_j \neq
    \emptyset\). We want to color the graphs so that the adjacent vertices have
different color.
\subsection{Definition}
\label{sec:org59c5540}
\(X \colon V(G)\rightarrow [s]\) is a proper vertex colouring of \(G\) if
\(\forall uv \in E(G)\), \(X(u) \neq X(v)\).

\(G\) is \(k\) colorable if there is a \(k\) coloring of it.

The chromatic number is the minimum number of colors that can make it
colorable.
\subsection{Complexity}
\label{sec:org71471aa}
Is this graph \(k\) colorable? A decision problem.

For \(k=2\), it is equivalent to being bipartite. And is in P for this case.

For \(k \ge 3\), the problem is NP-complete. It is easy to show that the
problem is in NP.

It is not know that the problem is in Co-NP.
\subsection{Giving lower bounds for chromatic number}
\label{sec:org0bff344}
\(X(G) \ge \omega(G)\). This is an obvious bound.
\subsection{Proposition}
\label{sec:org01668fd}
\(X(G) \ge \frac{v(G)}{\alpha(G)}\)
\subsubsection{Proof}
\label{sec:org972b249}
Was too quick
\subsection{Tightness of \(X(G)\) and \(\omega(G)\)}
\label{sec:org86ac285}
Cliques, Bipartite graph (and complements), perfect graphs.
\subsection{Non-tightness \(X(G) > \omega(G)\)}
\label{sec:orgfad100e}
A simple example is odd cycle. Also for their complements.

\href{https://en.wikipedia.org/wiki/Mycielskian}{Wikipedia}: Mycielskian construction. \(\omega(G) = 2\), but \(X(G) > 10^10\).

Random graphs: \(G = G(n, 1/2)\) and \(\omega(G) = 2\log n\) and \(X(G) = n/(2
   \log n)\). (This wasn't proven.) The result shows that the last proposition is
asymptotically tight.
\subsection{Hajos conjecture}
\label{sec:orgf7adda7}
\href{https://en.wikipedia.org/wiki/Haj\%C3\%B3s\_construction}{Wikipedia} There exists \(K_r\) subdivisions \(\Leftimplies X(G) \ge r\).

True for \(r = 2, 3\). For 3, we have an odd cycle.
\subsection{Dirac theorem}
\label{sec:orgfc0bddc}
Hajos conjecture is true for \(k=4\).
\subsubsection{Proof}
\label{sec:org363869e}
We'll use induction on \(n\), the number of vertices.

Base case: \(n=4\), the smallest \(n\) for which the graph is \(3\) colorable.
\(K_4\) is the only graph on \(4\) vertices that is non 3 colorable.

\(n>4\), we can without loss of generality, assume that \(G\) is \(4\) critical.
Meaning that if you remove an edge or vertex will make it \(3\) colorable. The
chromatic number \(X(H) \le 3\) and \(X(G) = 4\). We'll show that this will be
\(K_4\) divisible.

Case 0: \(\kappa(G) = 0\). Take a 4 chromatic component subgraph which is proper.

Case 1: \(\kappa(G) = 1\). You have a cut vertex. Then \(G\) is not \(4\)
critical. This is because you can color the lobes with \(3\) colors and name
the color of \(v\) in each \(1\). Then the union of these colorings is a proper
\(3\) colouring of \(G\).

Case 2: \(\kappa(G) = 2\). Take a cut set \(S \subset V, \vert S \vert = 2\).
Call \(S = \{x, y\}\). Color each \(S\) lobe with \(3\) colors, rename colors in
each lobe such that \(x\) is colored the same in each lobe and \(y\) is colored
the same in each lobe. But this would still have a problem. When is this
possible? An example is when \(x\) and \(y\) have an edge between them.

Case 2b: \(xy \notin E(G)\) for all \(S\) lobe there exists a proper \(3\)
coloring \(x\) and \(y\) have different colors. If there is an \(S\) lobe with no
proper \(3\) coloring with \(x\) and \(y\) have different color. This implies that
\(X(H + xy) = 4\) (\(\iff \forall\) 3 coloring of \(H\), \(X(x) = X(y)\))

There exists \(K_4\) subdivisions \(L \subset H + xy\) if \(xy \notin E(L)\), \(L
    \subset H \subset G\) if \(xy \in E(L)\). If \(xy \in E(L)\), create \(K_4\)
subdivisions \(L^{*}\) from \(L\) by deleting and replacing it with an \(x, y\)
path in a S-lobe different from \(H\).

Case 3: \(K(G) \ge 3\). \(G-x\) is \(2\) connected, implies that there is a cycle in
it (Mengers theorem), A cycle is a \(K_3\) subdivision. We need to somehow get
\(K_4\) subdivision.

We add a new vertex \(v\) and connect three vertices on this cycle. Now the
graph is \(3\) connected. There are three pairwise internally disjoint paths
between \(x\) and \(v\). This can be used to construct a \(K_4\) subdivision.

A lemma: If \(G\) is \(k\) connected and \(G^{*}\) is an extra vertex plus edges
to \(k\) vertices of \(G\), then \(G^{*}\) is also \(k\) connected.
\subsection{Counter examples of Hajos theorem for \(k\ge 7\)}
\label{sec:orgdd32f9d}
Still open for \(k = 5, 6\).
\subsection{Hadwiger Conjecture}
\label{sec:orgde93389}
\(X(G) \ge k\) implies that \(K_r\) minor.
\section{Lecture 14 \textit{<2018-11-27 Tue>}}
\label{sec:orgd899fe2}
\subsection{Proposition}
\label{sec:orge57b507}
For all \(X(G) \le \Delta(G) + 1\)
\subsubsection{Proof}
\label{sec:orgb6e2446}
A greedy algorithm.

Input: ordering of the vertices, \(v_1, \cdots, v_n\) of \(V(G)\)

We color \(v_i\) with the smallest available color. Basically take the
element, look at the neighbours, and assign the element a color that is the
minimum of number that doesn't clash with neighbours.

Claim: The greedy algorithm produces a proper coloring of \(G\) for every
ordering of \(V(G)\).

The number of colors of the greedy algorithm uses is not more than the
\(\Delta(G) + 1\).
\subsubsection{Characterize the extremal cases}
\label{sec:org53bbd2a}
\subsection{Theorem (Brooks)}
\label{sec:org74787d7}
\(G\) is connected, then \(X(G) = \Delta(G) + 1 \iff G = K_n or C_{2l+1}\)
\subsubsection{Proof}
\label{sec:org25b48cd}
One direction is obvious (\(\Leftarrow\))

Approach: Try to Color with greedy with different orderings and conclude
that the only orderings we can't do it is with \(\Delta\) colours is when \(G =
    K_n\) and \(C_{2l+1}\).

Idea: We take a spanning tree with root \(r\). From the spanning tree, create
an ordering \(\pi_T\) of \(V(G)\) such that we order from leaf to root. With the
property so that every vertex precedes all vertices on the unique path in
\(T\) from \(u\) to \(r\).

We can see that \(G\) is regular (meaning that every vertex have the same
number of neighbours)

run the greedy algorithm with \(\pi_t\), then \(\max \vert \{\cdots \}\vert \le
    \max \{\max\{d(v_{\pi_t(i)}) - 1\}, d(v_{\pi_t(u)})\}\)

For similar reason, we can say that \(G\) has no cut vertex. Otherwise, if \(x\)
is a cut-vertex, then, take a spanning tree with root \(x\) and look at the
lobes \(S_1, \cdots, S_l\). Color each lobe greedily with \(x\) as the root.
This is possible with at most \(\Delta(G)\) colors, since the degree of
\(d_s(x) \le \Delta(s) - 1\). Then put together all the colors.

So \(G\) must be at least \(2\) connected.

We are still in \(T\) ordering.

If there are two edges that \(v_1, v_2\) doesn't have an edge and \(G- \{v_1,
    v_2\}\) is not connected. If this is a strongly connected graph, then we will
be able to succeed. And then order them like \(v_1, v_2, v_3, \cdots, v_n\).
Where \(v_n\) is connected to \(v_1\) and \(v_2\). Then we can get a contradiction
doing the greedy coloring.

Take a vertex \(x\), \(d(x) < n-1\). Take a \(y\in N(x)\) such that \(z \in N(y)
    \setminus N(x)\), \(K(G - x) \ge 2\). This means that \(\kappa(G - x - y) \ge
    1\). This is a contradiction.

Case 2: \(\kappa(G - x) = 1\). We need a structure theorem. And we'll come
back to this.
\subsection{Definition}
\label{sec:org5324374}
Given a graph \(G\), a block is a set of vertices \(B \subset G\), is a maximal
connected subgraph of \(G\) without a cut vertex. Means that \(B\) itself is \(2\)
connected and for every \(B'\), such that \(B \subset B' \subset G\), \(B'\) is not
\(2\) connected.
\subsection{Lemma}
\label{sec:orgcd92b87}
\(B_1, B_2\) are blocks of \(G\), then \(\vert V(B_1) \cap V(B_2) \vert \le 1\).
\subsubsection{Proof}
\label{sec:orgb65c13d}
If \(\vert V(B_1) \cap V(B_2) \ge 2\), \(B_1\) and \(B_2\) are \(2\) connected,
then, \(x\) and \(y\) is indeed a section \(V(B_1) \cap V(B_2)\), \(x \neq y\). We
want \(z_1, z_2 \in V(B_1) \cap V(B_2)\) cycle \(z_1, z_2 \in C\). They are
disjoint (the lemma that we used yesterday.) This means that \(B_1 \cap B_2\)
are \(2\) connected and we have a contradiction
\subsection{Some structure}
\label{sec:org7ba9182}
The block/cut-vertex graph of \(G\) is a bipartite graph with vertex set \(\{B
   \subset G\colon B \textup{ is a block}\} \cup \{v \in V(G) \colon v \textup{
   is a cut vertex}\}\)

Block \(B\) is adjacent to cut-vertex \(v\) if \(v \in V(B)\)
\subsection{Proposition}
\label{sec:orgd2c0583}
The block/cut-vertex graph of a connected graph is a tree.
\subsubsection{Proof}
\label{sec:orgdb1bd4e}
Connectedness follows from the connectedness of \(G\). Every vertex is
contained in at least one block.

Acylicity? If there is a cycle of blocks, then take a shortest cycle and now
create a cycle through each of these paths and get a larger block.

Take shortest cycle \(B_1 v_1 B_2v_2 \cdots B_lv_l \implies B_1 \cap \cdots
    B_l\) is also \(2\) connected, because pair of vertices are on a cycle. Now a
contradiction to the maximality of \(B_i\).
\subsection{Going back}
\label{sec:org908b1d5}
We were doing \(K(G - x) = 1\).

Now use the structure theorem to \(K(G-x)\). Adding back \(x\), \(x\) must have a
neighbour in each of the leaf block that is not a cut vertex. Otherwise, \(G\)
is not \(2\) connected. There exists at least \(2\) leaf blocks.

We can pick \(v_1, v_2\) in two different leaf block. If we remove \(v_1\) and
\(v_2\), \(G-x\) should remain connected.

If \(G \neq G\), \(d(x) \ge 3\), so \(G - v_1 - v_2\) is connected.
\section{Lecture 15 \textit{<2018-12-05 Wed>}}
\label{sec:org2a22dfa}
\subsection{Formalizing a certain graph coloring}
\label{sec:org4f2bd23}
Given a graph \(G\), an assignment of subsets (lists) to the vertices \(L \colon
   V(G) \rightarrow 2^\N = P(\N)\), then \(f\colon V(G) \rightarrow \N\) is a
proper \(L\) list coloring with respect to \(L\) if for every \(v \in V(G)\), the
color \(c(v) \in L(v)\) (choose color from representative list) and \(\forall xy
   \in E(G), c(x) \neq c(y)\) (proper)
\subsection{Remark}
\label{sec:org066b057}
Usual proper coloring is just a list coloring with \(L(v) = [k]\).

Graph \(G\) is \(k\) list colorable (k-choosable) if \(\forall L \colon V(G)
   \mapsto \binom\N k\), there is a proper list coloring with respect to \(L\).

The parameter that we're looking for is \(\chi_l(G) = \min \{k \colon G
   \textup{is k-colorable}\}\).
\subsection{Claim}
\label{sec:org82b6abb}
\(\chi_l(G) \ge \chi(G)\)
\subsubsection{Proof}
\label{sec:org931ec66}
If a graph is \(k\) list colorable, then it is also \(k\) colorable.

We have already seen that for \(\chi_l(K_{3, 3}) > \chi (K_{3, 3}) = 2\). How
did we prove this? We gave certain lists with \(2\) elements. But \(K_{3, 3}\)
is not properly list colorable with respect to \(L\).
\subsection{Example}
\label{sec:orge02661d}
\(\chi_l(K_{2, 2}) = 2 = \chi(K_{2, 2})\).

We know that it is at least \(2\), by previous claim. To show the other way, we
need some stupid case analysis.
\subsection{Proposition}
\label{sec:orgc43a1b8}
\(\chi_l(C) \le \Delta(G) + 1\), the proof is identical and uses greedy
coloring algorithm with arbitrary order.
\subsubsection{Proof}
\label{sec:org70b7458}
Take an arbitrary assignment \(L \colon V(G) \rightarrow \binom\N{\Delta +
    1}\), run the greedy algorithm. In round \(i\), we color \(v_i\) and look at the
neighbours. Since the degree is at most \(\Delta(G)\), we'll be able to assign
a color from \(\{1, \cdots, \Delta + 1\}\).
\subsection{Complexity}
\label{sec:orgd7e1b79}
It's not known to be in NP or in co-NP.
\subsection{Proposition}
\label{sec:orgdec42e2}
For every \(k, \chi_l(K_{m, m}) > k\), where \(m = \binom{2k-1}{k}\)

We know that the regular coloring of this should be \(2\), but the list
coloring can be large.
\subsubsection{Proof}
\label{sec:org693eea5}
It is similar to how we did \(K_{3, 3}\).

For each vertex, we assign it a \(k\) subset of \(2k-1\). We can assign each \(m\)
in the top and bottom uniquely in this way.

We look at the colors on the bottom vertices and on the top vertices.
Because it is a complete graph, these should be disjoint. This means that
one of the top or bottom should have colors of size \(< k\).

Let's say \(C_x\) and \(C_y\) are the top and bottom vertices.

Say \(\vert C_x \vert \le k - 1\). Take a list \(L(v_0) \cap C_x \neq
    \emptyset\), but this is a contradiction. This is a contradiction.
\subsection{4 color theorem}
\label{sec:orgbd7ecc5}
If \(P\) is a planar graph, then one can color it with \(4\) colors. Proof used
computers.
\subsection{5 color theorem}
\label{sec:org9f97378}
A planar graph has 5 coloring. What about list coloring? Can planar graphs be
colored by \(4, 5, 6\) colors?

It was discovered that there were planar graphs that were not \(4\) list
colorable.

A Mirzakhani graph is an example of a planar graph with \(\chi_l(G) \ge 5\).
The graph wasn't the first example, but a simple one.
\subsection{Theorem (Thomasson)}
\label{sec:orgd13c9b6}
Every planar graph is \(5\) list colorable.
\subsubsection{Proof using induction}
\label{sec:orgd32e71d}
Stronger statement: If \(G\) is a plane graph, with an outer face bounded by a
cycle \(C\) and \(L \colon V(G) \rightarrow 2^\N\) is a list assignment such
that
\begin{enumerate}
\item \(\exists v_1v_2 \in E(C)\), \(\vert L(v_1) \vert = \vertL(v_2)\vert = 1\),
\(L(v_1) \cap L(v_2) = \emptyset\).
\item \(\forall v \in V(C) \setminus\{v_1, v_2\}\), \(\vert L(v)\vert = 3\).
\item \(\forall v \in V(G) \setminus V(C)\), \(\vert L(v)\vert = 5\)
\end{enumerate}

Then \(G\) is list colorable.

Embed \(G\) into the plane without crossing and if the boundary of the outside
face is not a cycle, then add edges until it is.

We use induction on our stronger statement.

The base case if \(V(G) = 3\). The graph would be a triangle. For \(v_1\) and
\(v_2\), we have fixed colors, and \(v_3\) is a list of size \(3\), and you'll
always have an available color.

\textbf{Case 1}: There exists a chord \(xy \in E(G)\) of the cycle \(C\). \(x\) and \(y \in
    V(C)\) but \(xy \notin E(C)\). \(v(C_1) \cap v(C_2) = \{x, y\}\). Say \(v_1, v_2
    \in V(C_1)\). List color \(C_1\) with respect to \(L\) by induction.

There are two sides \(C_1\) and \(C_2\). Use induction to color \(C_1\) and the
end vertex will have a color. Now color \(C_2\) and we're done. It is
important that there is no edge between \(C_1\) and \(C_2\).

\textbf{Case 2}: There is no chord between two vertices from the top to bottom.
 Let's add edges to make a triangulation, except the outside faces. (we can
 do this at the beginning.)

The idea is that we'll use induction on the graph \(G-\{v_3\}\). Here \(v_3\)
next to \(v_2\) in the outer circle.

Let \(x, y \in L(v_3) \setminus L(v_t)\), for every vertex \(v_t\) that is a
neighbour of \(v_3\), but, not on the cycle. \(L^{*}(v) = L(v) \setminus\{x,
     y\}\), and now color by induction. Extend \(c(v_3)\), choosing a color \(x\) or
\(y\), which is not equal to the color of \(v_{3+1}\).
\section{Lecture 16 \textit{<2018-12-11 Tue>}}
\label{sec:orge35d08d}
\subsection{Definition}
\label{sec:org2804d7b}
A function \(c\colon E(G) \rightarrow [k]\) is called a \(k\) coloring. \(f\) is
proper if incident edges get different colors.

A multi graph is \(k\) edge colorable if there exists a proper \(k\) edge
coloring of \(G\), \(\chi'(G) = \min \{k \colon G \textup{ is k edge
   colorable}\}\).
\subsection{Examples}
\label{sec:org51544fc}
\subsubsection{Clique}
\label{sec:orgc43e654}
For edge, it's 1. For a triangle, it is \(2\). For \(K_4\), it is \(3\).

For \(K_5\), we can see that each coloring forms a matching. So \(K_5\) can't
have \(K' = 4\), since there are no perfect matchings.

General case, when \(n\) is odd, \(X'(G) = n\), and when \(n\) is even it is
\(n-1\).
\subsection{Claim}
\label{sec:orge4c99d7}
\(\chi'(G) \ge \Delta(G)\).
\subsubsection{Proof}
\label{sec:org8f04c9c}
At the vertex with max degree, there are \(\Delta(G)\) number of edges, each
must receive a proper edge coloring.
\subsubsection{Remark}
\label{sec:org7bef90b}
When \(n\) is odd, \(n-1\) edge coloring would form a perfect matching. But this
is not possible.

\(K_n \subset K_{n+1}\). The inclusion is monotone.

\(\chi'(K_n) \ge n\).

\(\chi'(K_{n+1}) = n\).
\subsubsection{Observation}
\label{sec:org1614e42}
Any color class of a proper edge coloring is a matching. In particular, if
\(X'(G) = d\) for a \(d\) regular graph, then \(d\) has a perfect matching, then
\(G\) has a perfect matching. This is because at a vertex, all colors appear.
So each color class is a perfect matching. (Even more, the edge set is
partitioned into a perfect matching.)
\subsubsection{Peterson}
\label{sec:org10da43c}
If \(X'(P) = 3\), if and only if \(\exists\) a perfect matching, \(M\) such that
\(P - M\) has \(X'(P-m) = 2\). Implies that \(P-M\) is the disjoint union of even
cycles.

What do we know about the cycles of peterson graph, the minimum size is \(5\).
It cannot have \(6 + 4\) cycle. It does not have \(8+2\) cycle either.

What about \(C_{10}\). It cannot have a \(10\) cycle, because we know that the
Peterson graph has no Hamilton cycle.

The chromatic number is \(4\).
\subsection{Theorem (Konig)}
\label{sec:org9534d81}
If \(G\) is a bipartite Multi-graph, then \(X'(G) = \Delta(G)\).
\subsubsection{Proof}
\label{sec:org3ef5864}
What do we know about regular bipartite graph? We have a perfect matching
(Frobenius theorem.)

If we have this, we color it and remove it, now we have a \(d-1\) regular
graph, we again color it back.

Given a bipartite multi graph, we make it regular.

Add new vertices to the smaller part if necessary (with fewer vertices.)

Add edges to vertices with degree less than \(\Delta(G)\) until there is no
more. This way, you create a bipartite supergraph \(H \supset G\).

We cannot get stuck because, if there is a vertex in \(A\) such that the
degree of it is less than \(\Delta(G)\), then there is another one in the
other part with degree less than \(\Delta(G)\). Why? Because the sum of the
degrees in one part is same as that of the sum of the degrees in the second
part.

Now, just color it using the regular coloring.
\subsection{Vizing's theorem}
\label{sec:org653ed19}
For every \textbf{simple} graph, \(G\), \(\chi'(G) \le \Delta(G) + 1\).
\subsubsection{Remark}
\label{sec:org4580a16}
\begin{enumerate}
\item For most graphs, \(X'(G) = \Delta(G)\).
\item Can we decide if a graph is \(\Delta(G)\) or in the \(\Delta(G) + 1\)? This
is NP-complete.
\end{enumerate}
\subsubsection{More remarks}
\label{sec:org4ff5a89}
The line graph is just the edge set. The edge set of the line graph is those
pairs of edges such that they intersect. \footnote{Now, it should follow from the greedy vertex coloring of the graph,
provided we prove that \(\Delta\) of the line graph is same as the original
graph.}

There is a one-one correspondence between a proper coloring of \(L(G)\) and
proper edge coloring of \(G\).

\(\chi(G) \ge \omega(L(G)) = \Delta(L(G)) \le \Delta(L(G)) + 1 \le
    2(\Delta(G) - 1) = 2\Delta(G) -1\) (more or less true, except for \(K_3\)) Here
\(\omega\) is the clique number.

What is the maximum degree?

Any clique of \(L(G)\) corresponds to a \(K_{1, t}\) or a \(K_3\) in \(G\).
\subsubsection{Proof}
\label{sec:org1e51d25}
Proof using induction. Induction on \(V(G)\), \(V(G) - 1\). Take a vertex \(v =
    V(G)\), there is a proper \(\Delta(G) + 1\) coloring of \(G-v\). Extend to \(v\) to
form a proper \(\Delta(G) + 1\) coloring.

\textbf{Stronger statement}: \(\forall\) graph \(G\) and any integer \(k \in \N\), such
that

\begin{enumerate}
\item there is a vertex \(v\in V(G)\) such that the degree of \(d(v) \le k\).
\item And for every \(u \in N(v)\), \(d(u) \le k\)
\item There exists at most one neighbour \(w \in N(v)\), \(d(w) = k\).
\end{enumerate}

If \(G-v\) is \(k\) edge colorable, then \(G\) is \(k\) edge colorable.

For \(k=1\), it can be shown.

For \(k > 1\), take a \(k\) edge coloring where \(X_i = \{u \in N(v) \colon u
    \textup{ is missing i} \}\). There is no edge incident to \(u\) with color \(i\).

Without loss of generality, there does not exists \(w \in N(v), d(w) = k\) all
others have degree \(k-1\). You add vertices to make sure the second
condition.

We need to look at the sum \(\sum_{i=1}^{k} \vert X_i\vert\). The \(w\) is in
exactly one of the \(X_i\). So \(\sum_{i=1}^{k} \vert X_i\vert = 1 +
    2(d(v) - 1) = 2d(v) - 1\). So their average is less than \(2\).

\textbf{Case 1}: There is a color \(i\) such that \(\vert X_i \vert = 1\).

\(G' = G - w - \{xy \colon c(xy) = k\}\) and then apply induction on \(G'\) and
\(k-1\). This way you get a proper \(k-1\) edge coloring of \(G'\) and then color
remaining edges \(k\).

\textbf{Case 2}: When for every color \(\vert X_i \vert = 0\) or \(\ge 2\). But this
 will lead to a contradiction to the minimality.

There exists a color \(i\) such that \(\vert X_i \vert = 0\), (otherwise
contradiction to the inequality.)

There is a color \(j\) with \(\vert X_j \vert \ge 3\).

\(H\) is the subgraph spanned by \(i\) and \(j\). This is a contradiction to
minimality. (Sort of an involved argument.)
\section{Lecture 17 \textit{<2018-12-12 Wed>}}
\label{sec:orgc52d631}
\subsection{A general form of vizing's theorem}
\label{sec:org4ddfb03}
If \(G\) is a multi graph, then \(X'(G) \le \Delta(G) + \mu(G)\) where \(\mu\) is
the maximum multiplicity of \(G\). (Wasn't shown in the class)
\subsection{List coloring conjecture}
\label{sec:org041b388}
For arbitrary \(G\), \(X_l(G) - X(G)\) can be arbitrarily large.

The conjecture says, for every \(G\), \(\forall G\), \(X'(G) = X'_l(G)\). The one
on the right is the list coloring version of edge coloring.

Reformulation, for all \(G\), \(X(L(G)) = X_l(L(G))\). (We proved this for
cycles; a cycle is a line graph)
\subsection{Theorem (Kahn)}
\label{sec:org097d109}
\(X_l(G) = (1 + o(1))X(G)\).

This means that if we have a sequence of graphs \(G_n\) with \(X'(G_n)
   \rightarrow \infty\), \(X'_l(G)/X'(G) \rightarrow 1\). (Wasn't covered in the
lecture.)
\subsection{Theorem (Galvin)}
\label{sec:org065e82a}
If \(B\) is bipartite, then \(X'_l(B) = X'(B)\)

We'll prove the special case of complete bipartite graph (\(K_{n, n}\))
(Dinitz's conjecture.)
\subsubsection{Proof}
\label{sec:org1539b87}
\(X'(K_{n,n})\) it is \(n\). It is a bipartite graph (we learned last time that
we can regularize it and then do some stuff.)

Goal: For arbitrary assignment of lists of colors of size \(n\), there is a
selection of each color for each edge of \(K_{n, n}\) from its list such that
incident edges receive different colors.

(Look at the definition below)

The color class \(1\) in the greedy coloring with respect to some ordering of
a graph \(G\) is a kernel of the digraph created by oriented or directing
edges of \(G\) from right to left. (Because if there is no edge from a vertex
to this, this we would have colored it with \(1\))
\subsection{Definitions}
\label{sec:org15f50e1}
Given a directed graph \(D\), a \textbf{kernel} of \(D\) is a set \(S\) of the vertices

\begin{enumerate}
\item that is independent.
\item Every vertex \(v \in V\setminus S\) has a neighbour \(w \in S \cap N^{+}(v)\).
\end{enumerate}

A digraph is \textbf{kernel-perfect} if for all induced subdigraph, it has a kernel.

Let \(f\colon V(G) \rightarrow \N\), then a graph is called \(f\) choosable (or
list colorable) if for every family of lists, \(\{L_v\}_{v\in V(G)}\) with list
size at lest \(f(v)\) for every \(v\), there is a proper coloring \(c \colon V(G)
   \rightarrow \N\), \(c(v) \in L_v\).
\subsection{Lemma}
\label{sec:orga189d3c}
Let \(D\) be a kernel perfect orientation of some graph \(G\). Then \(G\) is \(f\)
choosable with \(f(v) = 1 + d^{+}_D(v)\).
\subsubsection{Proof}
\label{sec:orga77a3b1}
Orientation thing?

\sout{Take a kernel of the graph, \(S\), and color every vertex \(v\in S\), such that
\(c(v) \in L(v)\). It is possible since \(L(v) > 1\).}

\sout{Now, we delete this list from the graph, and in the other side, we modify
the lists.}

Take a color, 1, then \(W\) be all vertices such that \(1\) is in there. Now,
there is a kernel \(D\) inside this subgraph. Call it \(S\).

Color \(c(u) = 1\forlal u \in SE\)

Now, we do induction on \(D - S\) such that for all \(u\in W \setminus S\),
redefine \(L_u = L_u \setminus \{1\}\), for all \(u \in V \setminus W\) redefine
\(L_u = L_u\).

Now \(d^{+}_{D_s}(u) \le d^{+}_{D}(u) - 1\), for \(u\in W \setminus S\). Because
\(S\) is a kernel of \$D[W], because u had an edge going into \(S\) (and this
edge was removed with the deletion of \(S\).)

This implies that \(\vert L_u \vert \ge d^{+}_{D-S}(u) + 1\). This holds for
every \(u \in V(D) \setminus S\).

From induction, there is a coloring, proper, from the list. And there is no
contradiction with vertices colored by \(1\).
\subsection{About Kernel perfect orientation of \(L(K_{n, n})\)}
\label{sec:org28d0b17}
A kernel perfect orientation of \(L(K_{n, n})\) with \(\Delta^{+}(D) =
   n - 1\).

If by the lemma, then \(L(K_{n,n})\) is \(1 + n-1 = n\) choosable.

Note that \(L(K_{n, n})\) is \(2(n-1) = 2n-2\) regular graph.
\subsubsection{Claim 1}
\label{sec:orgaed18fd}
There is an orientation \(D\) of \(L(K_{n,n})\) such that \(\Delta^{+}(D) = n-1\)
and for any \(v \in V(K_{n, n})\), \(D[\{uw \colon w \in N(v)\}]\) is
transitive.

What is a transitive orientation?
\subsubsection{Claim 2}
\label{sec:org9331b94}
If \(D\) is an orientation from claim 1, then \(D\) is kernel perfect.
\subsubsection{Remark}
\label{sec:orga738fc2}
Notice that the above two claim and the lemma would give you the theorem.
\subsubsection{Proof of claim 1}
\label{sec:org10a9b81}
\(M = \{n_0, n_1, \cdots, m_{n-1}\}\)

\(W = \{w_0, w_1, \cdots, w_{n-1}\}\) are vertices of \(K_{n,n}\).

The edge \(m_iw_j \rightarrow m_{i'}w_j\) if \(i+j > i' + j (\mod n)\). (abuse of
notation) (Here the \(j\) is fixed.)

\(m_i w_j \rightarrow m_i w_{j'}\) if \(i + j < i + j' (\mod n)\), here j is
fixed.

This should clearly have the transitive property. An orientation is
transitive if \(a > b, b > c\), then \(a > c\). There is no cycle.

Indeed for every \(m_i w_j\), the out degree \(d^{+}(m_iw_j) = n-1\).
\section{Lecture 18 \textit{<2018-12-18 Tue>}}
\label{sec:org4d817cd}
\subsection{Lemma}
\label{sec:org8db859f}
If \(D\) is a kernel perfect orientation of a graph \(G\), then \(G\) is \(f\)
choosable with the following function \(f(v) = 1 + d_D^{+}(v)\) for all \(v \in
   V\).

Goal: There exists a kernel perfect orientation \(D\) of \(L(K_{n, n})\) with
\(\Delta^{+}(D) = n-1\).

We have a digraph \(D = (V, E)\), \(K \subset V(D)\) is a \textbf{kernel}
if \(K\) is independent and \(\forall v \in V \setminus K, \exists w \in K, uw
   \in E\)

\(D\) is \textbf{kernel} perfect if for all induced subgraph has a kernel.
\subsection{Remark}
\label{sec:org6b2036c}
Any ordering of vertices of graph \(G\) from right to left is kernel perfect.
\subsection{Observation}
\label{sec:org6471a8e}
An orientation of a complete graph is kernel perfect if and only if it is
transitive.

If \(D\) is a kernel perfect orientation of the line graph \(L(k_{n, n})\), then
\(\forall v \in V(K_{n, n})\), the clique \(\{vw\colon w \in N(u)\}\) must be
transitively oriented.

\textbf{Claim}: There exists orientation \(D\) of \(L(K_{n, n})\) such that
 \(\Delta^{+}(D) = u-1\).

\textbf{Claim}: If \(D\) is an orientation of \(L(K_{n, n})\) such that for every \(v
     \in V(K_{n, n})\), the restriction of \(D\) on \(\{vw \colon w \in N(v)\}\) is
 transitive, then it implies that \(D\) is Kernel perfect.\footnote{We've not done the full proof of this claim, I think.}
\subsection{About stable matchings}
\label{sec:org2d9d9b3}
We have two sets \(M = \{m_1, \cdots, m_n\}\) and \(W = \{w_1, \cdots, w_n\}\).
Each vertex has a preference of the other gender (a permutation of the other
list, \(\pi \in S_n\))
\subsubsection{Theorem (Gale-Shapley)}
\label{sec:orge915c9a}
For any preference lists of \(n\) man and \(n\) women, there exists a stable
matching \(S\).

A stable matching is a perfect matching of \(K\) and \(N\) with no unstable
pair. A vertex \(w\) and a vertex \(m\) forms an unstable pair if \(wm \notin S\),
\(w\) prefers \(m\) to her pair and \(m\) prefers \(w\) to his pair in \(S\).
\subsubsection{Proposal algorithm}
\label{sec:orgf3e9ad3}
\textbf{Input}: Preference ranking by men and women.

\textbf{Iteration}: in each iteration step, each man proposes to the women highest
on the list who has not previously rejected.

If each women receives one proposal, then STOP and return the resulting
matching.

Otherwise, every women receiving at least one proposal says, maybe to the
one highest on their list and rejects the others if there are more than
one.
\subsubsection{Theorem about Proposal algorithm}
\label{sec:org4ab4073}
The proposal algorithm produces a stable matching.
\begin{enumerate}
\item Proof
\label{sec:org94d76eb}
Algorithm terminates:

\begin{enumerate}
\item Combined length of lists of men decreases in each iteration.
\item No list of a man ever becomes empty. (So proposals can be made.) (If a
women rejects a man, she will keep receiving proposals from that point
on. So if a man is rejected by \(n\) women, then, all these women must
have proposals at the time of the last rejection. But these come from a
set of only \(n-1\) men, and this is a contradiction, since no man
proposes to more than one women in one round.)
\end{enumerate}

\textbf{Observation 1}: Men propose to women in a decreasing order of preference
on his list and does not skip anyone.

\textbf{Observation 2}: A women says maybe to a man in an increasing order of
preference.

So the algorithm terminates.

Output is a matching \(S\). Why is it stable?

Let \(wm \notin S\), let \(w_m\) and \(m\) be matched. and \(w and m_w\) be
matched. Suppose \(m\) prefers w\$ to \(w_m\). Then why are they not married? At
some point in the algorithm, \(m\) proposed to \(w_m\), but had to be rejected
eventually. Why? \(w_m\) rejects a man who is proposing only if there was a
better man \(m'\) who is higher on the \(w_m\)'s list than \(m\). In particular,
\(m_w\) is higher than \(m\) in \(w\)'s list. Thus this cannot be an unstable
pair.
\end{enumerate}
\subsection{Claim}
\label{sec:org848f507}
If \(D\) is an orientation of \(L(K_{n, n})\), such that \(\forall v \in V(K_{n,
   n})\), the set \(\{vw \colon w \in N(v) \}\) is oriented transitively, then \(D\)
is kernel perfect.
\subsubsection{Proof}
\label{sec:orgd34d626}
Define appropriate preferences for the vertices such that any stable
matching \(K \subset E(K_{n, n}) = v(D)\), \(K \cap I\) is a kernel. The kernel
is a vertex set of the line graph and hence a vertex set of the original
graph.

Men \(m_i \in M\) prefers women \(w_j\) to \(w_j'\) if \(m_i w_j \in I, m_iw_j' \in
    I\) and \(m_iw_j \leftarrow m_iw_j\).

If \(m_i w_j \in I\), \(m_i w_j' \notin I\), \(m_i w_j\), \(m_iw_j \notin I\) (\(m_i
    w_j \leftarrow m_i w_j\))

Women \(w_j\) prefers man \(m_i\) to \(m_i'\) if \(m_i w_j\), \(m_i'w_j \in I\) and
\(m_i w_j \rightarrow m_i w_j\) or \(m_i w_j \in I\), \(m_iw_j \notin I\) (\(m_iw_j
    \leftarrow m_i' w_j\))

Claim: \(K \cap I\) is a kernel of \(D[I]\).

Firstly, \(K\) is an independent set, indeed, since \(K\) is a matching in
\(K_{n, n}\).

Secondly, suppose you have a pair \(m_iw_j \in I \setminus K\), but has no out
neighbours in \(K \cap I\). We should be able to find an unstable pair. (There
is some issue with this part.)

Suppose \(w_j\) prefers \(m\) to \(m_i\), then if \(mw_j \in I\), then \(mw_i
    \rightarrow m_i w_j\). This cannot occur?
\section{Lecture 19 \textit{<2018-12-19 Wed>}}
\label{sec:orgcb84361}
\subsection{About network flows}
\label{sec:org933f854}
\((D, s, +, E)\) and \(D = (V, E)\) and \(s, t \in V\) and \(c \colon E \rightarrow
   \R_{\ge 0}\).

Want certain constraints: a flow \(f\colon E \rightarrow \R\). Capacity
constraints: \(0 \le f(e) \le c(e)\).

The second type of conservation constraints: \(\sum_{v \in N^{-1}(v)} (f(uv))
   = \sum_{w \in N^{+}(v)\) for all \(v \in V \setminus\{s, t\}\).

The \(\max \sum_{u \in N^{-1}(t)} f(ut) - \sum_{w \in N^{+}(t)} f(tw)\).

\(f \in \R^{\vert E \vert}\). There is a vector \(d \in \R^{\vert E \vert}\)
(\(\{0, 1, -1\}^{\vert E \vert}\))

Maximize \(d \cdot f\) subject to \(\bar{0} \le f \le c\). The bar means it's a
vector of zeros. One vector is less than or equal to the other one if
coordinate wise they are the same.

For every \(v \in V \setminus \{s, t\}\), \(f \cdot e_v = 0\), \(e_v \in \R^{\vert
   E \vert}\)

What we want is \(f \in \R^{\vert E \vert}\)

This is the framework of linear programming.
\subsection{Linear programming}
\label{sec:org2a21779}
In linear programming, we search for a maximum or a minimum of a linear
objective function, \(c^T \cdot x\), \(c\) is a constant vector subject to linear
constraints \(x\), where \(x \in \R^n\)

\(A x \ge b_i\) or \(A_x \le b_i\) or \(A_x = b_i\).
\subsection{Types of problems}
\label{sec:orge90c44a}
\subsubsection{Word problems}
\label{sec:orge48136b}
LP's were developed by Dantzig. And also by Kantorovich.
\subsubsection{\(n = 2\)}
\label{sec:orgb55dce8}
Maximize \(x_1 + x_2\).

Subject to \(-x1 + x_2 \ge -2\), \(-3x_2 + 2x_2 \le 2\), \(x_1 + 2x_2 \le 4\),
\(x_1, x_2 \ge 0, x_2 - x_1 = -2\).

A graph was drawn with the constraints and feasible region was identified.

Maximize \(x_1 + x_2\) such that \(-x_1 + x_2 \ge 2\) \(-3x_1 + 2x_2 \le 2\)
\(x_1 + 2x_2 \le 4\) \(x_1, x_2 \ge 0\), the feasible region has no solution.

We can also have unbounded optimal solution.

If the feasible region is bounded, then there is definitely an optimum.
\subsubsection{Book}
\label{sec:org80c2cb5}
Some book by springer.
\subsection{Uniformization of the setup}
\label{sec:org02de942}
Maximize or minimize a linear function subject to linear constraints.

Objective function:

Given a vector \(c \in \R^n\), \(c^{T}\cdot x = c_1 x_1 + \cdots + c_nx_n\).
\(\min c^{T} \cdot x = \max (-c^T)\cdot x\).

We may assume that we have a maximization problem.

Also we can represent \(\le, \ge\) into \(\le\).

From equality you can go to \(\le\), by introducing two constraints, both \(\le\)
and \(\ge\). Then transform the \(\ge\) further.

We can end up with \(Ax \le b\), \(A \in \R^{m \times n}\) and \(b \in \R^m\).
\subsection{A dietary example}
\label{sec:org232db70}

\begin{center}
\begin{tabular}{llrrr}
Ingredients & Requirement & Carrot & White Cabbage & Pickle\\
\hline
Vitamin A (mg/kg) & 0.5mg & 0.5 & 0.5 & 0.5\\
Vitamin C (mg/kg) & 15mg & 300 & 300 & 10\\
Dietary Fiber (g/kg) & 4g & 30 & 20 & 10\\
\hline
Cost &  & 0.75 & 0.5 & 0.15\\
\end{tabular}
\end{center}

Minimize the cost. \(x_c, x_k, x_p\) are the kilograms of Carrow, White cabbage,
and pickle in one Kebab.

Minimize \(0.75 x_c + 0.5 x_w + 0.15 x_p\) (the price of the pickle.)

Constraints:

(Vitamin A) \(35 x_c + 0.5 x_w + 0.5 x_p \ge 0.5\)

There is also non-negativity constraints. \(x_c, x_w, x_p \ge 0\).

Similar constrains for Vitamin C and Fiber.

The optimal solution \(0.009 kg\) of carrot, \(x_w = 0.038 kg\) of white cabbage.
From the pickle \(0.290 kg\).
\subsection{Another example}
\label{sec:org80bd213}
We have a data and want to find correlation.

Here it would be average rating and amount of chocolate offered to students.

We want \(a, b \in \R\). Then we have \(\sum (ax_i + b - y_i)^2\). Outliers would
come very much into this calculation. We can take \(\sum \vert ax_i + b -
   y_i\vert\) and minimize this instead. You want \(a\) and \(b\).

The function is not a linear function, it contains the absolute value.

We need to get rid of the absolute value.

Here we introduce new variables \(e_i\) and subject to the constraints.

\(e_i = \vert ax_i + b - y_i\vert \iff e_i \ge ax_i + b - y_i, e_i \ge y_i -
   ax_i - b\).

Input \((x_i, y_i)\) and we have variables \(a, b, e_i\). What you're interested
is in \(a\) and \(b\).

For the minimum, for all i, at least one is equality. For this reason, \(e_i\)
indeed expresses, the absolute value.
\section{Lecture 20 \textit{<2019-01-08 Tue>}}
\label{sec:orgaee835d}
\subsection{Linear programming (LP)}
\label{sec:org652fdfd}
Given \(c \in \R^n\) and \(A \in \R^{m \times n}\). We want to maximize, \(c^{T}
   \cdot x\) subject to \(Ax \le b\).

LP is in P. Simplex algorithm is an algorithm.
\subsection{A machine learning application}
\label{sec:org5881d56}
We need to device a trap for Rabbits. There could be other animals that fall
into the trap, we need to separate Rabbits from Weasels. We have a 2
dimensional vector corresponding to each animal.

We try to separate them by a line.

\(a, b \in \R\) such that \(a x(R_i) + b < y(R_i) \forall i = 1, \cdots, k\).
and \(a x(w_j) + b > y(w_j)\)

This looks like a linear program, except the inequalities are not strict.

We do a trick to avoid this situation. We'll have variables \(x, b, \delta\).
The goal will be to maximize \(\delta\).

\(\max \delta\), subject to \(a x(R_i) + b + \delta \le y(R_i)\) for all \(i = 1,
   \cdots, k\).

\(a x(W_j) + b - \delta \ge y(W_j) \forall j = 1, \cdots, l\).

If \(\delta > 0\), then there is an \(a, b\) which is good, i.e., there exists a
separating line.

If \(\delta \ge 0\), then there is no separating line. (In practise, everything
may be allowed.)
\subsection{Another example}
\label{sec:orgd599671}
You are a company that sells Wrapping paper, you produce paper rolls of width
\(300\) cm.

Order

\begin{center}
\begin{tabular}{rr}
Number & Width\\
\hline
97 & 135\\
610 & 108\\
395 & 93\\
211 & 42\\
\end{tabular}
\end{center}

There is a waste when we cut stuff and throw them away. We need to find a way
to minimize the waste.

Question: What is the smallest number of rolls of 300 cm that can be cut to
produce this order?

How would you set this up?

Minimize the variable \(x_{300}\) subject to the demand \(x_{135} \ge 300\),
\(x_{108} \ge 610\), \(x_{93} \ge 395\), \(x_{42} \ge 211\). This is not a good
linear program.

The possible ways to cut a roll of width \(300\) cm. This will be a vector of
length \(4\). How many \(135, 108, 93, 42\) that you produce.

Possibilities: \(P_1 \colon (2, 0, 0, 0)\), \(P_2 \colon (1, 2, 0, 1)\). There
are eventually finitely many choices.

\(P_3 \colon (1, 0, 1, 1) ,P_4 \colon (1, 0, 0, 3), P_5\colon (0, 2, 0,2), P_6
   \colon (0, 1, 2, 0), P_7 \colon (0, 1, 1, 2), P_8 \colon (0,1, 0, 4), P_9
   \colon (0, 0, 3, 0), P_{10} \colon (0, 0, 2, 2), P_{11} \colon (0, 0, 1, 4),
   P_{12} \colon (0, 0, 0, 7)\).

\(y_i\) is the number corresponding to \(P_i\).

LP is to minimize \(\sum y_i\) subject to \(2y_1 + y_2 + y_3 + y_4 \ge 97\),
\(y_{2} + 2y_5 + \cdots \ge 610\).

Optimal solution: \(y_1 = 48.5, y_5 = 206.25, y_6 = 197.5\) all others are \(0\).
When we round it above, \(450\) rows of width \(300\). The sum of these are
\(452.25\), this means that \(452\) is not enough.

\(y_1 = 49, y_5 = 207, y_6 = 196, y_9 = 1\).

There is an integer solution with values \(453\) rolls.

\subsection{Integer Linear Programming}
\label{sec:orge562b70}
\(\max c^{T} x\) subject to \(Ax \le b\), \(x \in \Z^n\)
\subsection{Max-weight matching}
\label{sec:org8f29c29}
We solved this by Hungarian algorithm. We have a bipartite graph, \(G\) where
the vertex set is \(G = (A \cup B, E)\) and you have a weight function \(w
   \colon E \rightarrow \R\).

Goal is to find a matching of maximum weight \(\sum_{e \in M} w(e)\). We solved
this problem in polynomial time.

Encode this as an Integer linear program.

Variable \(x_e\), for all \(e \in E\), \(x_e \in \{0, 1\}\), (\(x_e = 1 \iff e \in
   M\) otherwise \(x_e = 0\))

Maximize \(\sum_{e \in E} w_e x_e\)

Subject to the condition that \(\sum_{e \in E} x_e \le 1\) for all \(v \in V\).

If \(x^{*} \in \{0, 1}^{\vert E \vert}\) is optimal solution if and only if
\(\sum w_e x_e^{ *}\) is the weight of the maximal matching.

We take the LP relaxation, which is the same thing. Maximize, \(\sum_{e \in E}
   w_e x_e\) subject to \(\sum_{e \in E, v \in e} x_e \le 1, \forall v \in V\).

\(0 \le x_e \le 1 \forall e \in E\).
\subsection{Proposition}
\label{sec:org60aba5a}
For the max-weight problem, the LP-relaxation, has the same value as the
integer program. (For the complete bipartite graph.)
\subsubsection{Proof}
\label{sec:orgca5383d}
Optimum to LP-relaxation is at least as large as optimum for IP.

Let \(x^{*}\) be an optimal solution to LP. If \(x^{ *}\) is an integer vector,
we are done.

Otherwise, we'll make a change to this vector which will not change the
value, but it will increase the number of integer coordinates without
changing the value.

There is an edge, call it \(e_1\) such that the value of \(x^{*}\) is neither
zero, nor one.

Let's say \(e_1 = a_1b_1\). \(a_1 \in A\) and \(b_1 \in B\), the bipartite sets.

This implies that there is an edge \(e_2\) incident to \(b_1\) such that the
value of \(0 < x^{+}_{e_2} < 1\) since \(\sum x^k_e = 1\). At some point I must
return to the vertex. The idea is that we'll grab the cycle and change the
weight of the edges a bit. We'll add \(\varepsilon\) and subtract
\(-\varepsilon\) alternately.

There is a cycle \(e_1, e_2, \cdots, e_{2t - 1}, e_{2t}\) (an even cycle since
\(G\) is bipartite)

\(0 < x^{*}(e_i) < 1\), we'll change \(\tilde{x}(\varepsilon)_e\) is \(x^{ *} +
    \varepsilon\) if \(e = e_{2i - 1}\) and \(x^{ *} - \varepsilon\) when \(e =
    e_{2i}\) and \(x_e^{ *}\) otherwise.

\(\varepsilon\) is going to be \(\min(\{1 - x_e^{*} \colon e = e_{2i - 1}\}
    \cup \{x_e^{ *} \colon e = e_{2i}\})\).

We know that \(\varepsilon_0 > 0\).

The easy claim is that \(\tilde(\varepsilon_0)\) is a feasible solution for
the LP.

\(\sum_{e \in E} w_e \cdot \tilde{x}(\varepsilon_0)_e = \sum w_e x^{*} +
    \varepsilon_0 \sum_{j = 1}^{2} (-1)^{j+1} w_{ej} \ge \val(x^{ *})\).

There is at least one more integer coordinate.
\section{Lecture 21 \textit{<2019-01-09 Wed>}}
\label{sec:orgc8b8dfc}
\subsection{Repetition}
\label{sec:org7e35aef}
We have an even cycle with value along with edges that is between 0 and 1.
And we make a modification.

\(M_1 = \{e_1, \cdots, e_{2 t - 1}\}\) and \(M_2 = \{e_2, \cdots,
   e_{2t}\}\). \(\sum_{e \in M_1} w_e \ge \sum_{e \in M_2} w_e\) (without loss of
generality.)

You choose the \(+\varepsilon\) in this case. The value is the same as the
optimum value.

\(x(\varepsilon_0)\) has at least one more integer coordinate than \(x^{*}\). It
is also an optimal solution. This way we get an algorithm that gives us,
integer solution.
\subsection{Minimum vertex cover problem}
\label{sec:org56a3aa8}
We have a graph \(G\) and \(\beta(G) = \min\{ \vert C \vert \colon C \subset V,
   \forall e \in E(G), e \cap c \neq \emptyset\}\), we know that \(G\) is
bipartite, then \(\alpha'(G) = \beta(G)\).

In general this is not true, and it is know to be NP-Hard to decide whether
\(\beta\) is larger than \(\alpha'\).

An integer linear program formulation for \(\beta\).

Variables, \(\forall v \in V\), variable \(x_v \in \{0, 1\}\).

We'll have a constraint, \(\forall e \in E\), \(\sum_{v \in e} x_v \ge 1\).

The goal is to minimize the \(\sum_{v \in V} x_v\).

The claim is that the optimal solution is equal to \(\beta'\).

(More general statement) Claim: \(x \in \{0, 1\}^{\vert V \vert}\) is a
feasible solution to IP, if and only if \(S_x \{v \in V \vert x_v = 1\}\) is a
cover. In particular, \(x^{*}\) is optimal solution if and only if \$S\_\{x\^{}\{
\begin{itemize}
\item \}\}\$, hence \(\sum x^{ * }_v = \beta(G)\).
\end{itemize}
\subsubsection{Proof}
\label{sec:org111d0cf}
Let's prove that \(S_x\) is a cover.

Take arbitrarily edges \(e\). We need to show that \(S_x \cap e \neq
    \emptyset\).

Since \(x\) is feasible, \(\sum_{v \in e} x_v \ge 1 \implies x_{z_1}\) or
\(x_{z_2} \ge 1 \implies\), \(z_1 \in S_x\) or \(z_2 \in S_x\), \(e \cap S_x \neq
    \emptyset\).
\subsection{LP relaxation of the problem}
\label{sec:org2fc6c30}
\(\min \sum x_v\) subject to

\(\sum_{v \in e} x_v \ge 1\) and \(0 \le x_v \le 1\). This is the LP relaxation.

One could solve it.

If the linear program could give integer solution, then we could, solve an NP
hard problem in P, which would mean more.

But we can use the LP to form an approximation algorithm to the problem.

Let \(x^{*} \in [0, 1]^{\vert V \vert}\) an optimal solution to the LP.
\(S_{x^{ * }}\)

\(S_x = \{v \in V \colon x_v \ge 1/2\}\)

Claim: \(S_x\) is a cover.

Take \(e \in E\), \(x\) is feasible, implies \(x_{z_1} + x_{z_2} \ge 1\), implies
that \(x_{z_1}\) or \(x_{z_2}\), \(\ge 1/2\) implies \(z_1\) or \(z_2 \in S_x\).

Take \(x^{ * }\) be optimum, then \(\vert S_{x^{ * }} \vert = \sum_{v, x_v \ge
   1/2} 1 \le 2\sum x^{ * }_v \le 2 \vert_{v \in V} x^{*}_{v} \le 2 \beta(G)\)

Where \(\beta(G)\) is the optimal solution to the integer program.

Here we are using the that optimal solution to the LP is \(\le\) the optimum
for the IP.
\subsection{Remark}
\label{sec:orgc05e65c}
If you take a Maximal matching \(M\), then the set of vertices \(S = \{v \in
   V\colon v \textup{ is saturated by M}\}\) is a cover of size \(2n\).

\(2 \vert M \vert \le 2 \beta(G)\). This is a trivial observation. We can find
this using a greedy algorithm.

Finding a 1/2 approximation is not a big deal.
\subsection{Max incl set problem}
\label{sec:orgf7f3bb6}
Given \(G\), What is \(\alpha(G)\)? This is \(\max \{ \vert I \vert \colon I
   \subset V \forall e, e \notin I\}\)

ILP: \(\forall v \in V\), there is a variable \(x_v \in \{0, 1\}\), \(e \in E\),
constraint \(\sum_{v \in e} x_v \le 1\).

The maximum \(\sum_{v \in V} x_v\).
\subsubsection{Claim}
\label{sec:orgf7adbcf}
\(x\) is a feasible solution to the problem if and only if \(I_x = \{v \in
    V\colon x_v = 1\}\) is included. In particular, the implies that optimum
problem \(= \alpha(G)\).

The claim wasn't proven.
\subsection{LP relaxation}
\label{sec:orga068b86}
\(x \in [0, 1]^{\vert V \vert}\).

Can we learn something about \(\alpha(G)\) from the optimum value of the
objection from the LP relaxation.

For a clique, \(G = K_n, \alpha(K_n) = 1\). (The independence number.)

The vector \(x = 1/2\) is a feasible solution for any graph. And \(\sum x_v =
   n/2\). This means that, the LP relaxation is useless.

We know that \(I\) is independent if and only if \(V \setminus I\) is a vertex
cover. Even though that we know about a \(1/2\) approximation for the vertex
cover.
\subsubsection{A result by Hastead.}
\label{sec:org2b79780}
NP-complete to approximate \(\alpha(G)\) upto the factor \(n^{0.99999}\)
\subsection{LP in general}
\label{sec:org5b4b29c}
We'll first get rid of the inequalities make it into an equation form.

Every LP is equivalent to an LP of the following form:

Max \(cx\) subject to the constraint \(Ax = b\) and \(x \ge 0\). \(A\) is a matrix,
\(c\) and \(b\) are vectors.

\(a_{11}x_1 + \cdots + a_{1n}x \le b_i\), then we have a new variable, slack
variable, \(z_i \ge 0\), and replace constraint with \(a_{i1}x_ + \cdots +
   a_{in}x_n + z_i = b_i\).

New variables, \(y_1, y_i', \forall i 1, \cdots, n\) and \(z_j, \forall j = 1,
   \cdots, m\).

\(y_i = \max\{0, x_i\}\) and \(y'_i = \max\{0, -x_i\}\).

The number of variables is \(n\) and the number of constrains is \(m\).
\subsubsection{Some assumptions}
\label{sec:org5e1095e}
\begin{enumerate}
\item \(Ax = b\) has a solution. (We can do Gauss-elimination to decide it fast),
if not the LP has no feasible solution.
\item \(\rank(A) = m\). Because if not, we can delete the row. The assumption
says that the rows are linearly independent.
\end{enumerate}

With the assumption, the typical solution space would look like a simplex.
\subsection{About the linear system}
\label{sec:org81a6ef3}
\(Ax = b\)

\(A_B\) is the matrix is the matrix where columns of \(A\) indexed by \([n]
   \setminus B\) are indexed. Choose values arbitrarily, it gives you a solution
to \(Ax = b\). We have a solution space which is the right size.
\subsection{Definition}
\label{sec:orgb1d664e}
\(B \subset [n]\) is called a basis if rank \(A_B = m\). In fact, columns of \$A\(_{\text{B}}\)
are a basis in column space of \(A\).
\subsection{Definition}
\label{sec:org45c8404}
A feasible solution to \(*\) is called a basic feasible solution for a basis \(B
   \subset \binom{[n]}{m}\) if \(x_i = 0\) for every \(i \in [i] \setminus B\).
\subsection{Lemma}
\label{sec:org30bf6e6}
For every basis \(B\), corresponds to at most \(1\) basic feasible solution.
\subsubsection{Proof}
\label{sec:org3a837bf}
If \(x\) is a feasible feasible solution for \(B\), then \(Ax = A_Bx_B + A_N
    x_n = A_B x_B\).

We denote the complement of \(B\) by \(N\).

Now \(A_B\) is an invertible matrix, because it is a basis. This means that
\(x_B = A_B^{-1} b\) is determined (and of course \(x_N = 0\))

(Next week we'll show that if we have a bounded linear program, then it will
have a basic feasible solution that is optimal. This means that to find the
\(x_B\) for each \(m\) element subset, check if it a non-negative vector, go
through them and calculate the maximum value. For each \(B\), calculate \(c
    x_B\). Look at all of them find the maximum.)
\section{Lecture 22 Skipped}
\label{sec:org3fb2a84}
\section{Lecture 23}
\label{sec:org184257a}
\subsection{A review of simplex method}
\label{sec:org9c6f132}
Given \(A \subset \R^{m\times n}, b\in \R^m, c \in \R^n\). An LP in equational
form. LP: \(\max c^{T} x\) subject to \(Ax = b, x \ge 0\).

\textbf{Step 0}: Check whether \(Ax = b\) has a solution. Make sure that
\(\operatorname{rank}A = m\). You can assume this two things.

If we have a basis, then we know how to create a basic feasible solution.
\subsection{How to find a basis}
\label{sec:org204f3ba}
Add extra variables \(w_1, \cdots, w_n\). Your new system will look like
\(a_{i1}x_1 + \cdots + a_{in}x_n = b_i\). Make sure \(b_i \ge 0\). New
constraint, \(i = 1, \cdots, m\), \(a_{i1}x_1 + \cdots + a_{in} x_n + w_i =
   b_i\).

The objective function is to maximize the new variables. \(\max -w_1 - w_2 -
   \cdots - w_m\).

Matrix

\begin{verbatim}
+-------------------+----------+
|                   |          |
|                   |          |
|       A           |   I_n    |
|                   |          |
|                   |          |
|                   |          |
+-------------------+----------+
\end{verbatim}

Solve auxiliary LP with Simplex.
\subsubsection{Claim}
\label{sec:orgef7ef6d}
Value of auxiliary LP \(=0\) if and only if the original LP is feasible.

This claim is easy to see.

If you get a solution, then we got a solution, which end with a basis.

In the second case, the basis would be \(I_m\).
\subsection{Step 1 of Simplex}
\label{sec:orgb0f379e}
If there is an obvious feasible solution from the slack variables when
original \(b > 0\), then take it. Otherwise, solve the Auxiliary LP and if the
maximum is \(< 0\), then stop and return infeasible and if h
\subsection{Step 2 (Simplex tableau)}
\label{sec:org9ef9f69}
Compute a simplex tableau. Belongs to a feasible basis.

\(x_B = P + Q \times x_N\). \((\iff Ax = b)\)

The last row was the objective function, \(z = z_0 + r^{T} x_N\) (\(\iff z =
   c^{T}x\))

\(P \in \R^m, Q \in \R^{m \times (n-m)}\) and \(r \in \R^{n + m}\) and \(z_0 \in
   \R\) are all expressed from \(A, b, c\).
\subsection{Step 3 (Check for optimality)}
\label{sec:org0036b61}
If the whole vector \(r \le 0\), then we stop and return the current basic
feasible as optimal. (We proved this last time.)

Otherwise
\subsection{Step 4}
\label{sec:org79c4b77}
Select index \(i \in N\) entering the basis among \(\{i \in N \colon r_i > 0\}\).

How? pivot Rule.
\subsection{Step 5}
\label{sec:org15e4c90}
\(j \in B\) and \(x_j = P_j + \sum_{i \in \N} q_{jl}x_l\) if \(a_{ji} \ge 0\), then
there is no restriction.

If \(a_{ji} < 0\), then there is a restriction. Because you don't want \(x_j\) to
go below \(0\). Then \(0 \le x_j \le p_j + q_{ji}x_i\). (Was done yesterday)

First we test for unboundedness. If for every \(j \in B\), \(q_{ji} \ge 0\),
then, stop and return that the LP is unbounded. Because \(x_i\) can be
increased indefinitely.
\subsection{Step 6}
\label{sec:org43ddb9d}
Otherwise, select variable \(j \in B\) for which these restrictions are the
strictest. \(j \in B - \frac{p_j}{q_{ji}} = \min \{ - \frac{p_l}{q_{jl}} \vert
   \colon l \in B q_{l, i} < 0\}\) to leave the basis.

How? Pivot rule.
\subsection{Step 7 (The last step)}
\label{sec:org72aeb32}
Perform the pivot, move to a new basis \(B' = B \setminus (\{j\} \cup \{i \})\)

Update the Tableau. Go back to step \(2\) and do it again.

Use equation to replace to replace \(x_i\) with \(x_j\) everywhere else on the
RHS. Apparently all these were done yesterday.
\subsection{Pivot Rules}
\label{sec:org28cc791}
\subsubsection{The largest coefficient rule}
\label{sec:orgf002611}
Take the \(\operatorname{argmax} \{r_i \colon r_j > 0 \}\)
\subsubsection{Largest increase}
\label{sec:org9ec036f}
For a pair \(i, j\), what is the one with largest increase. This can be expensive.
\subsubsection{Steepest edge}
\label{sec:org265ba73}
Great in practise.
\subsubsection{Bland's rule}
\label{sec:orge0ceffb}
The entering variable, the smallest index is entering among \(r_i > 0\), \(i
    \in \N\). It has no concern in the value of \(r_i\). The leaving variable is
picked as the smallest for which \(-p_{i}{q_{j,i}} = \min \{ \cdots \}\).
\subsubsection{Random edge}
\label{sec:org4c99854}
Take a random.
\subsection{Theorem}
\label{sec:org0e70e75}
The simplex algorithm with Bland's rule will terminate in finitely many step.
\subsection{Proof}
\label{sec:orgb9e21f2}
If the algorithm does not terminate, then it follows that there is a sequence
of basis. \(B_1, B_2, \cdots, B_s = B_1\) following each other.

Let \(F = \{x_i \colon \cup_{d = 1}^{s} B_d\}\), these are the variables
participating in a basis of the cycle. These are called "fickle" variables.
These are the variables which are entering and leaving the basis.

All \(B_i\) have the same basic feasible solution and all fickle variables stay
\(0\) throughout. (This should be clear, is it so clear?)

Because we're in a cycle, the value cannot improve because the value is the
same at the end and at the beginning. Thus the value cannot improve in
between.

We'll identify the \(x_\beta \in F\), where \(\beta\) is the largest and now
we'll look at \(x_\beta\) enters and when it left. \(B\) is the basis just before
\(\beta\) enters and \(B'\) is the basis just before \(\beta\) leaves. Doesn't
matter if it repeats, pick any two.

Now, we apply the Bland's rule. The first deduction is that in the simplex
tableau. \(x_B = p + Q x_N\).

\(z = z_0 + r x_N\). In the simplex Tableau, \(r_\beta > 0\). On the other hand,
for every \(i \in F \setminus \{beta\} \cap N, \gamma_\beta = 0\).

The second deduction is that in the simplex tableau corresponding to
\(\tau(\beta')\), \(\beta'\)

\(x_B = P + Q x_N\) and \(z = z_0 + r x_N\) and \(q_{\beta, \alpha} < 0\). This
should be true, furthermore, \(q_{i, \alpha} \ge 0, \forall i \in B' \cap F
   \setminus \{\beta\}\)

We write down an Auxiliary linear program (it looks very similar to the
previous one.)

\(Ax = b\), \(x_B \le 0\), \(x_{F \setminus \{\beta\}} \ge 0\) and \(x_{N \setminus
   F} = 0\).

Let \(x^{ * }\) be the basic feasible solution corresponding to the \(B\) and \(B'\)
and everybody else. Then, Deduction 1 implies that \(x^{ * }\) is an optimal
solution to the Auxiliary LP. (Because of the fact that \(\beta\) was the largest)

The deduction \(2\) will imply the value of \(x^{ * }(t)\) is \(t\) if \(i =
   \alpha\), \(0\) if \(i \in N \setminus \{\alpha\}\) and \(x_{i}^{ * } + q_{\alpha}\)
if \(i \in B'\).

\(x^{*}(t)\) is feasible for all \(t > 0\). \(c^{t} \colon x^{ * }(t) \rightarrow
   \infty\). This would end up implying that the LP is unbounded, which is a
contradiction.
\section{Lecture 24 \textit{<2019-01-22 Tue>}}
\label{sec:org374a161}
\subsection{Example}
\label{sec:orgc53fa95}
The example from the beginning of 6th chapter.
\subsection{Weak duality}
\label{sec:orgc4d33c8}
Other examples: Matching, flows, covers.
\subsection{Corollary of weak duality}
\label{sec:org66b9555}
\begin{enumerate}
\item If both \(P\) and \(D\) are feasible and bounded, then optimum of \(P\) is less
than the optimum of \(D\).
\item For every feasible \(x^{*} \in \R^n\) of \(P\) and feasible \(y \in \R^n\) of
\(D\), then \(c^{t} x^{ * } = b^t y^{ * } \implies\), \(x^{ * }\) and \(y^{ * }\)
are optimal for \(P\) and \(D\) respectively

The strong duality theorem tells that the \(\Leftarrow\) is also true.
\item If \(P\) is unbounded from the above then \(D\) is infeasible. If \(D\) is
unbounded from below, then \(P\) is infeasible.
\end{enumerate}
\subsection{Definition of Primal-Dual for general LP}
\label{sec:org5da00e5}
Suppose that \(P\) is a an LP with LP with \(n\) variables \(x_1, \cdots, x_n\) and
\(m\) constrains.
\subsection{Proposition}
\label{sec:orgece1cfb}
If \(P\) is feasible and bounded, then \(D\) is feasible and bounded and the
respective optimum exists and are equal.

(The main part of the proof of the strong duality theorem.)
\subsubsection{Proof}
\label{sec:orgffe54e4}
(I think this is from section 6.3)

If \(D\) is feasible, then it is bounded by weak duality.

Want: \(D\) is feasible

\(P\), \(\max c^{t}x\) subject to \(Ax \le b\) and \(x \ge 0\).

\(P'\) introduces slack variable, maximize \(c^{t}x\) subject to \(Ax = b\), \(x
    \ge 0\).

Simplex terminates on \(P'\) with a basic feasible solution \(x^{*} \in \R^{n
    \times m}\) corresponding to a feasible basis \(B \subset [n + m]\).

Implies that the vector \((x_1^{ * }, \cdots, x_n^{ * } = x^{*} \in \R^n\) is
an optimal solution of \(P\).

We know that \(x^{*}\) multiplied by \(c\) is the same the original one.

The final tableau looks like 

\(\bar{x}_B = P + Q \bar{x}_N\)
\(z =z_0 + r^{T}\bar{x}_N\).

Where \(r \in \R^n\) and \(r \le 0\). (This is when the simplex algorithm halts.)

We can use this information. 
\subsection{Claim}
\label{sec:orgd530a66}
The vector \(y^{*} = (\bar{c}_B^{T} \bar{A}_B^{-1})^T\) is a feasible set of
\(D\) and \(c^{T}x^{ * } = b^{ * }\)
\subsubsection{Proof}
\label{sec:orgf34d0a4}
\(D\) is minimize \(b^{T} y\) subject to \(A^{T}x \ge c\) and \(y \ge 0\).

\(c^{T}x^{ * } = \bar{c}^T \bar{x}^{ * } = \bar{c}_B^T \bar{x}_B^{*} =
    \bar{c}_B^T (A_B^{-1} b) = (\bar{c}_B^{T} \bar{A}_B^{-1}) b\).

What we want: \(y^{*}\) is feasible for \(D\).

\(A^{T}y^{ * } \ge c\) and \(y^{ * } \ge 0\).

If and only if

\begin{verbatim}
+--------------+
|              |
|     A^T      |
|              | (y^{*}) \ge \bar{c}
|              |
+--------------+
|1             |
| 1            |
|           1  |
+--------------+
\end{verbatim}

Szabo's proof looked much longer than the proof in the book. Maybe he did
something different?
\section{Lecture 25}
\label{sec:org264ef0c}
\subsection{Duality recipe}
\label{sec:org266f4fd}
Page 85 of the book or section 8.2
\subsection{Maximum matching problem}
\label{sec:org3c0ff30}
\(G = (V, E)\), with variables \(x_e\) for all edge \(e \in E\), \(x_e \in \Z\).

Constraints: \(\forall v\), \(\sum_{e \in v} x_e \le 1\).

Sign-constrains: \(\forall e \in E, x_e \ge 0\).

We have shown that the maximum of the objective function: \(\sum_{e \in E} x_e
   = \alpha'(G)\)

We'll try to dualize this problem.

In dual variable \(y_v\), for all \(v \in V\).

Constrains \(\forall e\), \(\min \sum_{v \in V} y_v \le \beta(G)\).

If \(y \in \Z^n\), then \(\le\) becomes \(=\).

For Bipartite graphs, we will show that the LP becomes IP. This and the
duality leads to Konig's theorem
\subsection{Konig's theorem}
\label{sec:orgf306a58}
We can use Linear programming to show Konig's thoerem.

If \(G\) is bipartite, then \(\alpha'(G) = \beta(G)\)
\subsection{Plan}
\label{sec:org8555f27}
\begin{enumerate}
\item Prove that \(G\) is bipartite, then the optimal solution of \(P\) are integers,
\end{enumerate}
hence \(\max x_e = \alpha'(G)\).
\begin{enumerate}
\item Optimal solution for \(D\) (LP) is also integers, hence \(\min y_v = \beta(G)\)
\end{enumerate}

Now, using Duality theorem, \(\alpha'(G) = \max x_e = \min y_v = \beta(G)\).
\subsection{Proposition}
\label{sec:orgbf2aba1}
For LP, max \(c^{t} x\), subject to \(Ax \le b\) and \(x \ge 0\).

This is from chapter 8. Section 2.
\section{Lecture 26}
\label{sec:org7da3b3d}
\subsection{Zero sum two player games}
\label{sec:org876a26e}
\subsubsection{Payoff matrix}
\label{sec:orgad935f6}
\subsubsection{Strategy}
\label{sec:org8f85c34}
\subsubsection{Mixed strategy}
\label{sec:org2384b3f}
\subsubsection{Definition of \(\beta\)}
\label{sec:orgff3f443}
\subsubsection{Mixed Nash equilibrium}
\label{sec:org14a931a}
\subsubsection{Remarks on mixed Nash Equlibrium}
\label{sec:org1c346f7}
\begin{enumerate}
\item They are the best possible responses against each other.
\item Neither player can do better by switching strategies.
\end{enumerate}
\subsubsection{Question?}
\label{sec:orgb0d606d}
\begin{enumerate}
\item Do we always have a mixed Nash equilibrium?
\item Which one are the best?
\item How do we find them?
\item For the rock paper scissors, the Nash equilibrium is \((1/3, 1/3, 1/3)\).
The expected payoff will be \(0\). But if we change, then the opponent will
have a better strategy (provided they know about your distribution.)
\end{enumerate}
\section{Lecture 27}
\label{sec:org5df623c}
\subsection{Helly's theorem}
\label{sec:orgb67028f}
Section 8.6

He proved helly's theorem for 1 dimension.
\section{Lecture 28 \textit{<2019-02-05 Tue>}}
\label{sec:org763a26a}
\subsection{Randomized algorithms}
\label{sec:org3cab026}
Decision problems, problems with answers yes or no.

Two types of randomized algorithms.

\begin{center}
\begin{tabular}{ll}
Las Vegas & Monte Carlo\\
\hline
"House always wins" & Usually correct\\
Always correct & Always fast\\
Usually fast & \\
running time is random variable & solution is random variable\\
\end{tabular}
\end{center}
\subsection{Monte Carlo: types of errors}
\label{sec:org2f71eea}
\begin{enumerate}
\item False positive (algorithm says yes, answer is no)
\item False negative (algorithm says no, answer is yes)
\end{enumerate}

\textbf{Goal}: Bound the probability of an incorrect answer.

Sometimes we can ask for algorithms with one-sided errors, for example, never
give a false negative. In this case, we can repeat the test several times, if
we ever see a NO, the answer is NO. If we only see Yes, return Yes. Notice
that if the algorithm is repeated \(k\) different times, then
\(P(\textup{error}) = P(\textup{false positive})^k \rightarrow 0\).
\subsection{Example (Matrix multiplication verification)}
\label{sec:org692471a}
Problem: Given two \(n\times m\) matrices, \(A, B\) our matrix multiplying.
Machine claims \(C = A \times B\).

Question how can we verify this?

If we compute \(AB\) ourselves classically we need \(\Omega(n^3)\) operations.
\subsubsection{Randomized verification}
\label{sec:org4b524cb}
Pick a random entry \(C\), compute what it should be. If the answers agree,
then accept \(C\), else reject. To compute a single entry it take linear time.

No false negative.

\(P(\textup{false positive}) \le 1 - \frac{1}{n^2}\).
\subsubsection{A more precise verification}
\label{sec:orga0b540c}
Question \(AB = C\).

Reformulate the question: is \(AB - C = 0\).

So the general question is the matrix \(M = 0\).

\textbf{Problem} we don't have access to the entries. Instead we can multiply \(M\)
by vectors.

\(Mx = (AB - C)x = (AB)x - Cx\). We can compute \(Cx\).

\(ABx = A(Bx)\) This requires quadratic operations.

\textbf{Key observation} \(M = 0 \iff Mx = 0 \forall x\).
\subsubsection{Randomized algorithm}
\label{sec:org11ca7f6}
\begin{enumerate}
\item Choose a random \(x \in \{0, 1\}^n\)
\item Compute \(Mx\) if \(\neq 0\), return \(M \neq 0\). If \(= 0\), return \(M = 0\).
\end{enumerate}

We never get a false negative.
\subsubsection{Claim}
\label{sec:orge44eb81}
If \(M \neq 0\), and \(x \in \{0, 1\}^n\) uniformly random then \(P(Mx = 0) \le
    \frac{1}{2}\), i.e., the probability of false positive is \(\le \frac12\).
\begin{enumerate}
\item Proof
\label{sec:org8b389de}
Without loss of generality let us say that \(M_{11} \neq 0\).

Consider \((Mx) = \sum_{i=1}^{n} M_{1i} x_i = M_{11} x_1 + \sum_{i=2}^{n}
     M_{1i}x_i\).

For every choice of \(x_2, x_3, \cdots, x_n\). So the choice is zero for at
most one choice of \(x_1\).

Therefore \(P(Mx = 0) \le P((Mx)_1 = 0) \le \frac{1}{2}\).
\end{enumerate}
\subsubsection{About the algorithm}
\label{sec:orgb3fd6a3}
Repeat the algorithm \(k\) times and get that the probability of error \(\le
    \frac{1}{2^k}\).
\subsection{Definition}
\label{sec:orgb49bee0}
\(\omega\) exponent for matrix multiplication, i.e., two matrices can be
multiplied in \(\theta(n^\omega)\) time.
\subsection{Bounds}
\label{sec:org14cc184}
\(2 \le \omega \le 2.3728629\).
\subsection{Conjecture}
\label{sec:org2b2f0fa}
\(\omega = 2 + o(1)\).
\section{Lecture 29 \textit{<2019-02-07 Thu>}}
\label{sec:orgc52736a}
Didn't have charge in my laptop. Did stuff about matching etc.
\section{Lecture 29 \textit{<2019-02-12 Tue>}}
\label{sec:orgf2cb4e4}
\subsection{Hypergraphs}
\label{sec:org1dd5eb3}
A pair \((V, \mathscr{F})\), where \(V\) is a set of vertices and \(\mathscr{F}\)
is a family of subsets. \(F \subset 2^V = \{X \subset V\}\) set family
(elements are called Hyperedges)

Hypergraph is \(k\) uniform.

If \(\forall F \subset \mathscr{F}\), \(\vert F \vert = k\).

Example: \(2\) uniform Hypergraph \(\iff\) graph.

A \(3\) uniform hypergraph Fano Plane. \(7\) vertices and \(7\) hyperedges.

\begin{verbatim}
            |
           /|\
          / | \
         /  |  \
        /   |   \
      /-----+----\-
     /-/    |    \/-
    //---   | /--- \\
   /(    \--+-      )\
  /  \/---  | \--- /  \
 //----\    |    /\----\
------------+----------\--
\end{verbatim}

Finite Geometry. We talked about Hypergrpahs in Baranyai's theorem. A
complete \(k\) uniform on \(n\) edges. This can be partitioned into a Perfect
matching if \(k \vert m\).

Hypergraph \((v, \mathscr{F})\) is called \(2\) colorable if \(\exists c \colon V
   \rightarrow \{R, B\}\) that \(\forall F \in \mathscr{F}\), \(\vert c(F) \vert =
   2\)

It can be shown that the Fano plane is not \(2\) colorable.
\subsection{Problem}
\label{sec:orgeaa9bac}
\(m(k)\) is the minimum \(\min \{ \vert \mathscr{F} \vert \textup{non 2
   colorable k uniform hypergraph} \}\).

\(m(2) = 3\). For \(2\) edges, it is definitely two colorable.

\(m(3) = 7\)

Geometric rate of \(m(k)\) as \(k \rightarrow \infty\).

\textbf{Upper bound on m(k)}

\(\binom{[2k-1]}{k} = K^k_{2k-1}\) is not \(2\) colorable. For any two coloring
you color the vertices with \(\{R, B\}\), the bigger color class has at least
\(k\) vertices. But in our hypergraph, every \(k\) edge is present. Thus inside
this edge there is a monochromatic hyperedge.

\(m(k) \le \binom{2k-1}{k}\approx 4^k\)
\subsection{Lower bound}
\label{sec:org5c3a887}
If every \(k\) uniform hypergraph with \(f\) hyperedges can be \(2\) colored.
\subsection{Erdos}
\label{sec:orgefcd8fe}
\(m(k) \ge 2^{k-1}\)
\subsubsection{Proof}
\label{sec:org252ab6c}
Let \(F\) be \(k\) uniform and \(\vert F \vert < 2^{k-1}\), we need to \(2\) color
\(F\). 

Strategy \(2\) color \(V\) randomly and prove that with non-zero probability
that there is no monochromatic hyperedge. What we have shown is that there
exists a proper \(2\) coloring.

Color each \(v\) red with probability \(\frac{1}{2}\) blue with the same
probability. Choices of vertices are uniformly independent.

For \(F \in \mathscr{F}\), let \(E_F\) be event that \(F\) is m.c. \(Pr(E_r) =
    \frac{2}{2^k} = \frac{1}{2^{k-1}}\).

\(P(\exists F \in \mathscr{F}, F \textup{is m.c.}) \le \sum P(E) <
    2^{k-1}\frac{1}{2^{k-1}} = 1\) Weird proof.
\subsubsection{Next winter}
\label{sec:org3dfcc08}
In Extremal combinatorics, \(\frac{2}{\log k} 2^s \le m(k) < k^2 2^k\).
\subsection{Algorithm}
\label{sec:org0997066}
From the Erdos's proof, we get a Monte Carlo algorithm. Then a random
coloring succeeds in finding a proper \(2\) coloring with a probability
\(\ge 1 - \frac{1}{2} \ge \frac12\).

This means that we can repeat the algorithm several times to get a result. At
the end we produce a proper \(2\) coloring \(\frac{1}{2^i}\).
\subsection{Goal}
\label{sec:orgc6526bc}
\textbf{Derandomization} get rid of the randomization.
\subsection{Positional games}
\label{sec:org9868bf2}
A game Bridge-It
\subsection{Theorem}
\label{sec:orgb748796}
Player 2 does not have a winning strategy.
\subsubsection{Proof}
\label{sec:org80a9433}
Suppose player \(2\) has a winning strategy \(S\). Player \(1\) has access to this
strategy, player 1 plays an arbitrary edge \(e_1\), then follows \(S\). Ignoring
\(e_1\). If \(S\) calls for \(e_1\), take another arbitrary \(e_2\) etc. At the end,
we claim that Player 1 wins since, he always owns the moves according to \(S\)
plus one more. At the end you win because \(S\) is a winning strategy. But
player \(2\) plays according to \(S\), so they cannot win.

The proof doesn't really use the game.

This doesn't show that player 1 doesn't have a winning strategy. We need to
show that the game cannot end in a draw.
\subsection{Winning strategy}
\label{sec:orgea47e71}
How to win?

\begin{verbatim}

               -+---------+--------+-------+-
             -/ |         |        |       | \--
          --/   |         |        |       |    \--
       --/      |         |        |       |       \-
     -/   ------+---------+--------+-------+-----    \--
x --\\---/      |         |        |       |     \---------
     -\--\      |         |        |       |            /--/- y
       -\ ---\  |         |        |       |    /-------/--
         -\   --+---------+--------+-------+----    /---
           -\   |         |        |       |     /--
             -\ |         |        |       |  /--
               -+---------+--------+-------+--
\end{verbatim}

\(P_1\) and \(P_2\) color edges red and blue. Player wins if and only if \(x\) and
\(y\) are colored in the red group
\subsection{Theorem (Lehmann)}
\label{sec:org049fffa}
If connected and disconnected occupying edges of a graph \(G\) (alternatively)
the connector is able to occupy a spanning tree of \(G\) provided \(G\) provided
\(G\) contains two spanning tree (edge-disjoint)
\subsubsection{Proof}
\label{sec:orgb246b8c}
We assume that \(E(G) = E(T_1^{ * }) \cup E(T_2^{ * })\) in the beginning. In the
algorithm, the connector maintains two spanning trees \(T_1\) and \(T_2\) such
that the following properties hold.

\begin{enumerate}
\item \(E(T_1) \cap E(T_2)\) is the edges of the connector.
\item \(E(T_1) \triangle E(T_2)\) is the unclaimed edges.
\end{enumerate}

If succeeds then after \(n-1\) moves \(\vert E(T_1) \cap E(T_2) \vert = n -1\)
and \(T_1 = T_2\) is a spanning tree occupied by connector.

Start: \(T_1 = T^{ * }\), \(T_2 = T_2^{ * }\)

Now, after \(i\) moves, we have \(T_1\) and \(T_2\). In the next move, the
disconnector takes an edge \(e \in E(T_1) \triangle E(T_2)\). Without loss of
generality, \(e \in E(T_1) \setminus E(T_2)\). This disconnects \(T_1\) into two
components \(U_1\) and \(U_2\). \(T_2\) is connected, so \(\exists\) an edge \(e^{**}
    \in E(T_2)\) between \(U_1\) and \(U_2\). \(e^{ *} \notin E(T_1)\) otherwise \(U_1\)
will be connected in \(U_2\) in \(T_1 - e\).

So connector takes \(e^{*} \in E(T_2) \setminus E(T_1)\) and updates \(T_2 =
    T_1\) and \(T_1\) will be losing the edge \(e\) and adding the edge \(e^{ * }\).
\section{Tutorial}
\label{sec:orgb60789f}
\href{http://discretemath.imp.fu-berlin.de/DMII-2018-19/}{link}
\subsection{Tutorial 1 (Sheet 1)}
\label{sec:orga01e1cc}
\subsubsection{Problem 2}
\label{sec:org8b8e4c8}
Each step reduces the number of components by at most \(4\). After \(5\) steps, at least \(5\) components are left.
\subsection{Tutorial 2 (Sheet 2)}
\label{sec:org503b75e}
\subsubsection{SAT}
\label{sec:orgcdbf9af}
\begin{enumerate}
\item Example of un-satisfiable instance of SAT
\label{sec:orgc104bdc}
\(f(x_1) = x_1 \wedge \neg x_1\)

No matter what the instance is, this will evaluate to zero.
\item About \(2^k\) clauses
\label{sec:org71aa0ea}
We start by proving that the statement is true for exactly \(k\) variables.

Now we induct on the number of variables, starting from \(n\). If it is true
for \(m\), then it is also true for \(m+1\) because we can replace the \$m+1\$th
variable by \(x_1\) and bang.
\item Bound being strict
\label{sec:orgf2ba98f}
For \(k\) literals, and for \(2^k\), we take all possible combinations of \(x_1,
     \cdots, x_k\) such that no two literals are the same. This is not
satisfiable. (This should evaluate to \(1\) all the time.) Because no matter
what is the value of \(x_1, \cdots, x_k\), there is a literal where the or is
zero and that literal is present in it.
\end{enumerate}
\subsubsection{Problem 1 bipartite}
\label{sec:org37a314f}
We do a BFS. We have an array and it would be the distance. Now the claim is
that \(A(u) = A(v) \mod 2\) and if there is an edge that connects these two
vertices, then the graph is not bipartite.

We did prove that for BFS, the distances from the root are preserved.

The proof was a bit complex. But it turned out to be something about
applying BFS.
\subsubsection{Problem 3 Greedy algorithm can fail}
\label{sec:org411daf6}
\begin{enumerate}
\item part a
\label{sec:orga9588c6}
Algorithm: Sort the edges according to the weight in ascending order. \(E =
     \emptyset\).
\begin{enumerate}
\item No vertex of degree \(3\)
\item No cycle of length \(<\) n.
\end{enumerate}

\begin{verbatim}
          1
 +-------------------+
 | \-             -/ |
 |   \- 2     2 -/   |
 |     \-    --/     |
 |       \--/        |1
 |1      -/ \-       |
 |    --/     \-     |
 |  -/          \-   |
 |-/     1        \- |
-/------------------\+
\end{verbatim}

Another algorithm: start with an edge, something like a lightest edge. It's
vague.


\item part b
\label{sec:org8e1f8fb}
\end{enumerate}
\subsubsection{{\bfseries\sffamily TODO} Problem 2}
\label{sec:orgc4abf88}
The first \(m\) edges are the smallest weight forest.

Induction: First edge is true. Assume it's true for \(m-1\) edges are minimal
weight forest. Now kruskal adds an edge (it is the edge with smallest
weight). It does not make a cycle. We still have a forest. Now, the weight
the new is smaller than or equal to every other \(e_i\).

Apparently it doesn't work.

But we can do the induction backwards. Suppose that it is true for \(n-1\)
upwards.

\(K_{m+1}\) is the forest that it construct in \(m+1\) steps. (This doesn't work
either.)

Apparently, we could just repeat the proof for the Kruskal.

The problem is that there could be an edge that we didn't add because it
created a cycle before. This edge can create problems later.
\begin{enumerate}
\item {\bfseries\sffamily TODO} Go again through the argument of Kruskal
\label{sec:org5942d9b}
\end{enumerate}
\subsubsection{Exercise 4}
\label{sec:orge2a7b87}
\begin{enumerate}
\item Counter example
\label{sec:orgdb0db9f}
Apparently any algorithm would fail on it because the shortest path does
not make any sense.
\begin{verbatim}
           x
           /-\
          /   \
         /     \
        /       -\
       /          \
    1 /            -\  1
      |              \
     /                -\
    /                   \
   /                     \
  /        -2             -\
b---------------------------\c
\end{verbatim}
\item {\bfseries\sffamily TODO} Algorithm
\label{sec:orga6d8b35}
\textbf{Claim}: Right after \(i\) th step of second part, \(i\) th step of second
 part, \(v \in V, \dist(v) \le \min \{ \vert w\vert \vert w \textup{
      contains } \le i \textup{ edges} \}\).

\begin{enumerate}
\item For \(i=1\), it is trivially true.
\item For \(i\ge 2\), \(v \in V\), \$w = \$ shortest path \$ \(\le\) i\$ edges. There are
parts of the argument that I skipped. \footnote{Apparently Bellman-ford is an algorithm that is better than Djistra when
it comes to negative edges. But it works for digraphs and not for directed
graphs.}
\end{enumerate}

It is not clear how the last part of the algorithm is able to detect the
negative weighted cycle. The claim more or less does it.

The time complexity is \(O(mn)\)
\end{enumerate}
\subsubsection{Exercise 5 (SAT)}
\label{sec:orge8ec124}
\begin{enumerate}
\item Part a
\label{sec:org378723e}
It's easy
\item Part b
\label{sec:orgb785c6c}
Write \(f(x_1, \dots, x_m) = c_1 \wedge c_2 \cdots c_m\), \(m < 2^k\).

\(c_i = \tilde{x_{i_1}} \and \tilde{x_{i_2}} \cdots\)

for \(e\) or \(e_i\), define the set \(D_i = \{v\colon \{T, F\}^n \vert c_i(v) =
     false\}\). \(\vert D_i \vert = 2^{n-k}\), \(\sum D_i = \{v \vert f(v) = F\}\).\footnote{A lower bound for Ramsey numbers was also proved in same way. Also you
can do a probabilistic argument.}
\end{enumerate}
\subsection{Tutorial 3 (Sheet 3)}
\label{sec:org8b9c110}
\subsubsection{Problem 1}
\label{sec:orgc99a195}
We draw the graph, and remove the edges 2, 4, 8, 12. Now there are 6 odd
number of components. There is a characterization about the maximal matching
being the minimum of \(\{n - o(G\setminus S) + \vert S\vert\colon \forall S
    \subset V(G)\}\). Now, this is \(n-2\), which means if there is a matching with
\(n-2\), it will be maximal.

Easy solution: Show that there is no maximal matching. The number of
vertices is 18. There cannot be a matching on 17, because odd. Thus, the
matching on 16 has to be the maximal matching.
\subsubsection{Problem 2}
\label{sec:orgb0d72a3}
\begin{enumerate}
\item part a
\label{sec:org9fd2bbb}
It is clear that \(\vert V \vert = \sum \textup{vertices in even components
     of G\S} + \sum \textup{vertices in odd components of }G\setminus S + \vert S \vert\)

The above sum \(\mod 2\) evaluates to \(\vert V \vert \mod 2= o(G\setminus S) +
     \vert S \vert\). This is same as \(\vert S \vert - o(G \setminus S \mod 2\).
\item part b
\label{sec:orga247f7c}
The idea is something like this.

For each \(S\) such that the number of odd components in the complement of
\(S\) is greater than \(S\), introduce a \(K_d\), where it's the clique on \(d\)
vertices. What should be the value chosen for \(d\)? Each value of \(d\) should
be equal to the the difference for that set \(S\).

Now this should satisfy Tutte's theorem (maybe we can do this inductively
as well.) There is a maximal matching. We need to show that in the maximal
matching contains no edge in the clique. Now, we can arrive at a matching
on the original graph \(G\) and things should be okay?

I've omitted details in the steps.
\item part b (alternate solution)
\label{sec:org5157eb5}
Observation:
\begin{enumerate}
\item For all even components, there is a matching
\item For each vertex in the odd components, then \(C_o \setminus \{v\}\) has a
perfect matching.
\item If \(M\) is a maximal matching on \(G[s]\).
\end{enumerate}

Proof
\begin{enumerate}
\item Using Tutte's theorem: \(S' \subset V(e)\), \(\tilde S= S \cap S'\), then
\(\vert \tilde S\vert - o(G\setminus \tilde S)\ge \vert S \vert - o(G
        \setminus S)\) Now the LHS is equal to \(\vert S \vert + \vert S'\vert -
        o(G\setminus S) - o(Ce\setminus S) \ge 0\), now we can infer that Tutte's
theorem is satisfied inside the even components.
\item \(\vert S \vert + \vert S'' \vert - o(G \setminus S) - o(C_o \setminus
        S'') + 1\) (I missed some stuff here.)
\end{enumerate}
\end{enumerate}

\subsubsection{Problem 3}
\label{sec:org6478523}
\begin{enumerate}
\item Part a
\label{sec:org070316f}
This is kinda similar to the proof of the theorem about \(3\) regular graph
without any cut edges having a perfect matching.

First notice that every three regular graph must have even number of
vertices.

Given \(S\) and \(G\setminus S\), we think about the the degrees of elements
inside any disconnected component of \(G \setminus S\). They cannot be \(0\)
because then, it means that there are three cut edges. They may be \(1\) and
they maybe \(2\) at most \(2\) times.

If it is \(2\) exactly 0 times, there are no cut edges and we are done (the
idea is that the sum of the degrees of each vertex have to be even, but the
degrees are either 1 or 3, so there has to be an even number of vertices.)

If it is \(2\) exactly 1 times, then there can be at most \(1\) odd component,
and \(\vert S\vert > 0\), now, Tutte's theorem!

If it is \(2\) exactly \(2\) times, then we know that \(\vert S \vert -
     o(G\setminus S)\) has to have parity same as \(\vert V\vert\), but this means
that \(\vert S\vert\) is a non-zero multiple of \(2\), and \(o(G \setminus S)\)
is exactly equal to \(2\). Now, Tutte's theorem.
\item Part b
\label{sec:org8ec54c3}
The graph from the graph theory book
\item part c
\label{sec:orgfb1ca48}
If \(k\) is even, we can think about a clique on \(k+1\) vertices.

If \(k\) is odd, then we can do a similar construction as \(b\).
\end{enumerate}
\subsubsection{Problem 4}
\label{sec:orgb3ce116}
Induction on the number of vertices of the tree.

For \(2\) vertices. It has a unique perfect matching.

For \(n \ge 2\), \(n\) has to be even. There has to be at least one vertex that
is a leaf (Handshake lemma?) we picked a leaf because the edge must be in
the perfect matching. We delete the vertex right next to the edge. And on
each of the even component, we can use the induction argument? We need to
prove that if we delete a vertex inside the even component, there has to be
exactly one component is odd.

\(\vert T' \vert = \vert T' \cap C \vert (\mod 2)\)

We can prove this. And apply induction hypothesis.
\subsubsection{Problem 4 (Alternate proof)}
\label{sec:org2d7524f}
\textbf{Theorem}: In any tree there exist at most \(1\) perfect matching. (the idea
 is that if there are two matching, then their symmetric difference must
 have a cycle.)
\subsubsection{Problem 5}
\label{sec:org293c49c}
Let \(M\) be a maximal matching on \(G\). Let \(2\alpha\) be the number of
vertices here. Let \(2\alpha'\) be the number of vertices that gets connected
in \(2\alpha'\), we are supposed to show that \(\alpha' > 1/2 \alpha\).

Let the edges in the matching be \(\{e_{i_1}, \cdots, e_{i_n}\}\). Where \(i_1
    \le \cdots, i_n\). So when the algorithm is at the step \(i_k\), either it gets
added to \(M_{t-1}\) or gets rejected. Let \(x\) number of them get's rejected
and \(y\) number of them gets added.

When does an edge gets rejected? If one of the vertex in \(e_{i_k}\) is
already in \(M_{t}\). Let us now count the number of vertices in \(M_t\) at the
end.

\(M_t \ge x + 2y \ge x+y = (\alpha)\).

Strictness: Consider the following graph and ordering of vertices:
\begin{verbatim}
o----------------o----------------o----------------o
       e2              e1                 e3
\end{verbatim}

The maximal matching is \(\{e2, e_3\}\), whereas the algorithm generates
\(\{e_1\}\).
\subsection{Tutorial 4 (Sheet 4?)}
\label{sec:orgc63722a}
\subsubsection{Problem 1}
\label{sec:orgc683d95}
The inequality is super easy and just follows from the definition.

The proof is equality is also very easy. If no permutation satisfy the
condition, then we can easily prove that there cannot be an equality.
\subsubsection{Problem 2}
\label{sec:org2836ac9}
We can take the negative of all weights. And apply Hungarian algorithm.
\subsubsection{{\bfseries\sffamily TODO} Problem 2}
\label{sec:orga73b283}
I've not completed this.

I was wondering if the values in the equality matrix can increase. In one of
my iteration, the value increased.

Also I messed up since the problem talks about minimal matching. I computed
the maximal instead. So the first step should have been to negate the
matrix.
\subsubsection{Problem 3}
\label{sec:org54c0abb}
Step \(i\) of the algorithm: let \(M\) be the maximum matching in the equality
graph \(G_{u, v}\) and \(\vert r \vert < n\) and let \(Q\) be the vertex cover
provided by the algorithm, then \(T = Q \cap Y\) and \(R = Q \cap X\) and
\(\varepsilon = \min \{u_i + v_j - w_{ij}\vert x_i \in X \setminus R, y \in Y
    \setminus T\}\)

Let \((\bar u, \bar v)\) be the weighted cover after calculating with
\(\varepsilon\).

We need to show that \(\vert X \setminus R \vert > \vert T \vert\). This is
true, because if it is not true, then we have that \(n = \vert R \vert +
    \vert X \setminus R \vert \le \vert R \vert + \vert T \vert = \vert Q \vert
    = \vert r \vert < n\).

Indeed at each step, the cost function decreases by some number.

Why does the algorithm terminate?

\(w_{ij} = \frac{p_{ij}}{q_{ij}}\), and \(q = LCM(p_{ij}, q_{ij})\), then
\(\epsilon \ge 1/\varepsilon\), then we can always finish the algorithm.

Or we can make all the weights as integers. The rational case follows
easily.
\subsubsection{Problem 4}
\label{sec:org063d1a4}
Questions:
\begin{enumerate}
\item Is this the unique extremal examples? The answer is yes.
\item What are the extremal examples?
\item Hypergraphs: \(k\) uniform Hypergraphs. We can define the notion of perfect
matching in Hypergraphs. What is the maximum number? A paper in 2011
solved it.
\end{enumerate}
\subsubsection{Problem 5}
\label{sec:org1604b5a}
Isn't this kinda simple? Like it is trivial when \(n \ge k+2\), when \(n= k+1\),
the sum on the RHS evaluates to \(k -1/2\), since \(k\) is an integer,
\(\delta(G)\) has to be at least \(k\).
\subsubsection{Problem 5 (proof)}
\label{sec:org8663852}
What is the definition of \(k\) connected?

\(\kappa(G) \ge k \iff G\) is \(k\) connected.

\(G, n \ge k+1\), \(\delta(G?) \ge \frac{n+k-2}{2}\). It is sufficient to show
that if we subtract a set of size \(k-1\), then the graph is connected.
\(G[V(G)\setminus S]\) are either neighbours (in this case it's trivial.)

\(u, v \in V(G')\).

\begin{enumerate}
\item \(u \in N(v)\), then we are done.
\item \(N(v) \cap N(u) \neq \emptyset\). Suppose \(N_G'(v) \cap N_G'(u) = \emptyset\).
\(n+k-2 - 2(k-1) = n-k\), we should have so much vertices in \(G'\) for the
condition to be true. But \(G'\) has \(n-k+1\) vertices.
\item Remark: \(\delta \ge (n-1)/2\), then the graph is 1-connected or connected.
Whereas, if \(\delta \ge n/2\), then the graph has a hamiltonian cycle.
\end{enumerate}
\subsubsection{Problem 5 (Bonus)}
\label{sec:org2761b59}
An example where the bound in sharp.
\subsubsection{Problem 5 (my solution)}
\label{sec:orga72eefd}
I had a proof that counted the number or edges and arrived at a
contradiction. I forgot how it worked. It was something like if the degree
of each edge is \(\frac{n+k-2}{2}\), then we have a lower bound on the number
of edges because of handshake lemma. But the if we assume that there is a
\(k-1\) set that disconnects it, we can end up with an upper bound on the
number of edges. Turns our that this upper bound is lower than the lower
bound and hence the contradiction.
\subsection{Tutorial 5 (Sheet 4)}
\label{sec:org9567c10}
\subsubsection{Problem 4}
\label{sec:org3d4703c}
We can do this in cases.

\begin{enumerate}
\item \(\kappa(G) = 3\), then because \(\kappa \le \kappa' \le 3\), \(\kappa' =
       \kappa = 3\).
\item \(\kappa = 1\), then disconnecting one vertex would disconnect the graph.
Let \(U_1\), \(U_2\), \(U_3\), be the at most three components that would
\end{enumerate}
\subsection{Tutorial 6 (Sheet 5)}
\label{sec:org55cc93a}
\subsubsection{Problem 1}
\label{sec:org78081d3}
It is easy to see that \(\kappa = 2\). It is easy to see that \(\kappa' \le 4\).
We need to see how to ignore the other parts.

We find two hamiltonian cycles and then two of the edges have to be on the
hamiltonian cycle. And the third edge cannot really disconnect the graph.
Remove the Hamiltonian cycles, then graph still has \(\kappa' 2\). Thus the
original graph has \(\kappa' = 4\).

\textbf{Hint}: Let \(S\) be an edge cut of size \(\le 3\). Then we need to show \(S \ge
    5\), and \(\vert \bar{S}\vert \ge 5\). Then the induced graph has to be \(K_5\). But
there is no \(K_5\).
\subsubsection{Problem 3}
\label{sec:org30d8121}
\(K_n\), \(n\) is a natural number that is very large. There were two \(K_n\) at
both the end and in the center a graph of size \(k\).
\subsubsection{Problem 3}
\label{sec:org0da78bb}
\begin{enumerate}
\item \(k=d=k'\), \(K_{d+1}\) would do
\item \(k < d\) and \(k'\) two \(K_{d+1}\) graphs and do connections between them
\end{enumerate}
\subsubsection{Problem 4}
\label{sec:org9352206}
Something like if \(\kappa' = r\), then there are \(r\) edge disjoint paths. If
\(\kappa < \kappa'\), then there is a point in the edge disjoint path that
doesn't blah blah.

For proving for Peterson graph: there is a theorem by Brouwer that proves
this.

A simple way to do it is that remove a vertex and we get a graph that has a
hamiltonian cycle.

\(\vert S, \vert{S} \vert = \sum d(v) - 2e(G[S])\)\footnote{If there is an edge cut of size \(2\), then there is a triangle in the
peterson graph.}
\subsubsection{Problem 5}
\label{sec:org0ca4246}
You make a line graph and then argue that the \(\kappa'\) of the graph is same
as the \(\kappa\) of the line graph. (There is an intermediate graph which
should be formed by adding extra vertices \(u\) and \(v\), and the edge \(ux\) and
\(yx\).)

"Vertex transitive" graph. Any two vertices are the same in Peterson graph.

Called the Harrary graph.
\subsubsection{Problem 6}
\label{sec:org56b45b2}
\(\val(f) = f^-(t) - f^{+}(t) = \sum_{u \in \bar{S}} (f^{-1}(w) - f^{+}(w)) =
    \sum_{u\in \bar{S}}(\sum_{v \in V} f(vu) - \sum_{v \in V} f(uv))\) some
calculations and we are done.
\subsection{Tutorial 7 (Sheet 6)}
\label{sec:org173883b}
\subsubsection{Problem 1}
\label{sec:org48f5b86}
Given a bipartite Graph \(G\) with \(X\) and \(Y\) be two partitions. Add two
vertices \(x\), \(y\) and edges from \(x\) to every element of \(X\) and edges from
\(y\) to every element of \(Y\).

The theorem now follows from local Vertex Menger's theorem.

How does one prove Local Vertex Menger from Local edge menger? The other
side was an exercise.
\subsubsection{Problem 1 (after LP)}
\label{sec:org622d629}
This is a consequence of duality. We already know that Ford Fulkerson is a
consequence of duality. Not surprising.
\subsubsection{Problem 2}
\label{sec:orgb2d2b60}
I think I proved this for the grade. I think this was from the book.
\subsubsection{Problem 4}
\label{sec:orgfcd891e}
\(I\) is the set of all spanning trees and \(J\) is the set of all edge cuts.
\subsection{{\bfseries\sffamily TODO} Tutorial 8 (Sheet 7)}
\label{sec:orgeba72d5}
\subsubsection{{\bfseries\sffamily TODO} Exercise 1 (About Baryani)}
\label{sec:org065e0e9}
\subsubsection{Exercise 2}
\label{sec:org08cb0d2}
I think this is straightforward to prove.
\subsubsection{Exercise 3}
\label{sec:org3cb09e1}
\(G\) be a graph with the property that every two odd cycles share a vertex.
Then prove that the chromatic number of the graph is at most \(5\)

Proof is from a math stackexchange answer

Consider the smallest odd cycle of the graph. There are no chords in the odd
cycle, since if there were, we could take a detour and make a smaller odd
cycle. Thus we can color this cycle \(C\) with \(3\) colors.

Consider the cycle \(G \setminus C\). This graph has no odd cycles, this is
because if \(G \setminus C\) has an odd cycle, then it is also an odd cycle in
\(G\), but is disjoint from \(C\). Thus \(G \setminus C\) is bipartite. Thus we
can color this with \(2\) colors. Thus we can color the whole with thing with
at most \(5\) colors.

Also I'm not completely sure about the framing of the question. Do they mean
that every odd cycle intersect in exactly one point or at least one point? I
guess the proof will work in both cases. There was a math stackexchange
answer saying we can do even better by saying that the chromatic number is
at most \(4\), but it doesn't work when \(K_5\) is used.
\subsubsection{Exercise 4}
\label{sec:org8fffd80}
Chromatic number \(k\) means that at least \({k \choose 2}\) edges.

My proof to this was wrong. My mistake was that I ended up assuming that
every coloring should come from a greedy coloring. Is this true? Probably
not.

\href{https://math.stackexchange.com/questions/1135688/prove-that-every-k-chromatic-graph-has-size-m-geq-binom-k2}{Math.SE} question.

Consider the colors \(1, \cdots, k\). For each colors, there should be at
least one edge from the color \(i\) to \(j\), where \(i \le j\). If not, we could
just remove one of the color class to get a \(k-1\) coloring of the graph.

This means that there are at least \(0 + 1 + 2 + \cdots + k - 1 =
    \binom{k}{2}\) edges.
\subsubsection{Exercise 5}
\label{sec:org8f2f37b}
For triangle free graphs, the trivial upper bound \(\chi(G) \le \Delta + 1\)
can be strengthened into

$$\chi(G) \le \frac{3}{4} \left\lceil \frac{\Delta(G) + 1}{4}\right\rceil$$

There is a solution \href{http://www-sop.inria.fr/members/Frederic.Havet/Cours/coloration.pdf}{This pdf} proposition 8.16 pp. 119 (7 in pdf)

\textbf{Proof} let \(k = \left\lceil \frac{\Delta(G) + 1}{4}\right\rceil\), then let
 \(V_1, \cdots, V_k\) be a partition of the vertices such that the sum of
 internal vertices for each \(V_i\) is minimum (the minimum exists, although
 this computation may not be possible in polynomial time.)

Now the idea is that each component \(V_i\) can be colored with at most \(3\)
colors. We first not that for the subgraph \(V_i\), \(\Delta \le 3\). This is
because if there is a vertex \(v\) in \(V_1\) (WLOG) that has degree \(4\) or
more, then there must be another partition where it's degree is \(3\), if
not, there will be \(4k \ge \Delta + 1\) edges.

Now, from brook's theorem each \(V_i\) can be colored in \(\le 3\) colors,
since there are no cliques of size \(3\) and odd cycles can anyway be colored
in \(3\) colors.

Thus each components can be colored in \(3\) colors and hence the whole thing
can be colored in \(3k\) colors.
\subsubsection{{\bfseries\sffamily TODO} Exercise 6}
\label{sec:org425be87}
\subsection{Tutorial 9 (Sheet 8)}
\label{sec:org2f580a4}
\subsubsection{Exercise 1}
\label{sec:org1f1bde8}
If \(X(G) = 4\), then it has a subgraph \(H\), that is \(4\) critical. But then
\(\delta(H) \ge 3\). This is sort of a contradiction. There will always be a
vertex such that \(\delta(G) = 2\).
\subsubsection{Exercise 2}
\label{sec:orgbaddceb}
For odd, \(X = 3\) and \(X \le 3\). We are done.

When \(n\) is even. We can assume that all lists are the same.
\subsubsection{Exercise 3}
\label{sec:orgc1b175b}
The following is the proof by Yugin.

\(F - E + V = 2\) and \(4F \le 2E\). Suppose \(\delta(G) \ge 4\). \(4V \le \sum
    d(v) = 2E\).

Now, \(2F \le E\).

\(2V \le E \implies V \le E/2\).

\(V = 2 + E - F \ge 2 + E - E/2 = 2 + E/2\). A contradiction.

Now, do this by induction.

Also the notion of \(0\) free planar implies \(3\) colorable. Some version of
Brook's theorem.
\subsubsection{Exercise 3}
\label{sec:orge25d570}
\href{https://math.stackexchange.com/questions/454353/show-that-triangle-free-planar-graphs-are-four-colorable}{Math SO}
\subsubsection{Exercise 4}
\label{sec:orgcc985c1}
Similar to how the 5 list colorability of graphs were proven in the class.
\subsubsection{About exercise 4}
\label{sec:orgdfe372b}
Prove that every outer plane has a vertex of degree \(2\). Now one can repeat
the above proof.

Something about dual graph. Something about triangulation as well.
\subsubsection{Mirzakhani graph (Problem 5)}
\label{sec:org6631828}
The idea is the following: we take a subgraph, which is the square with the
center element. To color this with \(4\) colors, at least two elements of the
outer loop has to be the same. But two adjacent vertices can't have the same
color, hence one diagonal has to be the same color.

Let us, without loss of generality assume that \(2\) and \(4\) have the same
color. The color has to be \(1\) or \(3\). Now, notice that \(1\) and \(3\) are
diagonally opposite to \(2\) and \(4\). Regardless of what is the color for \(2\)
and \(4\), these diagonals, can't be same. Hence the other diagonal has to
have the same color.

For simplicity assume that the color chosen was \(1\). Thus, there is a line
\(\bar{4}, \bar{3}, \bar{2}\) which needs to have the same color. And this
color cannot be \(1\), but \(1\) is the only possibility.

To show the part (b), let us say that the color of the infinite vertex is
\(5\), then \(5\) cannot appear on the boundary of the first square. Now, it can
be reduced to the problem (a), that this graph can't be list colored.

The chromatic number cannot be \(2\) because there is \(K_3\) or a triangle in
the graph.

We color the outer sides with two colors \(1, 2\). The interior points with
color \(3\), and the exterior point with \(3\). This is now \(3\) colorable. The
chromatic number is \(3\).
\subsubsection{{\bfseries\sffamily TODO} New bonus}
\label{sec:orgbf0f583}
If you have a triangle free planar graph, it's chromatic number is at most \(3\).
\subsubsection{Random stuff}
\label{sec:orgec69092}
\(C_{2n}\), \(v_1, \cdots, v_{2n}\).

\(f_G = (x_1 -x_2)(x_2 - x_3) \cdots (x_{2n-1} - x_{2n})(x_{2n} - x_1)\).

The list is a box \(L_1 \times \cdots \times L_{2n} \subset \R^{2n}\).

\(G\) is list colorable if and only if \(\exists x \in\) the box such that the
polynomial does not vanish.

The monomial \(x_1x_2\cdots x_{2n}\) does not get cancelled out implies
Combinatorial Constellation.
\subsection{Tutorial 10 (Sheet 9)}
\label{sec:org625580a}
\subsubsection{Problem 1}
\label{sec:org9657a91}
\begin{enumerate}
\item Part 1
\label{sec:org833a812}
\(C(x_i, y_j) = j - i (\mod n)\)
\item Part 2
\label{sec:org042755f}
\(G\) be broken down into two components \(X\) and \(Y\)

We make a network by adding \(s\) and \(t\). Now from \(X\), add \(\Delta(G) -
     \deg x\) and from \(X\) to \(Y\) it should be \(1\).

Similarly for \(\Delta - \deg y\) to the sink.

Minimum capacity is \(\sum \Delta - \deg x\).
\item Part 2
\label{sec:orgb34b29d}
We double the graph. The graph \(\tilde{G}\), then \(\Delta(\tilde G) =
     \Delta(G)\) and \(\delta(\tilde{G}) = \delta(G) + 1\). We keep repeating the
construction.

Basically, we add edges between vertices that are not saturated.

This would lead to an exponentially large graph.
\item Bonus
\label{sec:orge338bae}
What is the best bound on size of the new graph.
\end{enumerate}
\subsubsection{Problem 3}
\label{sec:orgcdc5e9b}
\subsection{Tutorial 11 (Sheet 10)}
\label{sec:org4d96dda}
\subsubsection{{\bfseries\sffamily TODO} Exercise 1}
\label{sec:orgf4ec75b}
\subsubsection{Exercise 2}
\label{sec:org9b37cae}
Preference

\begin{center}
\begin{tabular}{rrrrr}
 & 1 & 2 & 3 & 4\\
\hline
1 & x & 2 & 1 & 3\\
2 & 3 & x & 2 & 1\\
3 & 3 & 2 & x & 1\\
4 & 2 & 1 & 3 & x\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
Matching & Unstable pair & Reason\\
\hline
(1,2) (3,4) & (2,3) & 34 < 32; 21 < 23\\
(1,3) (2,4) & (4,3) & 24 < 21; 13 < 12\\
(1,4) (2,3) & (4,2) & 23 < 24; 41 < 42\\
\end{tabular}
\end{center}


\(ab < ac\) means a prefers \(c\) over \(a\). Here the numbers means ranking,

smaller rank is preferred over a larger rank.

\begin{center}
\begin{tabular}{rrrrr}
 & 1 & 2 & 3 & 4\\
\hline
1 & x & 1 & 2 & 3\\
2 & 2 & x & 1 & 3\\
3 & 1 & 2 & x & 3\\
4 & 1 & 2 & 3 & x\\
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{lll}
Matching & Unstable pair & Reason\\
\hline
(1,2) (3,4) & (2,3) & 2 prefers 3 over 1; 3 prefers 2 over 4\\
(1,3) (2,4) & (2,1) & 2 prefers 1 over 4; 1 prefers 2 over 3\\
(1,4) (2,3) & (1, 3) & 1 prefers 3 over 4; 3 prefers 1 over 2\\
\end{tabular}
\end{center}





\subsubsection{Exercise 3}
\label{sec:org392be5d}
The weights be \(w_1, \cdots, w_n\) and for each student, \(S_i\), let \(e_{1,i},
    \cdots, e_{12,i}\) be their grade in a homework, now the average for the
student is \(w_1e_{1, i} + \cdots + w_ne_{n, i}\) and \(0 \le w_i \le 1\).

For the students, that he wants to fail, it'll be \(w_1e_{1, i} + \cdots +
    w_ne_{n, i} \le 12\), and for the ones he has to pass, \(w_1e_{1, i} +
    \cdots + w_ne_{n, i}\). We also have a constraint \(w_1 + \cdots + w_n\).

We don't really have to minimize a cost function here, or do we? We could
just say \(\min w_1\), just to make it an LP. But in fact, all we need to know
is if there is a solution inside the feasible region.


There might be another way to formulate the problem.

The cost function will be the sum of \(12 - (w_1 e_{1, i} + \cdots ) + (w_1
    e_{1, j} + \cdots + w_{12} e_{12, j})\). We maximize this, under the
constraint \(w_1 + \cdots + w_n = 1\). and \(0 \le w_i \le 1\).

This would ensure that the most number of people the teacher hate will fail
(kinda) and the maximum number of people that the teacher like will pass.
\subsubsection{{\bfseries\sffamily TODO} Exercise 4}
\label{sec:orga5b753b}
This is an example from the book.
\subsubsection{Exercise 5}
\label{sec:org56dc283}
What does it mean by decision version of a problem?

Given a IP with \(n\) variables and \(m\) equations, (Can we assume \(n \ge m\))
given value \(x\), is there a solution such that \(\max c^t x \le k\)?

Here \(k\) should also be part of the problem. The decision version is clearly
in NP. This is because if we know such a solution exist, you can verify it
in polynomial time.
\begin{enumerate}
\item Reduction of 3 sat to Decision-IP
\label{sec:org0289d5e}
If there is a clause like \((x_1 \wedge x_2 \wedge x_3)\), then we can form
the equation \(x_1 + x_2 + x_3 \ge 1\). Similarly for each of the clauses.

Also add extra clauses \(x_1 + \bar{x_1} = 1\) for each symbol. Here \(x_i \in \{0, 1\}\)

What should be the maximization or minimization problem?

We know that it is decidable if there exists a solution to the problem. So
we can kinda pick an arbitrary, equation to maximize or minimize.
\item Decision version
\label{sec:orgaaec0de}
I saw this question in CS Stackexchange about what is the decision version
of the IP. One answer said the decision version of any problem is that "is
there a solution", I found it a bit weird because, in our case, it would
just mean whether the feasible region is empty or not, and we're not
concerned about maximizing or minimizing a function.

But for the reduction for \(3\) sat to Decision IP, we just care about this,
again.
\end{enumerate}
\subsubsection{{\bfseries\sffamily TODO} Exercise 6}
\label{sec:orgbef93b5}
\(x_i\) should correspond to vertices.

\(y_{ij}\) should correspond to edges, but it is not known if an edge actually
exist.

The value of \(x_k\) means if a vertex is picked or not.
\subsection{Tutorial 12 (Sheet 11)}
\label{sec:org6856951}
\subsubsection{Problem 1}
\label{sec:orge901d93}
The following code does the calculation

\begin{verbatim}
x1 <- matrix(c(1, 2), nrow=2)
x2 <- matrix(c(-1, 2), nrow=2)
x3 <- matrix(c(-3, 1), nrow=2)
x4 <- matrix(c(1, -2), nrow=2)

b <- matrix(c(5, 4), nrow=2)

ls <- list(x1, x2, x3, x4)

for(i in 1:3)
{
    for(j in (i+1):4)
    {
        print(c(i, j))
        xb <- tryCatch({solve(matrix(c(ls[[i]], ls[[j]]), nrow=2), b)},
                       error=function(e){return(c(NaN, NaN))})

        print(xb)
    }
}

print(matrix(c(x1, x2, x3, x4), nrow=2))
\end{verbatim}

There is only one feasible basis \((1, 4)\) and \(x_B = (3.5, 1.5)\). The
maximum value is \(3.5 + 1.5 = 5\).
\subsubsection{Problem 2}
\label{sec:org9090b77}
\begin{verbatim}
x2 <- matrix(c(-2, 1), nrow=2)
y1 <- matrix(c(1, 0), nrow=2)
y2 <- matrix(c(0, 1), nrow=2)
y3 <- matrix(c(2, 2), nrow=2)
y4 <- matrix(c(-2, -2), nrow=2)

ls <- list(x2, y1, y2, y3, y4)
b <- matrix(c(-2, 2), nrow=2)

for(i in 1:4)
{
    for(j in (i + 1):5)
    {
        print(c(i, j))
        xb <- tryCatch({solve(matrix(c(ls[[i]], ls[[j]]), nrow=2), b)},
                       error=function(e){return(c(NaN, NaN))})
        print(xb)
    }
}
\end{verbatim}

Simplex, \(A_B^{-1}A_N\)

\begin{verbatim}
AB <- matrix(c(-2, 1, 1, 0), nrow = 2)

AN <- matrix(c(0, 1, 2, 2, -2, -2), nrow = 2)

xb <- matrix(c(2, 2), nrow=1)

print(solve(AB) %*% AN)

cb <- matrix(c(2, 0), nrow=2)

## Calculation of r
matrix(c(0, 1, -1), nrow=3) -  t(t(cb) %*% solve(AB) %*% AN)
\end{verbatim}

After a pivot we'll see that the problem is unbounded. This can be seen by
plugging in \(x_1 = t\) and \(x_2 = t +1\) and \(t \rightarrow \infty\).
\subsubsection{Problem 3}
\label{sec:org99a77e5}
\begin{verbatim}
cn <- matrix(c(46, 43, -271, 8))
\end{verbatim}

Basis z1, z2
\begin{center}
\begin{tabular}{rrrrrrr}
x1 & x2 & x3 & x4 & z1 & z2 & b\\
\hline
2 & 1 & -7 & -1 & 1 & 0 & 0\\
-39 & -7 & 39 & 2 & 0 & 1 & 0\\
\hline
-46 & -43 & 271 & -8 & 0 & 0 & 0\\
\end{tabular}
\end{center}

Basis x1, z2
\begin{center}
\begin{tabular}{rrrrrrr}
x1 & x2 & x3 & x4 & z1 & z2 & b\\
\hline
1 & 0.5 & -3.5 & -0.5 & 0.5 & 0 & 0\\
0 & -12.5 & 19.5 & 21.5 & 19.5 & 1 & 0\\
0 & -20 & 110 & -15 & 23 & 0 & 0\\
\end{tabular}
\end{center}



\subsubsection{Problem 4}
\label{sec:org6525b05}
This is a section from the book. I also solved this for the tutorial
\subsubsection{Problem 5}
\label{sec:org25567b5}
From the last equation it is clear that \(x_n \le 5^n\). 

The last equation can be rewritten as \(2(obj) \le x^n + 5^n\). Thus the
objective value is bounded above by \(5^n\). Clearly \((0, \cdot, 5^n)\) is an
optimal solution.
\subsection{Tutorial 13 (Sheet 12)}
\label{sec:org82b433c}
\subsubsection{Problem 1}
\label{sec:orgd697141}
\begin{verbatim}
x1 <- matrix(c(15, -1, -3), nrow=3)
x2 <- matrix(c(-1, 1, -1), nrow=3)
x3 <- matrix(c(-1, 0, 0), nrow=3)
x4 <- matrix(c(0, -1, 0), nrow=3)
x5 <- matrix(c(0, 0, -1), nrow=3)

ls <- list(x1, x2, x3, x4, x5)
b <- matrix(c(4, 100, 120), nrow=3)

for(i in 1:3)
{
    for(j in (i+1):4)
    {
        for(k in (j+1):5)
        {
            xb <- solve(matrix(c(ls[[i]], ls[[j]], ls[[k]]), nrow=3), b)
            if(all(xb >= 0))
            {
                print(c(i, j, k))
                print(xb)
            }
        }
    }
}
\end{verbatim}

Turns out that the feasible region of the problem is empty.

Thus the original problem must be unbounded.
\subsubsection{Problem 2}
\label{sec:org3532b33}
Optimum Optimum. Primal: maximize \(x\) subject to \(x \le 1\) and \(x \ge 0\).
Dual: minimize \(y\) subject to \(y \ge 1\) and \(y \ge 0\).

Unbounded, infeasible. Primal: maximize \(x\) subject to \(-x \le 1\) and \(x \ge
    0\). Dual: minimize \(x\) subject to \(-x \ge 1\) and \(x \ge 0\).

Infeasible, unbounded. Use the above problem again.

Infeasible infeasible. Primal maximize \(x + y\) subject to \(-x \le 1\) and \(y
    \le -1\) and \(x, y \ge 0\) Dual: minimize \(x - y\) subject to \(-z \ge 1\), \(a
    \ge 1\) and \(z, a \ge 0\).
\subsubsection{Problem 4}
\label{sec:org4c8ca76}
I solved this for the grade.

One can solve this directly exactly as we proved the theorem for optimal
solution. One can also solve it using the fact that basic feasible solution
of a linear program is a vertex of the polyhedra. So there exists a linear
program which has a unique optimum equal to the basic feasible solution, but
we know that unique optimal solution has integer coordinates. Thus, the
basic feasible solution has integer coordinates.
\subsubsection{Exercise 5}
\label{sec:org5370d82}
The primal solution is the following:

maximize \(\sum e_i - sum \tilde{e}_i\). Where \(e_i\) is the set of vertices
entering \(t\) and \(\tilde{e}_i\) is the set of vertices that leave \(t\).

Subject to: \(\sum e_i - \sum \tilde{e}_i = 0\) for each vertex \(v\) that is
not \(s\) or \(t\). \(e_i\) and \(\tide{e}_i\) have the same meaning. This is the
conservation constraint.

Also subject to \(e_i \le f(e_i)\) (this is the capacity constraint; some
abuse of notation)

And \(e_i \ge 0\).

The dual of the program is the following:

\(\minimize \sum f(e_i) e_i\) subject to

\(x_j + e_i - x_j \ge 0\) where \(e_i \ge 0\) and \(x_i \in \R\).

Meaning of the variables. \(e_j\) should denote a value that means that it is
an edge between the cut. \(e_j\) should be \(1\) if it is between the cut. Now
the value of \(x_j\) means that it should be \(1\) if it in the cut with \(s\).
Now if \(x_j\) and \(x_i\) have the same value, \(e_j\) can be anything, either
\(0\) or \(1\), but then, because of the minimization problem \(e_j\) will be
zero hence the equations represent the problem for min-cut.

\textbf{About the matrix being totally unimodular}.

Notice that we don't need to consider the bottom \(I_n\) matrix, because of
the lemma that adding a canonical basis won't change the unimodularity.

We prove this by induction of \(\vert V \Vert\). For \(\vert V \vert = 1\), we
are done. Let us assume that it is true till \(\vert V \vert \le n\). For
\(n+1\), we think about the matrix in terms of the columns.

Suppose there is a column with all zeros, we are done. Suppose there is a
column with exactly one non-zero, we are again done.

Notice that each column can have at most two nonzeros. So the only case left
if the case where every column has exactly two nonzeros. On the other hand,
there can only be at most one \(1\) and at most one \(-1\). In this case the
matrix is singular since we can just add all the rows and get the row \((0,
    0, \cdots, 0)\) Thus it is unimodular in every case. By induction, we are
done.
\subsubsection{Exercise 6}
\label{sec:org700dab9}
Original problem: \(Ax \le c\) subject to \(x \ge 0\).

Given a arbitrary linear program, construct another linear program \(P'\) such
that \(P\) has an optimal solution if and only if \(P'\) has a feasible solution.

Our new optimal solution is the following:

Minimize (doesn't matter)

Subject to 

\begin{center}
\begin{tabular}{rrllll}
A\(^{\text{t}}\) & 0 & . & y & >= & c\\
0 & -A & . & x & >= & -b\\
\end{tabular}
\end{center}

greater than or equal to

and \(y \ge 0\), \(x \ge 0\).

\textbf{Proof} Notice that the dual of the original program is \(\min c^t y\)
subject to \(A^t \ge c\) and \(y \ge 0\). The dual of this program is the
original program.

If the program is optimal, both the dual and the dual of the dual is optimal
and hence also feasible. Thus \(A^t y \ge c\) and \(-A^tx \ge -b\). Thus the new
program is feasible.

If the new program is feasible, this means that both the dual of the
original program and the original program is feasible. Assume that the
program is not optimal, then it has to be either unbounded or infeasible. It
cannot be unbounded because the dual of the program is feasible. It cannot
be infeasible, because we already know it is feasible. Thus the problem has
to have an optimal solution.
\subsection{Tutorial 14 (Sheet 13)}
\label{sec:org4d4e560}
\subsubsection{Problem 1}
\label{sec:orgac251b7}
\begin{center}
\begin{tabular}{lrr}
F\D & Rock & Paper\\
\hline
Rock & 0 & -1\\
Paper & 1 & 0\\
Scissor & -1 & 1\\
\end{tabular}
\end{center}

When they tell me to analyse the strategy, I'm not sure what exactly should
be done.

For the father, the row with paper has no losses. The father probably
shouldn't pick Rock, Scissor is even. For the Daughter both the columns
seems to do okay on average, but she should pick paper, since the father is
more likely to pick paper.
\subsubsection{Problem 2}
\label{sec:org95b3210}
\begin{center}
\begin{tabular}{rrrr}
A\B & 1 & 2 & 3\\
\hline
1 & 1 & -1 & 1\\
2 & -1 & 1 & 1\\
3 & 1 & -1 & 1\\
\end{tabular}
\end{center}

Let \((x_1, x_2, x_3)\) and \((y_1, y_2, y_3)\) be probabilities, then \(x^t M y
    = (1-2y_2)(1-2x_2)\).

\(\beta(x) = \min_{y} x^{t} M y\) and \(\alpha(y) = \max_{x} x^{t} M y\).

\(\beta(x)\) is if \(1 - 2x_2 > 0\), \(-(1-2x_2)\) for \(y_2 = 1\) and if \(1 - 2x_2
    < 0\) is \(-1(1-2x_2)\) for \(y_2 = -1\) and is zero when \(1-2x_2 = 0\)

\(\alpha(y)\) is if \(1-2y_2 > 0\) then \(1-2y_2\) for \(x_2 = 0\) and if \(1-2y_2 <
    0\) is \(-(1-2y_2)\) for \(x_2= 1\).

The mixed equilibrium should be for \(x_2 = y_2 = 1/2\) and the choice of
\(x_1\) and \(x_2\) are the same.

The value of the game should be \(0\).

\textbf{Problem 2} If B plays \((1/3, 1/3, 1/3)\), then \(x^{t} M y^{t} =
    (1-2x_2)(1/3)\) For \(x_2 = 0\), this becomes \(1/3\). It jumps from \(0\) to
\(1/3\).

\textbf{Problem 3} It seems that the number of ways to win doesn't really matter
in the general case. This is just the intuition.

So if B is playing uniformly, then there is a way for A to make profit.

\(x^t M x\) will turn out to be \(6x_1 -12x_2 + 6x_3\). Now, \((1, 0, 0)\) will be
a good strategy for A. So A should accept it.
\subsubsection{Problem 3}
\label{sec:org8ef5557}
Rotate and translate the plane. Consider pair of discs so that the distance
between the centres are maximal.

The property about two disc intersecting is same as that the centres are
less than or equal to \(2\).

Now we can reduce it into the hint. 

Turns out that we can improve the bound to \(3\). The example is not easy.

\textbf{Bonus} Prove some upper bound for the general problem. Prove the bound of
 seven.
\subsubsection{Problem 4}
\label{sec:org1bdbb48}
\begin{enumerate}
\item For \(n =1\) it's trivial. Assume it's true for \(\le n- 1\). Write \(f\) in
terms of the variable \(x_n\). Write \(f = \sum_{i = 0}^{d} g_i(x_1, \cdots,
       x_{n-1}) x_n^i\). Let \(Z = \{(r_1, \cdots, r_n) \in S^n \vert f(r_1,
       \cdots, r_n) \neq 0\}\). \(Z_1 = \{(r_1, \cdots, r_n) \in Z \vert
       g_d_n(r_1, \cdots, r_{n-1}) \neq 0\} \subset Z\)

By inductive hypothesis \(g_{d_n}\) has \(\ge \prod_{i=1}^{n-1} (\vert S
       \vert - d_i)\) non-zeros in \(S^{n-1}\).

fix some \((r_1, \cdots, r_{n-1})\). Then \(f'(x_n) = f(r, \cdots, r_{n-1},
       x_n)\) is a polynomial in \(x_n\) of degree \(=d_n\). Implies \(f'(x_n) \le
       d_n\) zeros and hence \(\ge \vert S \vert - d_n\) nonzeros in \(S\).
\item \(f(x_1, x_2) = x_1x_2\) and \(S = (0, 1)\) and \(d = 2\), \(d_1 = d_2 = 1\),
\(d\vert S \vert^{n-1} = 4\). Thus \(\vert S \vert^n - \Prod (\vert S
       \vert - d_i) =3\).
\item \(f = x_1^2 + x_2^2 - 5\) and \(S = \{\pm 1, \pm 2\}\). Then we have \(2 \cdot
       2^2\) zeros. Somehow it works.
\end{enumerate}
\subsubsection{Problem 5}
\label{sec:org5425b9d}
I only verified the first one. I hope the rest is true.

We can get the following relation. If \(T(k)\) denotes the number of
operations. Then there are a total of \(18\) addition and \(7\) multiplications
of size \(T(k-1)\).

\(T(k) = 7T(k-1) + 18 2^{2k-2}\) and \(T(0) = 1\).

Now, \(T(k) = 7^k \cdot\frac{10}{4} - 6 \cdot 4^k\)

This is better than \(8^k\).

For general matrices, we can make it a matrix on \(2^k\) by padding it with
zeros. It is similar to how we compute the complexity of the merge sort.
\section{{\bfseries\sffamily TODO} Things to study}
\label{sec:orgc3c26c5}
\subsection{{\bfseries\sffamily TODO} Proof of Kruskal}
\label{sec:org3fc9d03}
There was also an exercise talking about how the first \(n\) edges of the
kruskal algorithm forms a minimum weight forest. Anurag said this was a
tricky problem.
\subsubsection{{\bfseries\sffamily TODO} Problem in proof about complexity}
\label{sec:orge319362}
In the second step, there is this argument on if \(C_u = C_w\) takes \(O(n^2)\).
Shouldn't it be \(O(mn)\)?
\subsection{{\bfseries\sffamily TODO} Proof of lower bound for sorting}
\label{sec:org1a61e0e}
\subsection{{\bfseries\sffamily TODO} What is the lightest network that remains connected after the breakdown of any edge?}
\label{sec:orge1b3b89}
What does lightest mean here? 

In a clique, you really have to remove a lot of edges to make it disconnected.

Perhaps, he's talking about a graph on \(n\) vertices with minimum number of
edges that will remain connected, even after removing any edge from it.
\subsection{{\bfseries\sffamily TODO} Proof of Hall's theorem}
\label{sec:orge582667}
He didn't do this in current course. But was done in the previous course.
West has a proof.
\subsection{{\bfseries\sffamily TODO} Finish learning the proof of Tutte's theorem}
\label{sec:org4af17ef}
\subsection{{\bfseries\sffamily DONE} Minimum number of edges in a k connected graph?}
\label{sec:orgb434a82}
This was an exercise and I read a proof that this is \(\binom{k}{2}\)
\subsection{{\bfseries\sffamily TODO} Study Baryani theorem}
\label{sec:orga823e7f}
\subsection{{\bfseries\sffamily TODO} For edge coloring consider when we are working with multigraphs}
\label{sec:orgcda3ae2}
And when we are not.

For example, for Vizing's theorem, we are working with simple graphs.
\subsection{{\bfseries\sffamily TODO} Learn some properties of Peterson graph}
\label{sec:org069cdc2}
\subsection{{\bfseries\sffamily TODO} Matchings}
\label{sec:org47537d5}
\subsection{{\bfseries\sffamily TODO} I don't understand the deal about M augmenting path and maximal matchings}
\label{sec:orgb94bb92}

\begin{verbatim}
+-----
|     \---------
|               \---------
|                         X-------------------
|                  /------
|           /------
|    /------
|----
\end{verbatim}

Consider the above graph, A triangle and a protruding edge. Consider my
matching which only consists of the protruding edge, it is clearly not a
maximal matching, but I can't seem to find any m augmenting paths
\subsection{{\bfseries\sffamily TODO} Study the proof of Hungarian algorithm}
\label{sec:org2bf4a9d}
One can model this using linear programming, the argument for the matrix \(A\)
being unimodular follows from a theorem similar to
\subsection{{\bfseries\sffamily TODO} Proof of Vizing's theorem}
\label{sec:orgf91bff0}
\subsection{Theorems in the classs}
\label{sec:orgd27af48}
\begin{enumerate}
\item Proof of lower bound of sorting.
\item Proof of Djistra's algorithm
\item Tutte's Theorem and it's complicated proof
\item[{$\boxtimes$}] 3 regular graph without cut edges does not have perfect matching.
\item[{$\square$}] \(k\) regular bipartite matching has a perfect matching.
\item[{$\square$}] Konig's theorem. The IP/LP proof is nice.
\item[{$\square$}] The value of \(2\alpha'(G)\) is the minimum of \(\min\{n - o(G\setminus
      S) + \vert S \vert\}\). This was an exercise too.
\item[{$\square$}] Algorithm to find maximal matching in a Bipartite graph. Later in the
course a randomized algorithm was also done.
\item[{$\square$}] Augmenting path algorithm.
\item[{$\square$}] Hungarian algorithm and it's dual. (Maximum weighted matching and
minimum weighted cover problem.)
\item[{$\square$}] Proposition 8.4. For all \(k \ge 2\), there exists a \(k\) connected graph
with \(\lceil (n-k)/2 \rceil\) edges.\textit{<2019-02-15 Fri>}
\item[{$\boxtimes$}] \textbf{Theorem (Chvtal-Erdos)} 8.5. I feel like this might be a question for
the exam. \textit{<2019-02-15 Fri>}
\item[{$\boxtimes$}] Menger's theorem. Vertex menger, Edge menger etc. I feel like this
might show up for the exam.\textit{<2019-02-16 Sat>}
\item[{$\square$}] Study the proof of local implies global. Case 2.
\item[{$\square$}] Ford Fulkerson and it's proof. This might come for the exam. But I
don't know.\textit{<2019-02-16 Sat>}
\item[{$\square$}] Proof of vertex Menger using network flows. \textit{<2019-02-17 Sun>}
\item[{$\square$}] Integrability theorem.
\item[{$\square$}] Partitioning triplets and the deal about Baryani's theorem
\item[{$\square$}] Proposition about \(X(G) \ge \frac{v(G)}{\alpha(G)}\) I don't even know
if the statement is true. I don't have a proof in my lecture.
\item[{$\square$}] Hajos conjecture. and Proof for \(n = 4\). I feel like this can be a
possible theorem for the exam. (Dirac theorem)
\item[{$\square$}] Brook's theorem and proof. There were a couple of exercises that used
Brook's theorem to prove stuff. The proof was a bit lengthy and even
required some structure theorem. Probably this one's not coming.
\item[{$\square$}] 5 color theorem. This was already asked in the mock exam.
\item[{$\square$}] For Bipartite multigraph. \(X'(G) = \Delta(G)\). This is a relatively
simple proof. Might actally come up for exam.
\item[{$\square$}] \textbf{Vizing's theorem} Might come in the exam. Proof not too hard.
\item[{$\square$}] \(X_l(G) = X'(G)\) for Bipartite graphs. We proved it for \(K_{n,n}\).
Proof not too hard. Might end up in the exam. Okay no. I think the proof
is really lengthy.
\item[{$\square$}] \textbf{Proposal algorithm} and Galey Shapely. He might ask this for the
exam. Proof uses Kernel perfect stuff. \textit{<2019-02-16 Sat>}
\item[{$\square$}] \textbf{Linear Programming}.
\end{enumerate}
\subsection{Questions}
\label{sec:org809d565}
\begin{enumerate}
\item[{$\square$}] What is the difference between an independent set and vertex cover.
\end{enumerate}
\end{document}