#+TITLE: Discrete Math II
#+author: Hari
#+LATEX_HEADER: \usepackage[left=2cm, right=2cm, bottom=2cm, top=2cm]{geometry}
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \def\R{\mathbb{R}}
#+LATEX_HEADER: \def\Z{\mathbb{Z}}
#+LATEX_HEADER: \def\N{\mathbb{N}}
#+LATEX_HEADER: \def\max{\operatorname{max}}
#+LATEX_HEADER: \def\P{\textup{P}}
#+LATEX_HEADER: \def\NP{\textup{NP}}
#+LATEX_HEADER: \def\coNP{\textup{co-NP}}
#+LATEX_HEADER: \def\min{\operatorname{min}}
#+LATEX_HEADER: \def\dist{\operatorname{dist}}
#+LATEX_HEADER: \def\prev{\operatorname{prev}}
#+LATEX_HEADER: \def\val{\operatorname{val}}
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+OPTIONS: tex:dvipng

* Lecture 2<2018-10-17 Wed>
** Review
*** Random stuff about algorithms
    Algorithm A

    Input class I

    $T(A, I)$: time algorithm $A$, input $I$.

    The worst case time complexity of $A$ on $I$ is $\max_{I\in \mathbb{I}} (A, I) = T_(n)$.

    The worst case complexity of $I = \min_{A} T_A(n) = T_{I}(n)$. A function that grows with $n$.

    Average case complexity $\mathbb{E}(T(A, I))$.
*** Sorting
    Sorting algorithm $A$, $T(A, \pi) =$ number of comparisons that $A$ makes to find $\pi.$

    Yesterday, we proved that for insertion sort (the stupid algorithm), the
    worst case running time was $\binom{n}{2}$.

    Binary insertion:  $T(n) \le n log_2{n}$.

    Merges sort: $T_M(n) =le n\log_2{n}.$
*** Theorem about lower-bound of sorting
    If $A$ is a sorting algorithm (correctly sort $n$ numbers)

    Then $T_A(n) \ge \log_2(n!)$.

    In particular, using the stirling's formula, $T_A(n) \ge n\log_n$.
**** Proof
     The algorithm runs on the permutations on the set of $n$ numbers. Define
     $\forall \kappa \in \N$ blah blah.

     $S_n \subset S_n(\alpha, \kappa) =$ $\{\pi \in S_n \colon \textup{where A
     receives input} \pi$ $\textup{ there, it receives } \alpha \textup{ as the sequence of
     the list of first k answerers}\}$
**** Observation
     Everything is determined if we run it.

     $S^n_{\alpha, k} \cap S^n_{\beta, k} = \emptyset \forall \alpha \neq \beta$

     $\cap S^{n}_{\alpha, k} = S_n$

     Suppose, $T_A(n) < \log_2(n!)$.

     $S_n = \cap S^n_{\alpha, k}$ partition into $2^k$ sets.

     $2^k < n!$ implies that there exist $\beta$ such that there exist $\pi_1
     \neq \pi_2 \in S_{\beta, r}$.

     Run $A$ on $\pi_1$, we receive the answers $\beta$ implies that $A$ outputs
     $\pi^{*} \in S_n$

     + Case 1: $\pi^* \neq \pi_1$, a contradiction to the correctness of $A$.

     + Case 2: $\pi = \pi_1$. Run $A$ on $\pi_2$ implies $\beta$ is the answer.
       $A$ outputs $\pi_{*}$, this is a contradiction that $\pi_{*} \neq \pi_2$.
       Contradicts the correctness of $A$.

       Called the *information theoretical lower bound*.
** Graph algorithms
*** Connectedness of graph
    A graph $G$ is connected if for every two vertices $u$ and $v$, there exists
    a $uv$ path in $G$.

    This induces an *equivalence* relation where $u$ is equivalent to $v$ if
    there exists a path that connects $u$ and $v$. Create equivalence classes,
    which are called the *connected components* of $G$.

    Observation: There is no edge between different connected components.
*** How do you decide whether a graph is connected?
    Take a vertex $v$. We maintain a list of all vertices that are visited. We
    redo the same thing for every other vertex. We repeat this until we saw all
    the vertices in the graph.

    Algorithm (Comp (V))
    #+BEGIN_VERSE
    Initialize: Queue Q = v; and W = empty
    for all $i \ge 1$
    step i: v_i first vertex in Queue.
    remove v_i from the queue  and put it in W
    put all of $N(v_i) \setminus W$ into $Q$.

    IF Q = \empty, STOP and return $W$ as the connected component of $v$.
    else go to step i+1
    #+END_VERSE
*** Theorem: Comp(v) returns $C_G(v)$
    Suppose $u \in C_G(v) \cap W_{out}$.

    Let $P$ be a vu path in $G$.

    Comp(v) puts a vertex into $w$ only if all its neighbours are put into $Q$.
    We stop only if $Q$ is empty. Also $v_f$ was in $Q$ at some point $A$. $v_f$
    had to be moved to $w$. This is a contradiction.

    Other direction: $u\in W_{out}$. Before $u$ became part of $W$, $u$ was in
    $Q$. Why? Because there is a $u_1 \in Q$, $u \in N(u_1) \setminus W$.
    (More things, I skipped.)
*** Spanning tree
    Suppose we run Comp(v) on a connected graph, where a vertex $w$ is put into
    $Q$, then there is a unique edge coming with it that attaches it to $v$.
    (the vertex that is moved from $Q$ to $W$ at the same time.)
*** Theorem about spanning tree
    The following are equivalent: for an $n$ vertex graph.

    1. T is a tree (connected, acyclic.)
    2. T is connected and has $n-1$ edges.
    3. T is acyclic and has $n-1$ edges.
    4. For every pair of vertices $u$ and $v$ in $V(T)$, there is a unique $uv$
       path.
**** Definition (spanning tree)
     $T \subset G$ is a spanning tree if $T$ is a tree and $V(T) = V(G)$.
*** Special spanning trees
    Let $G$ be connected and run Comp(v) (don't forget the edges.)

    /What if/ we always put $N(v_i) \setminus W$ to the top of $Q$. (We call
    this the *depth first search* tree.) This is going to create a tree which is
    long (?)

    /What if/ if we put it to the bottom of the tree, this will create a
    *breadth first search*. You will create which is short.

    A diagram that I ignored.
** Minimal spanning tree
   Given a graph $G$. (can be a complete or arbitrary graph.)

   We have a weight function that is assumed on the edge set to $\mathbb{R}$.
   What we want is a spanning tree $T\subset G$ such that the cost of the sum of
   weights on the edges is minimum (i.e., for any other spanning tree, the sum
   of the weights on the edges would be more than the current one.)
*** Naive algorithm
    There is at most $n^{n-2}$ (Cayley's theorem.) spanning trees on $n$ vertices. Let's look at all
    of them and calculate the weights and output the minimum.
*** Kruskal's algorithm
**** Step 1
     Sort edges in increasing order of weights $e_1, \cdots e_m$ such that
     $w(e_1) \le w(e_n) \le \cdots, \le w(e_n)$.

     Start with an empty forest $E(F) \neq \emptyset$ for all $v \in V$, $c_v = v$.
**** Step 2
     For each edge $e_i = uv$. For $\forall i \ge 1$, if the forest plus the new
     edge has a cycle, then $C_v$ remains the same.

     If there is no cycle, we have a new forest, i.e., the bigger forest with
     the extra edge added to it.
**** The end
     Output $F$.
*** Theorem: Kruskal's algorithm returns the min-weight spanning tree.
    Proved in discrete Math 1.
*** Running time of Kruskal
    The first step involves sorting. This can be done in $O(|E| \log|E|)$.

    There is $O(m)$ and $O(n^2)$.

    If $G$ is dense, then $O(m\log m)$ and if $G$ is sparse, then $O(n^2)$.
* Lecture 3 <2018-10-23 Tue>
** Spanning trees
   Another perspective: get to one place to another in the fastest way possible.
   Versus the minimum spanning tree. [fn:1]
** Problem
   Given graph $G=(V, E)$, a distance for $d\colon E \rightarrow \mathbb{R}_{\ge 0}$.

   *Goal*: Given a vertex $u\in V$, find the shortest path to any vertex $v \in V$.

   The brute force way is to find all the path and find the minimu.
** Idea
   Maintain a set of vertices to where a shortest path from $u$ was found. And
   in each step we add one vertex to $W$.

   *Key observation*: If $P$ is a shortest $uv$ path, then for every $w$ on this
    path, $P[u, w]$, this is also the shortest path. ($P[u, w]$ represents the
    path from $u$ to $w$ through $P$.)
** Dijkstra's algorithm
   *Input* is a graph $G = (V, E)$ which is connected. [fn:2] We have a distance
   $d\colon E \rightarrow \mathbb{R}_{\ge 0}$.

   *Output*: For every vertex $u \in V$, the distance from $u$ and also a
   shortest path.
*** Algorithm
    *Initialization*: dist[u] = 0

    For every other vertex $v$, I set $d[v] = \infty$. $prev[v] =
    \textup{null}$. Maintain the set $W = \emptyset$.

    *Iteration*: Choose a vertex $v_0 = \min\{\dist[v]\colon v  \in V \setminus W\}$

    Update $W = W \cap \{v_0\}$.

    $\forall v \in V \setminus W$ if $\dist[v] > \dist[v_0] + d(v_0, v)$
    then $\dist[v] = \dist[v_0] + d(v_0, v)$ and $\prev[v] = v_0$.

    *Termination*: If $W = V$, then STOP and output $\dist[v]$ search head of
    $\prev$ for a $uv$ path.

    An example was done. [[https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm][Wikipedia]].
*** Analysis
**** Correctness
     *Claim*: At the time $v_0$ is put into $W$, $\dist[v_0]$ is the distance of
      $v_0$ to $u$.

      (This would prove the correctness, because $dist$ does not change after
      vertex is in $W$.)

      Proof: Induction on $\vert W\vert$.

      Because $\vert w \vert = 0$ $u$ is put into $W$, $\dist[u] = 0 = d(uu)$.

      Suppose $\vert W \vert \ge 1$, we put $v_0$ into $W$. If this is the case,
      then $\dist[v_0] = \min\{\dist[v_0]\colon v \in V \setminus W\}$.

      Suppose $\dist[v_0] > s(uv_0)$. (here $s$ is the shortest path going
      from one vertex to another.)

      Take the shortest $uv_0$ path $P$. There will be a first vertex on $P$ not
      in $W$, call it $v_f$ and $v_p$ be its predecessor. $\dist[v_0] > s(uv_0)
      = s(uv_f) + s(v_fv_0) \ge s(uv_f) = s(uv_p) + s(v_pv_f) = dist[v_p] +
      d(v_pv_f)$. (By our observation from before, both these paths are the
      shortest.)

      When we are updating after putting $v_p$ into $W$, we consider $v_f$ and
      we will put it in $W$. This is a contradiction.
**** Termination
     In each iterating step, one vertex is put into $W$ and stays there and then
     in $n$ iterations, we are done.
**** Cost
     Finding $v_0$, then $O(\vert V \vert)$.

     Adding $v_0$ to $W$ is $O(1)$

     Updating $\dist$,  $O(\vert V\vert)$.

     With better data structure $O(\vert E\vert + \vert V \vert log \vert V \vert)$.
** Euro 2020 or Travelling Salesman Problem
    Watch a game in every one of $13$ cities. We want to visit all $13$ but as
    cheap as possible. The English football fans cannot return to the same
    country. A $13$ vertex graph, between any two vertices, there is a price of
    the air ticket.

    We are looking for a Hamilton cycle.

    Given graph $G = (V, E)$ and $w\colon E \rightarrow \R_{\ge 0}$. A cycle
    that does not repeat.
** Complexity classes
   $\P$, polynomial time running problem.

   |    $n$ | $1000n$  | $1000n\log n$ | $10n^2$    | $2^n$           | $n!$            |
   |--------+----------+---------------+------------+-----------------+-----------------|
   |     10 | 0.01 sec | 0.0002 sec    | 0.001      | 0.0000001 sec   | 0.003 sec       |
   |    100 | 0.1 sec  | 0.001 sec     | 000001 sec | 400000 years    | $>10^100$ years |
   | 100000 | 17 min   | 20 sec        | 2450 min   | $>10^100$ years |                 |
* Lecture 4 <2018-10-24 Wed>
** Decision problems
   Problems that output yes or no
*** Example
    - Is there a spanning tree of weight $\le 42$. (Kruskal algorithm.)
    - Is there a path of weight $\le 405$ from $u$ to $v$? (Djistra's algorithm.)
** Class P
   The set of all decision problems with a polynomial time algorithm.
** Traveling salesman problem
   We don't know if the problem is in $\P$.

   As a decision problem: There is a graph $G = (V, E)$ and $w\colon E
   \rightarrow \R_{\ge 0}$., You ask what is the smallest weight Hamiltonian cycle. [fn:3]
*** Approximation algorithm
    *Definition*: An $\alpha$ approximation of TSP is an algorithm that turns a
     Hamiltonian cycle whose weight is within $\alpha$ fraction of the min
     weight Hamiltonian cycle.[fn:4]
*** Extra conditions
    Triangle inequality: the weight function satisfies the triangle inequality
    if every two vertices of the graph, the weight $w(xy) \le w(xz) + w(zy)$.

    Examples: The usual Euclidean distance satisfies this.
    A non-example is Airfare cost.
** Approximation algorithm for TSP
*** Algorithm
    *Input*: a weight function $w\colon E(K_n) \rightarrow \R_{\ge 0}$ with
    triangle inequality. (We assume that it is a complete graph.)

    *Output*: Hamiltonian cycle $C$.

    *Algorithm*:
    1. Find the minimum weight spanning tree (Kruskal algorithm.)
    2. From the spanning tree, we create a closed walk spanning all vertices by
       traversing each edge of $T$ twice in both directions.
    3. Traverse $W$, when hitting a vertex that was used before, we do a short
       cut. (Go instead to next vertex $W$) Do this iteratively.
    4 *Termination*: when all vertices are traversed, output $C$.

    We know that $w(W) = 2w(T)$ and $w(C) \ge w(W)$.

    $C^{*}$ is a minimum weight Hamilton cycle. How does this compare to the
    weight of the spanning tree. We know that $w(C^{*}) \ge w(T)$. and thus
    $w(C) \le 2 w(C^{*})$.
*** Running time
    1. Kruskal: $O(n^2\log n)$
    2. Closed walk $W$, $O(n)$.
    3. short cutting: $O(n)$.
** Hall's theorem
   If $G = (A \cap B, E)$ a bipartite graph, then $G$ has a matching $A$ if and
   only if for every subset $S \subset A$, $\vert N(S) \vert \ge \vert S \vert$.

   The non-trivial direction implies that when there is no matching saturating
   $A$, then there is an $S \subset A$, $\vert N(S) \vert < \vert S \vert$.
** Class $\NP$
   A decision problem is in class $\NP$ if the YES answer can be verified
   efficiently (within time that is polynomial in variable size.) (In other
   words, there is a polynomial size certificate.)

   The perfect matching problem is in NP. [fn:5]

   Opposite of perfect matching: Does $G$ has a $PM$? We can use Hall's
   condition as a certificate. Hence the problem is in NP.
** Class $\coNP$
   Means that the problem is in $NP$ and the negation of the problem is also in
   $NP$.
** About Hamilton path
   The Hamilton path problem is in $NP$.

   But the negation of the HAM is not known to be in $NP$. In other words, we
   don't know if HAM is $\coNP$.[fn:6]
** Problem reduction
   Maximum weight spanning tree problem can be reduced to a minimum weight
   spanning tree. (You can solve the minimum weight spanning tree problem by
   inverting the sign of the edges.) Furthermore, it is a polynomial time
   reduction.

   A problem is called $\NP$ hard if any problem in $\NP$ class can be reduced
   by the problem.

   If furthermore, the problem is in $\NP$, then we call it $\NP$ complete.

   Example: 3-SAT is $\NP$ hard and also $\NP$ complete.

   Karp came up with $21$ natural $\NP$ complete problems, all of them are $\NP$
   complete.
* Lecture 5 <2018-10-30 Tue>
** NP class
   A yes/no problem is in class NP if the answer yes can be verified
   efficiently.
*** Examples
    1. Does the bipartite graph have a perfect matching.
    2. Does the bipartite graph have no perfect matching.
    3. Does the graph have a Hamiltonian-cycle?
    4. *Don't know* Whether a problem have no hamiltonian cycle.
** P class
   A yes/no decision problem is in P if the answer can be found in polynomial
   time. It is obviously true that $P \subset NP$.
** Co-NP
   A yes/no problem is in the class Co-NP if the no-answer can be verified
   efficiently. Again trivially, $P \subset NP \cap no-NP$.
*** Example of NP intersection co-NP
    1. Perfect matching problem in bipartite graph is in the intersection.
    2. Is this graph 2-colorable.
    3. Is this graph Eulerian?[fn:7] Verify that the degree of each vertex is even.
       (polynomial time algorithm.) Another answer: The yes answer is the list
       of edges in an Eulerian edges. For the NO answer, we will be given a
       vertex of odd-degree.
** Conjecture P $\neq$ NP
** Stronger conjecture of $P \neq$ $NP$ intersection $co-NP$
   Is there a factor of $n < k$. This problem is in the intersection of NP and
   co-NP.

   Is $n$ a prime. This was also a problem. But in 2002, it was proven to be
   true. (The input size is in $\log n$.)

   A problem in $NP$ and co-NP and then trying to find a good characterization
   and then solving the problem.
** NP completeness
   Subtle difference between easy and hard problem.
   1. The graph is 2-colorable? is in P[fn:8]
   2. Is the graph 3-colorable? is in NP-complete.
   3. Is this planar graph 3-colorable? is in NP-complete.
   4. Is this planar graph 4-colorable? is in P. (The is in complexity class TRIVIAL)
      The answer is always yes.
** Hall's theorem
   If you have a graph $G$ that is bipartite, then $G$ has a perfect matching if
   and only if for every $S$ inside $A$, the $\vert N(S) \vert \ge \vert S
   \vert$ and for every $S \subset B$.
** Necessary conditions for Hamiltonianity
   Dirac's theorem $d(G) \ge n/2 \implies G$ is hamiltonian. [[https://en.wikipedia.org/wiki/Hamiltonian_path#Bondy%E2%80%93Chv%C3%A1tal_theorem][Wikipedia]] (This is
   a sufficient condition.) For a cycle, this fails.

   Proposition: If a graph $G$ is hamiltonian then $\forall S \subset V(G)$,
   $C(G\setminus S) \subset \vert S \vert$. (This is a necessary condition.)[fn:9] [fn:10]

   A simple example is an edge. It's probably also true for Peterson graph.

   We can try to frame something like if $t C(G\setminus S) \subset \vert S
   \vert$. For peterson graph $t = 4/3$. There is a conjecture on if we can talk
   about a value of $t$ and do stuff.
** Does a graph have a perfect matching? Tutte's theorem
   The question is whether this is in NP intersection co-NP.

   The hall's theorem was for bipartite graph.

   Consider $K_{2k+1}$. It has all the edges, but has no perfect matching. Odd
   (vertices) graphs are bad obviously.

   There was something about applying the necessary condition for Hamiltonian
   cycle to the matching problem and arriving at a necessary condition (and sufficient condition.)

   $G$ has a perfect matching $\implies$ $\forall S \subset V(S)$, $o(G
   \setminus S) \subset |S|$.[fn:11]

   *Proof*: Let $M$ be a perfect matching in $G$. In each odd component, there
   is at least one edge $e_L \in M$ which has one vertex in $b$ and the other in
   $S$. These edge $e_L$ are disjoint $\implies$ [fn:12] [fn:13]
** Proof of Tutte's theorem
   Let $G$ be a counter example with maximum number of edges.[fn:14]

   What is a counter example? It should satisfy the following properties:
   1. $G$ has no perfect matching
   2. $\forall S \subset V(G)$, $o(G \setminus S) \le \vert S \vert$

   Add $xy$ to $G$ and $G+xy$ is not a counter example. We claim that $\forall S
   \subset V(G)$, $o((G+xy)\setminus S) \le \vert S \vert$. [fn:15]

   I know that $o(G\setminus S) \le \vert S \vert$.
   - If $xy \in S = \emptyset \implies \vert S \vert$ does not decrease.
   - If $xy$ goes between even components, then nothing changes.
   - If $xy$ goes to an odd components, the number of odd components decreases.
     Basically do a case analysis and it checks out.

   $U = \{v \in V(G) \colon d(v) = n- 1\}$

   Case 1. $G \setminus U$ is the disjoint union of cliques. There are even
   cliques and odd cliques. Even cliques can be matching within themselves. In
   odd cliques, you match everything but one, but we can match the extra vertex
   to $U$. Now what happens with the vertices inside $U$ that doesn't get a pair
   in $U$. If that part is odd, then the whole thing is not odd. But it is not
   odd, because we have a contradiction when we put $S = \emptyset$. So after
   everything, the number of unmatched vertices is even (otherwise we have a
   contradiction.)

   Case 2. $G \setminus U$ is not a disjoint union of cliques. The idea is from
   two almost perfect matching of $G$, create a perfect matching of $G$ and two
   more edges, create a perfect matching. This leads to a contradiction.

   Claim: In $G, \exists x, u, v, w$ such that $xu, xv, \in E$, $uv, vw \notin
   V(G)$. $w$ is anything that is not in the neighbourhod of $x$ which is non
   empty.[fn:16]

   #+BEGIN_SRC artist

                        x .--------------------------- w
                         /.
                        /  -\
                      -/     \
                     /        -\
                    /           -\
                  -/              \
                 /                 -\
                /                    \
               /                      -\
             -/                         \
            /                            -\
           /                               -\
         -/                                  \
        /                                     -\
     u .----------------------------------------. v


   #+END_SRC
* Lecture 6 <2018-10-31 Wed>
** TODO Tutte's theorem proof
   $\Leftarrow$: G$, a counter example with maximum number of edges.

   *Claim*: $G+xy$ has a p.m. $xy\in E(G)$, $G$ has no p.m., $\forall S \in
    V(G)$, $o(G \setminus S) \le o(\vert S \vert)$

    $U = \{v \colon deg(v) = n-1\}$ and $n=\vert V(G)\vert$.

    Case 1: $G \setminus U$ is the union of cliques. We are done, we use Tutte's
    condition for empty set.

    Case 2: Otherwise, there exists the diagram that I already drew. Our claim
    implies that there exists a perfect matching $M_1$ in $G + xw$ and also
    there is a perfect matching in $G$ if one adds $uv$. Our goal is to find a
    perfect matching in $M_1 \cap M_2$. Our goal is to find a perfect matching
    in $M_1 \cap M_2 \setminus \{xw, uv\} \cap \{ux, xv\} \subset E(G)$.

    $M_1 \cap M_2$ is the disjoint union of $K_2$s and even cycles[fn:19]. The degree
    of each vertex in the union is either $1$ or $2$, because the matching is
    perfect because there are two of them. If there is one, then the vertex
    participates in the same edge with () matching $\implies$ $K_2$ component.

    If it is $2$ $\implies$ vertex participates in a cycle component.

    (cycle is even since edges of the matchings alternate.)

    There was a diagram and the proof involved doing stuff on the diagrams. I
    don't understand what he did.

    The proof in the class was from bondy and murthy. [[http://www.zib.de/groetschel/teaching/WS1314/BondyMurtyGTWA.pdf][Bondy and murthy]] page 76.

    The wikipedia link seems to have the same proof.
** Perfect matching is in NP intersection co-NP
   Tutte's theorem tells us that the problem is in the intersection of NP and
   co-NP. The certificate for the yes case is a matching and for the No case is
   a case where the Tutte's theorem is false.

   The problem is also in P.
** Corollary to hall theorem (Theorem of Frobenius)
   A $k$ regular bipartite graph has perfect matching.[fn:20] (1-factor)

   A $k$ factor is a spanning $k$ regular subgraph.

   This is not true for general graphs. Example: odd cycles, they are $2$
   regular and 1-factor. Are there examples with even number of vertices.
   (3-regular graph with no $1$ factor.)


** Theorem (peterson)
   A $2k$ regular subgraph has a $2$ factor.
** Theorem (another peterson theorem)
   Every $3$ regular graph without cut edges[fn:21] has a perfect matching. (Theorem in
   Bondy and Murthy) [fn:22]
*** Proof
    The proof is component wise. Now we assume that $G$ is connected.

    We will check that Tutte's condition holds. Then Tutte's theorem tells us
    that $G$ has a perfect matching.

    $S$ be an arbitrary subset.

    Consider the number of edges between odd components and $S$.

    Claim: For every odd component, there is at least three edges going to $S$
    from $C$.

    Proof:
    1. $0$ edges is not possible because connected.
    2. $1$ edge is
       not possible, because it would be a cut edge.
    3. $2$ edges are not possible
       because the sum of the degrees of the vertices inside the component -2,
       $\sum d(v) - 2 = 2 \cdot e(C)$. Now this is just a handshake lemma.[fn:23]

    The number of edges between odd components and $S$. The number of edges
    going is at least $3$ times the odd components. On the other hand, the
    number of components cannot be more than $3 \vert S \vert$.

    $$ 3\cdot o(G \setminus S) \le \textup{number of edges between odd components and S } \le 3 \cdot \vert S \vert$$

    Thus $\cdot o(G \setminus S) \le \vert S \vert$
** Maximum matching problem
   In the decision problem formulation. Is there a matching of size $k$ in the
   graph on $n$ vertices.

   Is this problem in NP intersection co-NP? The problem is obviously in NP.

   For Bipartite graphs, we can provide the other verification by Konig's
   theorem.
** Konig's theorem
   $G$ is bipartite, then $\alpha(G)=\beta(G)$. Here $\alpha$ is the size of the
   largest matching and $\beta$ is the size of smallest vertex cover.

   $C \subset V(G)$, the vertex cover if $\forall e \in E(G)$, $e\cap C \neq
   \emptyset$.
** Konigs on Maximal matching problem
   Suppose $\alpha(G) = 88$, then konig gives a certificate to show that there
   exists a vertex cover of size $88$. So this means that there are no matching
   of size more than $89$.
** Homework: a corollary of Tutt due to Berge
   If $G$ is a arbitrary graph, then it is true that $2\alpha'(G)$ is equal to
   the minimum of the following sum of quantities: $\min \{ n - o(G\setminus
   S) + \vert S \vert \colon S \subset V(G) \}$.[fn:24]

   It is easy to show one direction. But this is the maximum size, which is the homework.

   This example would put the problem of maximum matching into the intersection.
** How to find maximum matchings in polynomial time?
** Proposition about maximum matching
   IF $M \subset E(G)$ is a maximum matching of $G$, $\iff$ there is no
   $M$ augmenting path.

   $M$ augmenting path: It's a path in which non-edges and edges follow each
   other alternatively. One direction is easy. If $M$ is a maximum matching,
   then there is no $M$ augmenting path.
** A M alternating path
   A path of $G$ where edges of $M$ alternate with non-edges of $m$.

   An $M$ alternating path that starts and ends in an unsaturated vertex is
   called $M$ augmenting. [[https://en.wikipedia.org/wiki/Saturation_(graph_theory)][Wikipedia]]
** Using the characterization for Bipartite graph
   [fn:25] You have a matchin and then unsaturated vertices. The idea is to
   somehow extend the matching to the unsaturated edges.

   #+BEGIN_SRC artist
     -------------------------------------------------------------\------------------------------\---------------
     -                                                                                                           \------
     (                                                                                                                  )
     \              |           |              |              .                                                  /------
     \              |           |              |                            .                    /---------------
     \              |           |              |                  /------------------------------
     ---------------------------+--------------+------------------
                     |          |              |
                     |           \              \
                     |           |              |
                      \          |            --+----------------------------------------------------------------------------------------------------------------------
            ----------+----------+-----------/  |                                                                                                                      \---------------------------------
     ------/          |          |             .|                 .           .     .       .                                                                                                            \------------
     (                           |                                                                                                                                                                                    )
     ------\                                                                                                                                                                                             /------------
            ---------------------------------\                                                                                                                         /---------------------------------
                                              -------------------------------------------------------------------------------------------------------------------------



   #+END_SRC
* Lecture 7 <2018-11-06 Tue>
** Maximum matching is in P
*** Proposition
    $M \subset E(G)$ is a matching in $G$.

    $M$ is a maximum $\iff$ there is no $M$ augmenting path in $G$. (is
    $M$ alternating if it starts and ends at an unsaturated vertex.)
*** Augmenting path algorithm
    Input: Bipartite graph $G = (X \cap Y, E)$, a matching $M \subset E$.
    Output: Either an $M$ augmenting path or a cover of size $\vert M \vert$.

    Initialization: $S = U$, $Q = \emptyset$, $T = \emptyset$

    Iteration: If $Q = S$ STOP and return $M$ (as maximal matching), $TU(x
    \setminus S)$ (as min cover of size $\vert M \vert$) Else select $x \in S
    \setminus Q$, $\forall y \in N(x)$ with $xy\in M$, DO if $y$ is unsaturated,
    then stop return a $M$ augmenting path from $U$ to $y$. Else $\exists w \in
    X$, $yw \in M$ update, $T = T \cap \{y\}$ and $S = S \cup \{w\}$. Update $Q
    = Q \cup \{x\}$. [fn:26]
*** Proposition
    $G$ graph, $M \subset E(G)$ is a matching. $C \subset V(G) \implies \vert M
    \vert \le \vert C \vert$. Here $C$ is the cover.

    The idea is that every cover has to be bigger than the matching.
*** TODO Proof of correctness
    Stopping the algorithm: The algorithm can either stop with a $M$ augmenting
    path or it can stop with a maximum matching or a cover.

    Proof of correctness: If the algorithm terminates with a matching $M$ and a
    cover $T \cap (X \setminus S)$, we terminate at $Q = S$, which means that we
    have explored all the neighbours of $S$ and they are all in $T$. We want to
    conclude that there exists no edge between $S$ and $Y-T$. (Because if there
    is no edge between $T$ and $Y-T$, then $T$ together $S-T$ is a cover.)

    If there is an edge from $S$ to an unsaturated vertex $y \in T$, then we
    would have immediately put this vertex into $T$. These two cases are not
    possible.
*** Comments
    $\vert M \vert = \vert T \vert + \vert X \setminus S\vert$. By the selection
    of $S$ and $T$, vertices of $T$ are put into $T$ where their $M$ partner is
    put into $S$.

    $\implies$, $S = U \cap M$ partners of vertices in $T$.
*** Internet reference
    [[http://www.columbia.edu/~cs2035/courses/ieor8100.F12/lec4.pdf]]
** Theorem
    Repeatedly applying APA to bipartite graph produces a maximal matching and
    minimal cover. The running time is $O(V(G) \cdot e(G))$

    If we repeat APA $\le n/2$ times. One running of APA considers each edge
    $\le 1$, implies $O(e(G))$.
** Matching with weights
   We have a weight function on the edges. $w\colon E(K_{n, n}) \rightarrow \R$.

   The goal is to find a perfect matching $M$ such that the weight of the
   matching which is the sum $\sum w(e)$ is maximum.[fn:27]

   In general, we say that the weighted cover $W$ is $u_0, \cdots, u_n, v_1,
   \cdot, v_n$ such that $u_i+v_j \ge w_{ij}$ for all $i, j = 1, \cdots, n$. The
   cost of $(u, v)$, $c(u, v) = \sum u_i + \sum v_j$. (*The minimum weighted
   cover problem* is to find a cover of minimum weight.)

   The interesting part is that these two problems can be solved together.
** Duality lemma
   For all perfect matching $M$ and cover $(u, v)$ in a weighted bipartite graph
   $G$, $C(u, v) \ge w(M)$ (*Home work*)

*** Corollary
    If $C(u, v) = w(M)$, then $(u, v)$ is a min-cost cover and $M$ is a maximum
    weight matching.
** Algorithm for Maximal weighted matching
   Equality: Subgraph $G_{u, v} \subset K_{n, n}$, a spanning subgraph which has
   the same vertex set and the edges at those $x$ and $y$ where $w_{i, j} =
   u_i+u_j$.
** Hungarian algorithm
   Input: A matrix $w_{i, j}$ of weights of the edges of $K_{n, n}$ with points
   $X = \{x_1, \cdots, x_n\}$, $Y=\{y_1, \cdots, y_n\}$.

   Idea: Iteratively adjust a cover $(u, v)$ until $G_{u, v}$ has a perfect
   matching.

   if $G_{u, v}$ has a perfect matching, then $(u, v)$ and $M$ are both optimal.
   Initial $u_i = \max \{W_{i, j}\colon j=\{1, \cdots, n\}\}$ and $v_i =0$. Note
   that this is a cover and $u_i + v_j \ge w_{i, j}$ for all $i, j$.

   Iteration: Create $G_{u, v}$, using APA and find a maximal matching $M$ and a
   minimum vertex cover $Q$ and $Q$ will be equal to $T \cup R$ (where $T = Y
   \cap Q$ and $R = X \cup Q$)

   If $M$ is a perfect matching, then we are done. (By corollary of the duality
   lemma.)

   Else $\varepsilon = \min\{u_i + v_i -w_{i, j}\colon x_i \in X \setminus R,
   y_j \colon Y\setminus T\}$ (all elements are positive here.)

   We update as follows: $u_i = u_i - \varepsilon$ if $x \in X \setminus R$ and
   $V_j = v_j+\varepsilon$ if $y \in T$. Now you iterate.

   Why is the update $(u, v)$ still a cover? It involves 4 cases depending of
   where the pair $(i, j)$ goes to.

   1. If $x_i \in R$ and $y_j \in Y \setminus T$.
      $u_i, v_j$ are unchanged.
   2. $x_i \in R, y_j \in T$ implies that $u_i + v_j$ grew by $\varepsilon$ which
      is okay.
   3. $x_i \in X \setminus R$, $y_j \in T$, $u_i - \varepsilon, v_j +
      \varepsilon = u_i + v_j$
   4. x_i \in X \setminus R, y_j \in Y \setminus T$. So $u_i + v_j \ge w_{i,
      j}$.
* Lecture 8 <2018-10-31 Wed>
  *Input*: $(w_{i, j})_{i, j =1}^n$ weights or $E(K_{n, n})$, $X = \{x, \cdots,
   y_n\}$, $Y = \{y, \cdots, y_n\}$

   *Initialization*: $u_i = \max_\{w_{i, j}, j\cdots n\}$, $v_j = 0$

   *Iteration*: For $m$, $G_{u, v}$, $V(G_{u, v} = V(K_{n, n}), E(G_{u, v}) =
    \{x_iy_j \colon w_ij = u_i + v_j\}$

    Find a maximal matching $M \subset G_{u, v}$ and min vertex case $Q = T \cup
    R$.

    If $M$ is perfect matching, then return (as max weighted is perfect
    matching.), $R = X \cap Q, T = Y \cap Q$. $(u, v)$ as a minimum cost cover.

    Else $\epsilon = \min\{u_i + v_j - w_{ij}\colon x_i \in X \setminus R, y_j
    \in Y \setminus T \}$ Update $u_i = u_i - \epsilon$ if $x_i \in X \setminus
    R$ and $v_j = v_j + \epsilon$ if $y_j \in T$.

    *Remark*: $G$ is bipartite, define $w_{i, j} \iff w_{i, j} = 1 \iff x_iy_j
     \in E(G)$ vertex cover $G$ implies $u, v$ characteristic $(011)$ vectors of
     $C$, implies cover of $w_{ij}$, $w_{i, j} \le u_i + v_j$. True since if
     $w_{i, j} = 1$ then $x_iy_j \in E(G)$, then $x_i$ or $y_j \in C$, then
     $u_i$ or $v_j =1$.

   | x | 0 | 0 | 0 | 0 | 0 |
   |---+---+---+---+---+---|
   | 5 | 1 | 2 | 3 | 4 | 5 |
   | 8 | 6 | 7 | 8 | 7 | 2 |
   | 5 | 1 | 3 | 4 | 4 | 5 |
   | 8 | 3 | 6 | 2 | 8 | 7 |
   | 5 | 4 | 1 | 3 | 5 | 4 |

   Excess matrix
   | x | 0 | 0 | 0 | 0 | 0 |
   | 5 | 4 | 3 | 2 | 1 | 0 |
   | 8 | 2 | 1 | 0 | 1 | 6 |
   | 5 | 4 | 2 | 1 | 1 | 0 |
   | 8 | 5 | 2 | 6 | 0 | 1 |
   | 5 | 1 | 4 | 2 | 0 | 1 |

   Now we form the graph with $0$ edges and find a perfect matching.

   | x | 0 | 0 | 1 | 1 | 1 |
   |---+---+---+---+---+---|
   | 4 | 3 | 2 | 2 | 1 | 0 |
   | 7 | 1 | 0 | 0 | 1 | 6 |
   | 4 | 3 | 1 | 1 | 1 | 0 |
   | 7 | 4 | 1 | 6 | 0 | 1 |
   | 4 | 0 | 3 | 2 | 0 | 1 |

   | x |  1 |  0 |  1 |  2 |  2 |
   |---+----+----+----+----+----|
   | 3 |  3 |  1 |  1 |  1 | 0* |
   | 7 |  2 | 0* |  0 |  2 |  7 |
   | 3 |  3 |  0 | 0* |  1 |  0 |
   | 6 |  4 |  0 |  5 | 0* |  1 |
   | 3 | 0* |  2 |  1 |  0 |  1 |

   Now we end up with a perfect matching in the equality subgraph. (The ones
   labelled *)

   | x |  0 |  0 |  0 |  0 |  0 |
   |---+----+----+----+----+----|
   |   |  1 |  2 |  3 |  4 | 5* |
   |   |  6 | 7* |  8 |  7 |  2 |
   |   |  1 |  3 | 4* |  4 |  5 |
   |   |  3 |  6 |  2 | 8* |  7 |
   |   | 4* |  1 |  3 |  5 |  4 |

   The above table represents the cover in the original graph.

   Perfect matching of weight $5 + 7 + 8 + 4= 28$ and $C(u, v) = 3 + 7 + 3 + 6 +
   3+ 1 + 1 + 2 + 2 = 28$.
** Proof
   If we add edges at every step in the graph from which we get the matching, we
   would be done. But we are not exactly doing it.

   Observations: $\vert Q \vert = \vert M \vert$, no $M$ edge is covered twice by $Q$.

   $T = \{y \in Y \colon \exists M$ alternating $(U, y)$ path $\}$.

   $R = \{x \in X \colon \nexists M$ alternating $(U, Y)$ path $\}$.

   where $U = \{x \in X \colon x$ is $M$ unsaturated $\}$.

   For termination of the Hungarian algorithm, count for *the number of vertices
   that are reached from $U$ on an $M$ alternating path*. This quantity grows in
   each iteration. (or $M$ augmenting path, which implies that there is a larger
   matching.)

   The edges of $M$ alternating path starting at $U$ remain in $G_{u, v}$. Edges
   can be lost only between $T$ and $R$. But these edges are not participating
   in the alternating path. In $M$ alternating path, vertices of $T$ are only
   connected to vertices in $S = X - R$.

   By the choice of $\varepsilon$, there is at least one pair $x_i y_j$ such
   that $x_i \in X \setminus R$, $y_j \in T$,such that $x_i y_j$ is a new edge
   in the equality sub-graph.

   This means that after $\le \frac{n}{2}$ iterations, or $M$ unsaturated $y\in
   Y$ is reached via a $M$ alternating path, which means that the matching is
   growing, and the matching can grow at most $n/2$. Thus after $\frac{n^2}{4}$
   iterations.
** Connectivity problem
*** Definition (Vertex cut)
    A vertex cut of a graph $G$ is a set of vertices such that $G-S$ is
    disconnected.[fn:28]
*** Definition (Connectivity of G)
    Connectivity of $G$, denoted by $\kappa$, is the minimum size of the vertex
    cut. If your graph is disconnected to begin with, then this is zero.

    By definition, for a clique, $K(K_n) = n-1$. The empty graph is "considered"
    to be disconnected.
*** Examples
    $K(K_{n, m})= \min\{n, m\}$. Proof: $\le$, Given a vertex cut of size
    $\min\{m, n\}$, smaller side

    $\ge$ Now we remove $\{m, n\} - 1$, vertices of $S$, $K_\{n, m\} - S$, and
    through them everybody else can be reached.
*** Proposition
    For all $G$, $K(G) \le n-1$. The clique is $K(G) = n-1 \iff G = K_n$.

    $K(G) \le \delta(G)$, here $\delta$ is the min degree. This is kinda clear,
    because we can pick a vertex with the minimum degree and remove all it's
    neighbours.

    $K(Q_d)$, the one skeleton of the $d$ dimensional cube.

    $E(Q_d) = \{uv \colon$ $u$ and $v$ differ in exactly one coordinate $\}$.
    From the proposition, the minimum degree is $d$.

    I didn't write the rest of the argument. But doesn't look so hard. The proof
    was by induction.
* Lecture 9 <2018-11-07 Wed>
** Connectivity of graph
   $G \neq K_n$, then $K(G) = \min \{ \vert S \vert \colon G - S \textup{ is not
   connected } \}$.

   $G = K_n$, $K(G) = n-1$.
** Proposition
   $K(G) \le v(G) - 1$ and $K(G) \le \delta(G)$.
** Extremal questions
   Given $n$ and $k$, what is the smallest number $e$ of edges that there exists
   as $n$ vertex $k$ connected  graph with $e$ edges.
** Proposition
   For all $k \ge 2$, there exists a $k$ connected graph on $n$ vertices with
   the ceil of $(n-k)/2$ edges.
** Theorem (Chvatal-Erdos)
   If $G \neq K_2$ and its connectivity $K(G) \ge \alpha(G)$. $\alpha(G)$ is the
   size of the largest independent set of vertices. And $\alpha'$ is the size of
   the largest matching [fn:29]

   Then $G$ is hamiltonian.
*** Proof
    Take a cycle in $C \subset G$ which is the longest. If the length of $C$ is
    $n$, we are done. So we can assume the length of $C$ is less than $n$.

    Let $H$ be a component of $G -C$.

    $k \le \delta(G)$. The length of the longest cycle is at least $\delta$.
    Take the largest path in $G$ and the end points of $P$ only have neighbours
    on $P$, otherwise the path could be extended. The farthest neighbour of $w$
    of $x$ on $P$ has distance at least $\delta(C)$.

    This means that the segment $P[x, w] + xw$ forms a cycle of length $\ge
    \delta(G) + 1$.

    Back to $H$. Let name the vertices which have an edge to $H$ as $v_i$. There
    exits at least $k$ vertices on $C$ which have an edge to $H$ (this is
    because of $k$ connectedness; otherwise we could separate $C$ from $H$ by
    the deletion of strictly less than $k$ vertices, which is a contradiction.)

    Notice that two adjacent vertices in $C$ cannot have an edge to $H$,
    otherwise, we can extend $C$ by entering $H$ and coming back. We call
    $v_{i}^{+}$ denote the vertices that are the vertices that follows $v_i$ on
    the cycle. We now know that all off $v_{i}^{+}$ is distinct from $v_i$.

    We can see that there cannot be two edges between $v_{i}^{+}$ and
    $v_{j}^{+}$, one can see this by drawing an easy diagram, for one can form a
    bigger cycle than $C$, which is a contradiction.

    This implies that $\{v_i^+, \cdots, v_k^+\}$ is an independent set of size
    $k$. This union with any vertex in $H$ is an independent set. This is a
    contradiction because $\alpha(G) \ge k+1$, which is not what we assumed.
** Definition (Disconnecting set of edges)
   $F$ is a disconnecting set of edges if its removal makes the graph
   disconnected, i.e., $G-F$ is disconnected.

   $\kappa'(G) = \min\{\vert F \vert \colon F \subset E(G) \textup{ is a
   disconnecting of edges}\}$

   This is the *edge connectivity* of $G$. $G$ is called $k$ edge connected,
   $k'(G) \ge k$.
** Definition
   A subset $S \subset V(G)$, then an edge-cut of the Multi-Graph is an edge-set
   of the form $[S, \bar{S}] = \{xy \colon x = S, y \in \bar{S}\}$. For some
   subset $S, \emptyset \neq S \neq V(G)$. Here $\bar{S} = V(G)\setminus S$, the
   set theoretic complement.
** Observation
   $K'(G) = \min\{[S, \bar{S}] colon S\subset V(G), S\neq \emptyset, S \neq V(G)
   \}$
*** Proof
    If $F \subset E(G)$ is a disconnecting set of edges with $\vert F \vert
    \subset K'(G)$, then let $S$ be a components of $G - F$, then $[S, \bar{S}]
    \subset F$. Since $F$ is minimal, this implies that $F = [S, \bar{S}]$.
** Bounds
   1. Initial bound, $k'(G) \le \delta(G)$.
   2. $\kappa'(G) > \kappa(G)$ is true for simple graphs.

      *Proof*: The proof is easy. Try to show that there is a vertex cut of size
      $\kappa'(G)$ whose removal disconnects $G$.

      Take an edge cut such that the size $\vert[S, \bar{S}]\vert = K'(G)$.

      Case 1: $G$ contains $K_{\vert S \vert, \vert \bar{S} \vert}$ on $S$ and
      $\bar{S}$. Then the statement is trivially true, since $K'(G) = \vert S,
      \bar{S}\vert = \vert S \vert \vert \bar{S}\vert = \vert S \vert (n - \vert
      S \vert) \ge n - 1\ge \kappa(G)$.

      Case 2: We don't have a complete bipartite graph, so we can identify
      vertices that we will disconnect, $x\in S$, $y\in \bar{S}$ such that $xy
      \notin E(G)$.

      $T_1$ is the set of neighbours $N(x) \cap \bar{S}$ on the other side. $T_2
      = \{w \in S \setminus x \cap S \colon N(w) \cap \bar{S}\} \neq \emptyset$.
      The removal of $T_1$ and $T_2$ separate $x$ and $y$. Now $\vert T_1 \cap
      T_2 \vert \le \vert[S, \bar{S}]$. Because there is an injection from the
      left set to the other one, given by if $w\in T_1, take xw$. $w\in T_2
      \mapsto wu_w$.

      Note $S$ is just one side that is separate from the other side.
** Is graph $G$ $k$ connected?
   It's definitely in co-NP. If it's not $k$ connected, we get a set a vertices,
   and, we'll be able find it in polynomial time.

   But is it in $NP$?

   $1$ connectedness if in $P$.

   But $2$ connectedness is in $P$. There are only a $n^2$ number of vertices.
   Hence polynomial.

   But we want to know about $k$ connectivity, when $k$ is the function of $n$.
** Theorem (A characterization of $2$ connectivity)
   $G$ is $2$ connected if and only if for all $x, y \in V(G)$, they are on a
   cycle.[fn:30]

   $G$ is $2$ connected if and only if for all $x, y \in V(G)$ if there exist
   $2$ internally disjoint $x, y$ paths. [fn:31]
* Lecture 10 <2018-11-14 Wed>
** Menger's theorem (VMT)
   A graph $G$ is $k$ connected if and only if for all pairs of vertices $x, y
   \in V(G)$, there exits $K$ pairwise internally disjoint paths.
*** Remarks
    1. This implies that $k$ connectivity is in $NP$ intersection co-NP. It
       takes $O(kn)$ to check that they are internally disjoint. There are
       $O(n^2)$ pairs to check
    2. For $k=2$, the VMT says that $G$ is $2$ connected if and only if for
       every $x, y \in U(G)$ is a cycle.
*** Proof of ($\Leftarrow$) VMT
    Suppose for a contradiction that there are $k$ internally disjoint vertices
    between every vertices, and $\kappa(G) \le k-1$. Let $S$ be a such a
    separating set.

    Let $S$ be such a separating set which separates $x$ from $y$, then every
    $xy$ path passes through $S$, and pigeon-hole implies that two halfs share a
    vertex of $S$. This is a contradiction.
*** Proof of $\implies$ of VMT for $k=2$
    Assume $K(G) \ge 2$, need a cycle containing $x, y \forall$ pairs $x, y \in
    V(G)$.

    Induct on $d(x, y) = \{\vert e(P)\vert \colon P \textup{ a xy path }\}$.

    *Base case*: $d(x, y) = 1$, $\kappa'(G) \ge \kappa(G) \ge 2$. Implies $G
    \setminus \{xy\}$ is still connected $\implies xy$ path $P$ in $G\setminus
    e \implies P \cap \{e\}$ is a cycle which contains $x, y$.

    *Induction step*: $d(x, y) = d$, let $P$ be a path of length $d$. Let $v$ be
     the last vertex before $y$ on $P$. Induction says that some $C$ containing
     $x$ and $v$.

     If $y\in C$, we are done. If not, there exits a $xy$ path $Q$ in
     $G\setminus \{v\}$ as $G$ is $2$ connected. Now, it's easy to construct a
     cycle from $x$ to $y$.

     Let $z$ be the last $v_x$ of $C$ on $Q$ if it exits. Let $D$ be $C\setminus
     [z, v]$ such that $x \in D$. Now $C' = D \cup Q\vert[x, y] \cup \{xy\}$
     cycle containing $x$ and $y$.

     If no such $z$ exists, then it's straightforward to construct a cycle.
** Edge Menger's theorem
   A graph $G$ is $k$ connected if $\kappa'(G) \ge k$ if and only if for all $x,
   y \in V(G)$, there exists pairwise edge disjoint $x-y$ paths.
** Local version of Menger's theorem
*** Separating set
    1. A set $S \subset V(G)$ is an $xy$ separating set if $x$ and $y$ are in
       different connected components of $G\setminus S$.
    2. $\kappa(x, y) = \min\{\vert s \vert \colon S \textup{ is a xy separating set}\}$
    3. $\lambda(x,y) = \max$ size of a family of pairwise disjoint $xy$ paths.
*** Local version of theorem
    For all graphs $G$ and $x, y \in G$, we require that $x$ and $y$ are not
    adjacent edges.

    For all $x, y$, $\kappa'(x, y) = \lambda'(x, y) = \max$ size of family of
    pid $xy$ paths. Here $\kappa'$ is the minimum $\vert F \vert$ such that $x$
    and $y$ are in different components of $G\setminus F$.
*** Local theorem implies Global
    What we need to do is that for all $x, y \in G$, $\exists \ge k = \kappa(G)$
    pid paths.
    1. $x$ and $y$ are not adjacent (xy \neq E(G)) implies $\kappa(G) \le
       \kappa(x, y)\le \lambda(x, y)$. The local theorem says that the last term
       is at least $\lambda(x, y)$, implies $\ge k$ pid $xy$ paths.
    2. For $xy = e$, $\kappa(G-e) \ge K(G) - 1$. Suppose for a contradiction
       that $S$ separates $G-e$ and $\vert S \vert \le \kappa - 2 \le n-3$. We
       know as $\vert s \vert < K$, $G\setminus S$ is connected implies that
       $G\setminus S$ must have a "bridge" $e$. Meaning we have two disconnected
       sets only connected by $e$ between $T$ and $T'$.

       Without loss of generality $x \in T$, and $\vert T \vert \ge 2$. But then
       $S \cap \{x\}$ is a separating set of $G$ of size $\le \kappa - 1$.
    3. If $xy$ is an edge in $E$. Removing it means that $\kappa(G-e) \ge k-1$,
       then 1 implies that $k-1$ pid $xy$ paths in $G-e$ and $e=xy$ adds another
       path.
** Flow networks
*** Example
    A graph was drawn.
*** Definition
    A network is a quadruple $(D, s, t, c)$ where $D$ is a directed (multi)
    graph. $s \in V(D)$ is the source vertex and $t$ is the sink vertex. $c
    \colon E(D) \rightarrow \R^{+}$ is the capacity.

    A flow is *feasible* if
    1. For all $v \neq s, t$, the net flow in is equal to the net flow out, i.e,
       for all $v\neq s, t$, $\sum f(uv) =f^{-}(v) = f^{+}(v) = \sum(vu)$. This
       is called the /conservation constraints/. [fn:32]
    2. For all directed edges, we require that the flow on the edge is
       non-negative and at most the capacity. This is called /capacity
       constraint/

    The *value* of of a flow is the net flow into the sink, $f^{-1}(t) -
    f^{+}(t) = val(f)$.

    A *max flow* is a feasible flow with maximum $\val(f)$.
*** Problem
    Given a network flow, we need to find a max value and if possible find a max
    flow.
*** Homework
    For any $Q \subset V(D)$, with $s\in Q$ and $t\in \bar{Q} = V(D) \setminus
    Q$, $\val(f) = \sum_{e\in [Q, \bar{Q}]} f(e) - \sum_{e\in [\bar{Q}, Q]}
    f(e)$
*** Definition
    Given $Q \subset V(D)$, $\bar{Q} = V \setminus Q$, with $s\in Q, t\in
    \bar{Q}$, the capacity of the cut is capacity $\cap[Q, \bar{Q}] = \sum_{e\in
    [Q, \bar{Q}] c(e)}$
*** Lemma
    Weak duality: If $f$ is a feasible flow and $[Q, \bar{Q}]$ a source/sink
    cut, then the value of the flow $\val(f) \le \cap([Q, \bar{Q}])$
**** Proof
     Fix $f$ and $[Q, \bar{Q}]$. $\val(f) = \sum_{[Q, \bar{Q}]} f(e) -
     \sum_{[\bar{Q}, Q]} f(e) \le \sum_{[Q, \bar{Q}]} c(e) - 0 = \cap([Q,
     \bar{Q}])$
** Theorem (Ford-Fulkerson) (Max-flow min-cut theorem)
   Let $f$ be a flow of max value and $[Q, \bar{Q}]$ a source-sink cut of
   minimum capacity, then the $\val(f) = \cap([Q, \bar{Q}])$.
* Lecture 11 <2018-11-20 Tue>
** Network
   $D$ is a directed multigraph. $s = V(D)$, the source vertex and $t \in V(D)$
   the sink vertex and $c\colon E(D) \rightarrow \R_{\ge 0}$ capacity
   function.

   *Flow* is a function $f\colon E(D) \rightarrow \R$

   Flow is function $E(D) \rightarrow \R$. For feasible flow
   1. For every $v$ that is not $s$ and $t$, $\sum_{(v,u) \in E} f(v, u) =
      f^{+}(v)$
   2. $\forall e \in E$, $0 \le f(e) \le c(e)$. The value of the flow is
      whatever goes into the sink $\val(f) = f^{-}(t) - f^{+}(t)$.

   The *maximum flow* is a feasible flow of maximum value.
** Weak duality
   For every feasible flow $f$ and source sink cut $[S, \bar{S}]$, the value of
   the $f \le \cap(S, \bar{S})$, in particular, the equality happens if $f$ is a
   maximum flow and $[S, \bar{S}]$ is a min-cut.

   $[S, \bar{S}] = \{(u, v) \in E(D) \colon u \in S, v \in \bar{S}\}$ is a
   source sink cut if $s\in S$ and $t\in \bar{S}$.

   $[S, \bar{S}]$ is a min cut if its capacity $\cap(S, \bar{S}) = \sum_{u \in
   S, v \in \bar{S}} c(u, v)$ is minimum among source/sink cut.
** Max Flow Min cut theorem (Ford-Fulkerson, 1956)
   Let $(D, s, t, c)$ be a network. Let $f_{\max}$ be a max flow on $D$ and
   $[S_{\min}, \bar{S}_{\min}]$ a min cut, then $\val(f) = \cap (S_{\min},
   \bar{S}_{\min})$.

   [[https://en.wikipedia.org/wiki/Ford%E2%80%93Fulkerson_algorithm][Wikipedia]]
*** Proof
    $\le$ direction is just the weak duality.

    $\ge$ direction. We will find a source sink cut such that the capacity of
    the source sink cut is equal to the value of the maximal flow.

    That is, $\cap(S, \bar{S}) = \val(f_\max)$.

    $S = \{v \in V(D) \colon \exits f \textup{augmenting path from s to v}\}$

    Observation $t \in \bar{S}$ and $s \in S$. This is because if there is a $f$
    augmenting path from $s$ to $t$, then $f$ is not maximum, but we know that
    $f$ is maximum, a contradiction.[fn:33]

    $t\in \bar{S}$, $s\in S$,
    1. $[S, \bar{S}]$ is a source/sink cut.
    2. $(u, v) \in [S, \bar{S}]$, $f(u,v) = c(u, v)$. $(u, v) \in [\bar{S}, S]$
       implies that $f(u, v) = 0$. Because if not, we can add them to $S$.

    Now we see that the capacity of the cut $\cap(S, \bar{S}) = \sum c(u, v) =
    \sum_{(u, v) \in [S, \bar{S}] f(u, v) - \sum_{(u, v) \in [\bar{S}, S]} f(u,
    v) = \val(f_\max)$ the last statement is a homework problem.
** Augmenting path
   An $s, t$ path $s = v_0e_1v_1e_2v_2 \cdots v_{k-1}e_k v_{k} = t$ in the
   underlying undirected graph $G$ of a network $D$. This is called an
   $f$-augmenting path if for every $i$,
   1. $f(e_i) < c(e_i)$ is true if $e_i$ is a "forward" edge.
   2. or $f(e_i) > 0$ if $e_i$ is a backward edge.

   The tolerance of a $f$ augmenting path $P$ is just the minimum of the values
   $\min \{E(e) \colon e \in E(P)\}$ where $E(e) =$, $c(e) - f(e)$ if $e$ is
   forwards and $f(e)$ when $e$ is backward.

   If we define a $f$ augmenting path, we can improve the value of the path. You
   can improve the value of the path by the tolerance of the path.
** Lemma
   Take a feasible flow and an $f$ augmenting path with tolerance $z$, then we
   define a new flow $f'$ on the edges such that $f'(e) = f(e) + z$ if $e$ is
   forward in $P$ and $f(e) - z$ if $e$ is a backward edge. And if the edge is
   not on the path, you do nothing.

   Then $f'$ is feasible and the $\val(f') = \val(f) + z$.
*** Proof
    Capacity constraints hold by the definition of $z$.

    Conservation constraints holds because: if we have a vertex $v\in V(P)$,
    then.

    We had four cases and four diagrams. Basically given a vertex, there is one
    vertex leaves the vertex and one vertex that comes to $z$. We can argue for
    each of these cases.
** Corollary
   If there exits an $f$ augmenting path implies that $f$ is not maximum.
** Local Vertex Menger's theorem
   For all $x, y \in V(G)$, $xy\notin E(G)$, $\kappa(x, y) = \lambda(x, y) =
   \max\{\vert P \vert \colon P$ is a set of pairwise internally disjoint $x, y$
   path $\}$.
*** Proof
    The idea is to build a network so that the Ford-Fulkerson theorem implies
    it.

    We choose the following network: $(D, x^{+}, y^{-}, c)$,

    $V(D) = \{v^{-1}, v^{+}, v\in V(G)\}$,

    $E(D) =\{(u^{+}, v^{-} \colon uv \in E(G)\} \cap \{(v^{-1}, v^{+} \colon v
    \in V(G)\}$.

    $c(v^{-}, v^{+}) = 1$ for all $v \in V(G)$, and $c(u^{+}, v^{-}) = \infty$.

    If there is an $xy$ separating set in $G$, this corresponds to a source sink
    cut in $D$. If $C$ is an $xy$ separating set, i.e., $C$ contains $x$ and
    $\bar{C}$ contains $y$. We'll first show that the value of minimum cut is
    exactly equal to $\kappa(x, y)$.

    $s = \{v^-, v^{+}, v\in Q\} \cap \{u^{-1}\colon u \in C\} \subset V(D)$ and
    $\operatorname{cap} (S, \bar{S}) = \vert C \vert$. If $v \in S$, there exits no
    $v^{+}w^{-}$ edge to $w \notin S \cap Q$.

    Minimum cut $[S, \bar{S}] \le K(x, y)$

    $\ge$ take a minimum cut in $D$. There does not exist an edge $v^{+}u^{-}
    \in [S, \bar{S}]$.

    The set $C = \{u \in V(G) \colon (u^{-1}, u^{+}) \in [S, \bar{S}]\}$ is an
    $x, y$ separating set in $G$ and the size $\vert C \vert = \operatorname{cap}(S,
    \bar{S})$.

    $\lambda(x, y) \le \operatorname{maxval}(f) = \mincap(S, \bar{S}) =
    \kappa(x, y)$ If you have a family of internally disjoint, this in the
    network correspond to several flows in the network. You definitely could
    create a flow.

    Actually this part is kinda obvious. We need to come up with an algorithm
    that produces internally disjoint paths for max capacity. We can get this
    from the Ford-Fulkerson algorithm.
* Lecture 12 <2018-11-21 Wed>
** LMVT proof
   $V(D) = \{u^{+}, u^{-}, u \in V(D)\}$

   $E(D) = \{u^{+}, v^{-1}, uv \in E(G)\} \cup \{(v^{-1}, v^{+}\colon v\in
   V(G)\}$

   $c(v^{-}, v^{+} =1$ for all $v \in V(D)$.

   $c(u^+, v^-) = \infty$ for all $uv \in E(D)$

   $(D, x^+, y^-, c)$

   The maximum value of a feasible flow is the minimum capacity $(S, \bar{S}) =
   \kappa(x, y)$.
** Ford Fulkerson algorithm
   *Initialization*: Network $(D, s, t, c)$. Choose an initial flow $f \eq 0$.

   *Iteration*: Look for augmenting paths to improve the algorithm. Explore
    network for $f$ augmenting paths starting from $s$ (can be done using BFS.)
    Collect vertices reached in set $S$.

    Once you're done we have two cases. You didn't have an augmenting path which
    implies that we have reached a maximum. If there is $t\in S$, return
    augmenting path and improve the flow.

    If $t\in S$, return augmenting path and an improved flow.

    else return $f$ as max flow and $[S, \bar{S}]$ as min cut with $\val(t) =
    \cap(S, \bar{S}$

*** Integrability theorem
    If $c$ is a function from the edges to the natural numbers, then the
    Ford-Fulkerson algorithm terminates with a flow with integer values.
**** Proof
     Proof is by induction on the number of times the flow has been improved.
     And noting that the tolerance of any $f$ augmenting path is integer since
     both are integers and hence the improved flow will also be an integer.

     The algorithm hence terminates in at most maximum of the capacity steps.
*** Corollary
    If the capacities are integers, then there exists a maximum flow that can be
    represented as the flow of $\val(f)$ many unit flows $f = g_1 + g_2 +
    \cdots + g_\val(f)$ with $\val g_i = 1$.
*** Continuation
    By the IT theorem there exist flows $g_1, \cdots, g_k$ from $x^+$ to $y^-$
    each of these are the form $x^+v_{i,1}^-v_{i,1}^+v_{i, 2}^+, \cdots,
    v_{i1,l_1}^-v_{i1,l_1}^+y^-$

    Hence the $xy$ path $xv_{i, 1}v_{i, 2} \cdots v_{i, e_1}y$ in $G$ are
    pairwise internally disjoint paths.
** Partitioning triples
   We want to partition $\binom{n}{3}$ triples into "perfect matchings" that are
   pairwise disjoint. You need to have $3 \vert n$

   This number will turn out to be $\frac{n}{3} \cdot \frac{(n-1)(n-3)}{2}$.

   For $6$ vertices, we could take the set and the complement. Which is just
   $\binom63$.[fn:36]
** General version of Partitioning
   Let $k, n$ integers and $k\vert n$. Is it possible to partition the family
   into $\binom{n}{k}$ sets into pairwise disjoint perfect matchings. Perfect
   matching is a set of $n/k$ pairwise disjoint edges.

   The answer is yes for every $k$.
*** Definition
    $m = n/k$ is the number of sets in the perfect matching.

    $M = \binom{n}{k}/(n/k) = \binom{n-1}{k-1}$ The number of perfect matchings needed

    Given an integer $1\le l \le n$ a $m$ partition of $[l]$ is a multi-set $r
    = \{A_1, \cdots, A_m\}$ consisting of pairwise disjoint sets whose union is
    $[l]$. ([fn:35])
*** Remark
    Restriction of a perfect matching to $[l]$ is a $m$ partition.
*** Proposition
    For every $l$ there exists $M$, $m$ partitions of $[l]$, called $A_1,
    \cdots, A_2, \cdots, A_M$ such that for every subset of $[l]$, occurs in
    exactly $\binom{n-l}{k-\vert S\vert}$ of the $m$ partitions. (The empty set is always
    counted with multiplicity)
*** Remarks
    1. If $\vert S \vert > k$, the statement says it occurs $0$ times. Only $\vert
       S \vert \le k$ occurs.
    2. $l=n$, There exists $M$, $m$ partitions $=0$ and $1$ when $k = \vert S
       \vert$. $M$, $m$ partitions in which every $k$ occurs exactly once and
       nothing else. This means that there exists a partition $A_1, \cdots, A_M$
       are perfect matchings consisting of $k$ sets that partition
       $\binom{n}{k}$. Thus the theorem is proved.
*** Proof of Proposition
    When $l = 1$, $A_i = \{\{1\}, \emptyset, \emptyset, \cdots, \emptyset\}$
    (the empty set occurs for $m-1$ number of times.)

    If $s = \{1\}$, then $s$ occurs $M$ times. $\binom{n-l}{k-s} =
    \binom{n-1}{k-1} = M$

    When $s$ is the empty set. Then $S$ should occur $\binom{n-1}{k-0} =
    \binom{n-1}(k}$. Count: $M(m-1) = \binom{n-1}{k-1}(n/k - 1)$ this is true as
    well. I missed the rest of the proof.
* Lecture 13 <2018-11-27 Tue>
** Colorings of graphs
*** Example
    100 employees and $6$ projects. They have meetings. The company tries to
    schedule the meetings. Some employees are in different projects. So you
    cannot schedule them at the same time.

    The projects: $A_1, A_2, \cdots, A_6 \subset S$

    Schedule: $A_i \cap A_j \neq \emptyset \implies$, meeting times are
    different from $A_j$. Minimize the number of hours your teams spend with
    meetings.

    We form a graph with $A_i$ and form edges when $A_i \cap A_j \neq
    \emptyset$. We want to color the graphs so that the adjacent vertices have
    different color.
** Definition
   $X \colon V(G)\rightarrow [s]$ is a proper vertex colouring of $G$ if
   $\forall uv \in E(G)$, $X(u) \neq X(v)$.

   $G$ is $k$ colorable if there is a $k$ coloring of it.

   The chromatic number is the minimum number of colors that can make it
   colorable.
** Complexity
   Is this graph $k$ colorable? A decision problem.

   For $k=2$, it is equivalent to being bipartite. And is in P for this case.

   For $k \ge 3$, the problem is NP-complete. It is easy to show that the
   problem is in NP.

   It is not know that the problem is in Co-NP.
** Giving lower bounds for chromatic number
   $X(G) \ge \omega(G)$. This is an obvious bound.
** Proposition
   $X(G) \ge \frac{v(G)}{\alpha(G)}$
*** Proof
    Was too quick
** Tightness of $X(G)$ and $\omega(G)$
   Cliques, Bipartite graph (and complements), perfect graphs.
** Non-tightness $X(G) > \omega(G)$
   A simple example is odd cycle. Also for their complements.

   [[https://en.wikipedia.org/wiki/Mycielskian][Wikipedia]]: Mycielskian construction. $\omega(G) = 2$, but $X(G) > 10^10$.

   Random graphs: $G = G(n, 1/2)$ and $\omega(G) = 2\log n$ and $X(G) = n/(2
   \log n)$. (This wasn't proven.) The result shows that the last proposition is
   asymptotically tight.
** Hajos conjecture
   [[https://en.wikipedia.org/wiki/Haj%C3%B3s_construction][Wikipedia]] There exists $K_r$ subdivisions $\Leftimplies X(G) \ge r$.

   True for $r = 2, 3$. For 3, we have an odd cycle.
** Dirac theorem
   Hajos conjecture is true for $k=4$.
*** Proof
    We'll use induction on $n$, the number of vertices.

    Base case: $n=4$, the smallest $n$ for which the graph is $3$ colorable.
    $K_4$ is the only graph on $4$ vertices that is non 3 colorable.

    $n>4$, we can without loss of generality, assume that $G$ is $4$ critical.
    Meaning that if you remove an edge or vertex will make it $3$ colorable. The
    chromatic number $X(H) \le 3$ and $X(G) = 4$. We'll show that this will be
    $K_4$ divisible.

    Case 0: $\kappa(G) = 0$. Take a 4 chromatic component subgraph which is proper.

    Case 1: $\kappa(G) = 1$. You have a cut vertex. Then $G$ is not $4$
    critical. This is because you can color the lobes with $3$ colors and name
    the color of $v$ in each $1$. Then the union of these colorings is a proper
    $3$ colouring of $G$.

    Case 2: $\kappa(G) = 2$. Take a cut set $S \subset V, \vert S \vert = 2$.
    Call $S = \{x, y\}$. Color each $S$ lobe with $3$ colors, rename colors in
    each lobe such that $x$ is colored the same in each lobe and $y$ is colored
    the same in each lobe. But this would still have a problem. When is this
    possible? An example is when $x$ and $y$ have an edge between them.

    Case 2b: $xy \notin E(G)$ for all $S$ lobe there exists a proper $3$
    coloring $x$ and $y$ have different colors. If there is an $S$ lobe with no
    proper $3$ coloring with $x$ and $y$ have different color. This implies that
    $X(H + xy) = 4$ ($\iff \forall$ 3 coloring of $H$, $X(x) = X(y)$)

    There exists $K_4$ subdivisions $L \subset H + xy$ if $xy \notin E(L)$, $L
    \subset H \subset G$ if $xy \in E(L)$. If $xy \in E(L)$, create $K_4$
    subdivisions $L^{*}$ from $L$ by deleting and replacing it with an $x, y$
    path in a S-lobe different from $H$.

    Case 3: $K(G) \ge 3$. $G-x$ is $2$ connected, implies that there is a cycle in
    it (Mengers theorem), A cycle is a $K_3$ subdivision. We need to somehow get
    $K_4$ subdivision.

    We add a new vertex $v$ and connect three vertices on this cycle. Now the
    graph is $3$ connected. There are three pairwise internally disjoint paths
    between $x$ and $v$. This can be used to construct a $K_4$ subdivision.

    A lemma: If $G$ is $k$ connected and $G^{*}$ is an extra vertex plus edges
    to $k$ vertices of $G$, then $G^{*}$ is also $k$ connected.
** Counter examples of Hajos theorem for $k\ge 7$
   Still open for $k = 5, 6$.
** Hadwiger Conjecture
   $X(G) \ge k$ implies that $K_r$ minor.
* Lecture 14 <2018-11-27 Tue>
** Proposition
   For all $X(G) \le \Delta(G) + 1$
*** Proof
    A greedy algorithm.

    Input: ordering of the vertices, $v_1, \cdots, v_n$ of $V(G)$

    We color $v_i$ with the smallest available color. Basically take the
    element, look at the neighbours, and assign the element a color that is the
    minimum of number that doesn't clash with neighbours.

    Claim: The greedy algorithm produces a proper coloring of $G$ for every
    ordering of $V(G)$.

    The number of colors of the greedy algorithm uses is not more than the
    $\Delta(G) + 1$.
*** Characterize the extremal cases
** Theorem (Brooks)
   $G$ is connected, then $X(G) = \Delta(G) + 1 \iff G = K_n or C_{2l+1}$
*** Proof
    One direction is obvious ($\Leftarrow$)

    Approach: Try to Color with greedy with different orderings and conclude
    that the only orderings we can't do it is with $\Delta$ colours is when $G =
    K_n$ and $C_{2l+1}$.

    Idea: We take a spanning tree with root $r$. From the spanning tree, create
    an ordering $\pi_T$ of $V(G)$ such that we order from leaf to root. With the
    property so that every vertex precedes all vertices on the unique path in
    $T$ from $u$ to $r$.

    We can see that $G$ is regular (meaning that every vertex have the same
    number of neighbours)

    run the greedy algorithm with $\pi_t$, then $\max \vert \{\cdots \}\vert \le
    \max \{\max\{d(v_{\pi_t(i)}) - 1\}, d(v_{\pi_t(u)})\}$

    For similar reason, we can say that $G$ has no cut vertex. Otherwise, if $x$
    is a cut-vertex, then, take a spanning tree with root $x$ and look at the
    lobes $S_1, \cdots, S_l$. Color each lobe greedily with $x$ as the root.
    This is possible with at most $\Delta(G)$ colors, since the degree of
    $d_s(x) \le \Delta(s) - 1$. Then put together all the colors.

    So $G$ must be at least $2$ connected.

    We are still in $T$ ordering.

    If there are two edges that $v_1, v_2$ doesn't have an edge and $G- \{v_1,
    v_2\}$ is not connected. If this is a strongly connected graph, then we will
    be able to succeed. And then order them like $v_1, v_2, v_3, \cdots, v_n$.
    Where $v_n$ is connected to $v_1$ and $v_2$. Then we can get a contradiction
    doing the greedy coloring.

    Take a vertex $x$, $d(x) < n-1$. Take a $y\in N(x)$ such that $z \in N(y)
    \setminus N(x)$, $K(G - x) \ge 2$. This means that $\kappa(G - x - y) \ge
    1$. This is a contradiction.

    Case 2: $\kappa(G - x) = 1$. We need a structure theorem. And we'll come
    back to this.
** Definition
   Given a graph $G$, a block is a set of vertices $B \subset G$, is a maximal
   connected subgraph of $G$ without a cut vertex. Means that $B$ itself is $2$
   connected and for every $B'$, such that $B \subset B' \subset G$, $B'$ is not
   $2$ connected.
** Lemma
   $B_1, B_2$ are blocks of $G$, then $\vert V(B_1) \cap V(B_2) \vert \le 1$.
*** Proof
    If $\vert V(B_1) \cap V(B_2) \ge 2$, $B_1$ and $B_2$ are $2$ connected,
    then, $x$ and $y$ is indeed a section $V(B_1) \cap V(B_2)$, $x \neq y$. We
    want $z_1, z_2 \in V(B_1) \cap V(B_2)$ cycle $z_1, z_2 \in C$. They are
    disjoint (the lemma that we used yesterday.) This means that $B_1 \cap B_2$
    are $2$ connected and we have a contradiction
** Some structure
   The block/cut-vertex graph of $G$ is a bipartite graph with vertex set $\{B
   \subset G\colon B \textup{ is a block}\} \cup \{v \in V(G) \colon v \textup{
   is a cut vertex}\}$

   Block $B$ is adjacent to cut-vertex $v$ if $v \in V(B)$
** Proposition
   The block/cut-vertex graph of a connected graph is a tree.
*** Proof
    Connectedness follows from the connectedness of $G$. Every vertex is
    contained in at least one block.

    Acylicity? If there is a cycle of blocks, then take a shortest cycle and now
    create a cycle through each of these paths and get a larger block.

    Take shortest cycle $B_1 v_1 B_2v_2 \cdots B_lv_l \implies B_1 \cap \cdots
    B_l$ is also $2$ connected, because pair of vertices are on a cycle. Now a
    contradiction to the maximality of $B_i$.
** Going back
   We were doing $K(G - x) = 1$.

   Now use the structure theorem to $K(G-x)$. Adding back $x$, $x$ must have a
   neighbour in each of the leaf block that is not a cut vertex. Otherwise, $G$
   is not $2$ connected. There exists at least $2$ leaf blocks.

   We can pick $v_1, v_2$ in two different leaf block. If we remove $v_1$ and
   $v_2$, $G-x$ should remain connected.

   If $G \neq G$, $d(x) \ge 3$, so $G - v_1 - v_2$ is connected.
* Lecture 15 <2018-12-05 Wed>
** Formalizing a certain graph coloring
   Given a graph $G$, an assignment of subsets (lists) to the vertices $L \colon
   V(G) \rightarrow 2^\N = P(\N)$, then $f\colon V(G) \rightarrow \N$ is a
   proper $L$ list coloring with respect to $L$ if for every $v \in V(G)$, the
   color $c(v) \in L(v)$ (choose color from representative list) and $\forall xy
   \in E(G), c(x) \neq c(y)$ (proper)
** Remark
   Usual proper coloring is just a list coloring with $L(v) = [k]$.

   Graph $G$ is $k$ list colorable (k-choosable) if $\forall L \colon V(G)
   \mapsto \binom\N k$, there is a proper list coloring with respect to $L$.

   The parameter that we're looking for is $\chi_l(G) = \min \{k \colon G
   \textup{is k-colorable}\}$.
** Claim
   $\chi_l(G) \ge \chi(G)$
*** Proof
    If a graph is $k$ list colorable, then it is also $k$ colorable.

    We have already seen that for $\chi_l(K_{3, 3}) > \chi (K_{3, 3}) = 2$. How
    did we prove this? We gave certain lists with $2$ elements. But $K_{3, 3}$
    is not properly list colorable with respect to $L$.
** Example
   $\chi_l(K_{2, 2}) = 2 = \chi(K_{2, 2})$.

   We know that it is at least $2$, by previous claim. To show the other way, we
   need some stupid case analysis.
** Proposition
   $\chi_l(C) \le \Delta(G) + 1$, the proof is identical and uses greedy
   coloring algorithm with arbitrary order.
*** Proof
    Take an arbitrary assignment $L \colon V(G) \rightarrow \binom\N{\Delta +
    1}$, run the greedy algorithm. In round $i$, we color $v_i$ and look at the
    neighbours. Since the degree is at most $\Delta(G)$, we'll be able to assign
    a color from $\{1, \cdots, \Delta + 1\}$.
** Complexity
   It's not known to be in NP or in co-NP.
** Proposition
   For every $k, \chi_l(K_{m, m}) > k$, where $m = \binom{2k-1}{k}$

   We know that the regular coloring of this should be $2$, but the list
   coloring can be large.
*** Proof
    It is similar to how we did $K_{3, 3}$.

    For each vertex, we assign it a $k$ subset of $2k-1$. We can assign each $m$
    in the top and bottom uniquely in this way.

    We look at the colors on the bottom vertices and on the top vertices.
    Because it is a complete graph, these should be disjoint. This means that
    one of the top or bottom should have colors of size $< k$.

    Let's say $C_x$ and $C_y$ are the top and bottom vertices.

    Say $\vert C_x \vert \le k - 1$. Take a list $L(v_0) \cap C_x \neq
    \emptyset$, but this is a contradiction. This is a contradiction.
** 4 color theorem
   If $P$ is a planar graph, then one can color it with $4$ colors. Proof used
   computers.
** 5 color theorem
   A planar graph has 5 coloring. What about list coloring? Can planar graphs be
   colored by $4, 5, 6$ colors?

   It was discovered that there were planar graphs that were not $4$ list
   colorable.

   A Mirzakhani graph is an example of a planar graph with $\chi_l(G) \ge 5$.
   The graph wasn't the first example, but a simple one.
** Theorem (Thomasson)
   Every planar graph is $5$ list colorable.
*** Proof using induction
    Stronger statement: If $G$ is a plane graph, with an outer face bounded by a
    cycle $C$ and $L \colon V(G) \righatrrow 2^\N$ is a list assignment such
    that
    1. $\exists v_1v_2 \in E(C)$, $\vert L(v_1) \vert = \vertL(v_2)\vert = 1$,
       $L(v_1) \cap L(v_2) = \emptyset$.
    2. $\forall v \in V(C) \setminus\{v_1, v_2\}$, $\vert L(v)\vert = 3$.
    3. $\forall v \in V(G) \setminus V(C)$, $\vert L(v)\vert = 5$

    Then $G$ is list colorable.

    Embed $G$ into the plane without crossing and if the boundary of the outside
    face is not a cycle, then add edges until it is.

    We use induction on our stronger statement.

    The base case if $V(G) = 3$. The graph would be a triangle. For $v_1$ and
    $v_2$, we have fixed colors, and $v_3$ is a list of size $3$, and you'll
    always have an available color.

    *Case 1*: There exists a chord $xy \in E(G)$ of the cycle $C$. $x$ and $y \in
    V(C)$ but $xy \notin E(C)$. $v(C_1) \cap v(C_2) = \{x, y\}$. Say $v_1, v_2
    \in V(C_1)$. List color $C_1$ with respect to $L$ by induction.

    There are two sides $C_1$ and $C_2$. Use induction to color $C_1$ and the
    end vertex will have a color. Now color $C_2$ and we're done. It is
    important that there is no edge between $C_1$ and $C_2$.

    *Case 2*: There is no chord between two vertices from the top to bottom.
     Let's add edges to make a triangulation, except the outside faces. (we can
     do this at the beginning.)

     The idea is that we'll use induction on the graph $G-\{v_3\}$. Here $v_3$
     next to $v_2$ in the outer circle.

     Let $x, y \in L(v_3) \setminus L(v_t)$, for every vertex $v_t$ that is a
     neighbour of $v_3$, but, not on the cycle. $L^{*}(v) = L(v) \setminus\{x,
     y\}$, and now color by induction. Extend $c(v_3)$, choosing a color $x$ or
     $y$, which is not equal to the color of $v_{3+1}$.
* Lecture 16 <2018-12-11 Tue>
** Definition
   A function $c\colon E(G) \rightarrow [k]$ is called a $k$ coloring. $f$ is
   proper if incident edges get different colors.

   A multi graph is $k$ edge colorable if there exists a proper $k$ edge
   coloring of $G$, $\chi'(G) = \min \{k \colon G \textup{ is k edge
   colorable}\}$.
** Examples
*** Clique
    For edge, it's 1. For a triangle, it is $2$. For $K_4$, it is $3$.

    For $K_5$, we can see that each coloring forms a matching. So $K_5$ can't
    have $K' = 4$, since there are no perfect matchings.

    General case, when $n$ is odd, $X'(G) = n$, and when $n$ is even it is
    $n-1$.
** Claim
   $\chi'(G) \ge \Delta(G)$.
*** Proof
    At the vertex with max degree, there are $\Delta(G)$ number of edges, each
    must receive a proper edge coloring.
*** Remark
    When $n$ is odd, $n-1$ edge coloring would form a perfect matching. But this
    is not possible.

    $K_n \subset K_{n+1}$. The inclusion is monotone.

    $\chi'(K_n) \ge n$.

    $\chi'(K_{n+1}) = n$.
*** Observation
    Any color class of a proper edge coloring is a matching. In particular, if
    $X'(G) = d$ for a $d$ regular graph, then $d$ has a perfect matching, then
    $G$ has a perfect matching. This is because at a vertex, all colors appear.
    So each color class is a perfect matching. (Even more, the edge set is
    partitioned into a perfect matching.)
*** Peterson
    If $X'(P) = 3$, if and only if $\exists$ a perfect matching, $M$ such that
    $P - M$ has $X'(P-m) = 2$. Implies that $P-M$ is the disjoint union of even
    cycles.

    What do we know about the cycles of peterson graph, the minimum size is $5$.
    It cannot have $6 + 4$ cycle. It does not have $8+2$ cycle either.

    What about $C_{10}$. It cannot have a $10$ cycle, because we know that the
    Peterson graph has no Hamilton cycle.

    The chromatic number is $4$.
** Theorem (Konig)
   If $G$ is a bipartite Multi-graph, then $X'(G) = \Delta(G)$.
*** Proof
    What do we know about regular bipartite graph? We have a perfect matching
    (Frobenius theorem.)

    If we have this, we color it and remove it, now we have a $d-1$ regular
    graph, we again color it back.

    Given a bipartite multi graph, we make it regular.

    Add new vertices to the smaller part if necessary (with fewer vertices.)

    Add edges to vertices with degree less than $\Delta(G)$ until there is no
    more. This way, you create a bipartite supergraph $H \supset G$.

    We cannot get stuck because, if there is a vertex in $A$ such that the
    degree of it is less than $\Delta(G)$, then there is another one in the
    other part with degree less than $\Delta(G)$. Why? Because the sum of the
    degrees in one part is same as that of the sum of the degrees in the second
    part.

    Now, just color it using the regular coloring.
** Vizing's theorem
   For every *simple* graph, $G$, $\chi'(G) \le \Delta(G) + 1$.
*** Remark
    1. For most graphs, $X'(G) = \Delta(G)$.
    2. Can we decide if a graph is $\Delta(G)$ or in the $\Delta(G) + 1$? This
       is NP-complete.
*** More remarks
    The line graph is just the edge set. The edge set of the line graph is those
    pairs of edges such that they intersect. [fn:37]

    There is a one-one correspondence between a proper coloring of $L(G)$ and
    proper edge coloring of $G$.

    $\chi(G) \ge \omega(L(G)) = \Delta(L(G)) \le \Delta(L(G)) + 1 \le
    2(\Delta(G) - 1) = 2\Delta(G) -1$ (more or less true, except for $K_3$) Here
    $\omega$ is the clique number.

    What is the maximum degree?

    Any clique of $L(G)$ corresponds to a $K_{1, t}$ or a $K_3$ in $G$.
*** Proof
    Proof using induction. Induction on $V(G)$, $V(G) - 1$. Take a vertex $v =
    V(G)$, there is a proper $\Delta(G) + 1$ coloring of $G-v$. Extend to $v$ to
    form a proper $\Delta(G) + 1$ coloring.

    *Stronger statement*: $\forall$ graph $G$ and any integer $k \in \N$, such
    that

    1. there is a vertex $v\in V(G)$ such that the degree of $d(v) \le k$.
    2. And for every $u \in N(v)$, $d(u) \le k$
    3. There exists at most one neighbour $w \in N(v)$, $d(w) = k$.

    If $G-v$ is $k$ edge colorable, then $G$ is $k$ edge colorable.

    For $k=1$, it can be shown.

    For $k > 1$, take a $k$ edge coloring where $X_i = \{u \in N(v) \colon u
    \textup{ is missing i} \}$. There is no edge incident to $u$ with color $i$.

    Without loss of generality, there does not exists $w \in N(v), d(w) = k$ all
    others have degree $k-1$. You add vertices to make sure the second
    condition.

    We need to look at the sum $\sum_{i=1}^{k} \vert X_i\vert$. The $w$ is in
    exactly one of the $X_i$. So $\sum_{i=1}^{k} \vert X_i\vert = 1 +
    2(d(v) - 1) = 2d(v) - 1$. So their average is less than $2$.

    *Case 1*: There is a color $i$ such that $\vert X_i \vert = 1$.

    $G' = G - w - \{xy \colon c(xy) = k\}$ and then apply induction on $G'$ and
    $k-1$. This way you get a proper $k-1$ edge coloring of $G'$ and then color
    remaining edges $k$.

    *Case 2*: When for every color $\vert X_i \vert = 0$ or $\ge 2$. But this
     will lead to a contradiction to the minimality.

     There exists a color $i$ such that $\vert X_i \vert = 0$, (otherwise
     contradiction to the inequality.)

     There is a color $j$ with $\vert X_j \vert \ge 3$.

     $H$ is the subgraph spanned by $i$ and $j$. This is a contradiction to
     minimality. (Sort of an involved argument.)
* Lecture 17 <2018-12-12 Wed>
** A general form of vizing's theorem
   If $G$ is a multi graph, then $X'(G) \le \Delta(G) + \mu(G)$ where $\mu$ is
   the maximum multiplicity of $G$. (Wasn't shown in the class)
** List coloring conjecture
   For arbitrary $G$, $X_l(G) - X(G)$ can be arbitrarily large.

   The conjecture says, for every $G$, $\forall G$, $X'(G) = X'_l(G)$. The one
   on the right is the list coloring version of edge coloring.

   Reformulation, for all $G$, $X(L(G)) = X_l(L(G))$. (We proved this for
   cycles; a cycle is a line graph)
** Theorem (Kahn)
   $X_l(G) = (1 + o(1))X(G)$.

   This means that if we have a sequence of graphs $G_n$ with $X'(G_n)
   \rightarrow \infty$, $X'_l(G)/X'(G) \rightarrow 1$. (Wasn't covered in the
   lecture.)
** Theorem (Galvin)
   If $B$ is bipartite, then $X'_l(B) = X'(B)$

   We'll prove the special case of complete bipartite graph ($K_{n, n}$)
   (Dinitz's conjecture.)
*** Proof
    $X'(K_{n,n})$ it is $n$. It is a bipartite graph (we learned last time that
    we can regularize it and then do some stuff.)

    Goal: For arbitrary assignment of lists of colors of size $n$, there is a
    selection of each color for each edge of $K_{n, n}$ from its list such that
    incident edges receive different colors.

    (Look at the definition below)

    The color class $1$ in the greedy coloring with respect to some ordering of
    a graph $G$ is a kernel of the digraph created by oriented or directing
    edges of $G$ from right to left. (Because if there is no edge from a vertex
    to this, this we would have colored it with $1$)
** Definitions
   Given a directed graph $D$, a *kernel* of $D$ is a set $S$ of the vertices

   1. that is independent.
   2. Every vertex $v \in V\setminus S$ has a neighbour $w \in S \cap N^{+}(v)$.

   A digraph is *kernel-perfect* if for all induced subdigraph, it has a kernel.

   Let $f\colon V(G) \rightarrow \N$, then a graph is called $f$ choosable (or
   list colorable) if for every family of lists, $\{L_v\}_{v\in V(G)}$ with list
   size at lest $f(v)$ for every $v$, there is a proper coloring $c \colon V(G)
   \rightarrow \N$, $c(v) \in L_v$.
** Lemma
   Let $D$ be a kernel perfect orientation of some graph $G$. Then $G$ is $f$
   choosable with $f(v) = 1 + d^{+}_D(v)$.
*** Proof
    Orientation thing?

    +Take a kernel of the graph, $S$, and color every vertex $v\in S$, such that
    $c(v) \in L(v)$. It is possible since $L(v) > 1$.+

    +Now, we delete this list from the graph, and in the other side, we modify
    the lists.+

    Take a color, 1, then $W$ be all vertices such that $1$ is in there. Now,
    there is a kernel $D$ inside this subgraph. Call it $S$.

    Color $c(u) = 1\forlal u \in SE$

    Now, we do induction on $D - S$ such that for all $u\in W \setminus S$,
    redefine $L_u = L_u \setminus \{1\}$, for all $u \in V \setminus W$ redefine
    $L_u = L_u$.

    Now $d^{+}_{D_s}(u) \le d^{+}_{D}(u) - 1$, for $u\in W \setminus S$. Because
    $S$ is a kernel of $D[W], because u had an edge going into $S$ (and this
    edge was removed with the deletion of $S$.)

    This implies that $\vert L_u \vert \ge d^{+}_{D-S}(u) + 1$. This holds for
    every $u \in V(D) \setminus S$.

    From induction, there is a coloring, proper, from the list. And there is no
    contradiction with vertices colored by $1$.
** About Kernel perfect orientation of $L(K_{n, n})$
   A kernel perfect orientation of $L(K_{n, n})$ with $\Delta^{+}(D) =
   n - 1$.

   If by the lemma, then $L(K_{n,n})$ is $1 + n-1 = n$ choosable.

   Note that $L(K_{n, n})$ is $2(n-1) = 2n-2$ regular graph.
*** Claim 1
    There is an orientation $D$ of $L(K_{n,n})$ such that $\Delta^{+}(D) = n-1$
    and for any $v \in V(K_{n, n})$, $D[\{uw \colon w \in N(v)\}]$ is
    transitive.

    What is a transitive orientation?
*** Claim 2
    If $D$ is an orientation from claim 1, then $D$ is kernel perfect.
*** Remark
    Notice that the above two claim and the lemma would give you the theorem.
*** Proof of claim 1
    $M = \{n_0, n_1, \cdots, m_{n-1}\}$

    $W = \{w_0, w_1, \cdots, w_{n-1}\}$ are vertices of $K_{n,n}$.

    The edge $m_iw_j \rightarrow m_{i'}w_j$ if $i+j > i' + j (\mod n)$. (abuse of
    notation) (Here the $j$ is fixed.)

    $m_i w_j \rightarrow m_i w_{j'}$ if $i + j < i + j' (\mod n)$, here j is
    fixed.

    This should clearly have the transitive property. An orientation is
    transitive if $a > b, b > c$, then $a > c$. There is no cycle.

    Indeed for every $m_i w_j$, the out degree $d^{+}(m_iw_j) = n-1$.
* Lecture 18 <2018-12-18 Tue>
** Lemma
   If $D$ is a kernel perfect orientation of a graph $G$, then $G$ is $f$
   choosable with the following function $f(v) = 1 + d_D^{+}(v)$ for all $v \in
   V$.

   Goal: There exists a kernel perfect orientation $D$ of $L(K_{n, n})$ with
   $\Delta^{+}(D) = n-1$.

   We have a digraph $D = (V, E)$, $K \subset V(D)$ is a *kernel*
   if $K$ is independent and $\forall v \in V \setminus K, \exists w \in K, uw
   \in E$

   $D$ is *kernel* perfect if for all induced subgraph has a kernel.
** Remark
   Any ordering of vertices of graph $G$ from right to left is kernel perfect.
** Observation
   An orientation of a complete graph is kernel perfect if and only if it is
   transitive.

   If $D$ is a kernel perfect orientation of the line graph $L(k_{n, n})$, then
   $\forall v \in V(K_{n, n})$, the clique $\{vw\colon w \in N(u)\}$ must be
   transitively oriented.

   *Claim*: There exists orientation $D$ of $L(K_{n, n})$ such that
    $\Delta^{+}(D) = u-1$.

    *Claim*: If $D$ is an orientation of $L(K_{n, n})$ such that for every $v
     \in V(K_{n, n})$, the restriction of $D$ on $\{vw \colon w \in N(v)\}$ is
     transitive, then it implies that $D$ is Kernel perfect.[fn:38]
** About stable matchings
   We have two sets $M = \{m_1, \cdots, m_n\}$ and $W = \{w_1, \cdots, w_n\}$.
   Each vertex has a preference of the other gender (a permutation of the other
   list, $\pi \in S_n$)
*** Theorem (Gale-Shapley)
    For any preference lists of $n$ man and $n$ women, there exists a stable
    matching $S$.

    A stable matching is a perfect matching of $K$ and $N$ with no unstable
    pair. A vertex $w$ and a vertex $m$ forms an unstable pair if $wm \notin S$,
    $w$ prefers $m$ to her pair and $m$ prefers $w$ to his pair in $S$.
*** Proposal algorithm
     *Input*: Preference ranking by men and women.

     *Iteration*: in each iteration step, each man proposes to the women highest
     on the list who has not previously rejected.

     If each women receives one proposal, then STOP and return the resulting
     matching.

     Otherwise, every women receiving at least one proposal says, maybe to the
     one highest on their list and rejects the others if there are more than
     one.
*** Theorem about Proposal algorithm
    The proposal algorithm produces a stable matching.
**** Proof
     Algorithm terminates:

     1. Combined length of lists of men decreases in each iteration.
     2. No list of a man ever becomes empty. (So proposals can be made.) (If a
        women rejects a man, she will keep receiving proposals from that point
        on. So if a man is rejected by $n$ women, then, all these women must
        have proposals at the time of the last rejection. But these come from a
        set of only $n-1$ men, and this is a contradiction, since no man
        proposes to more than one women in one round.)

     *Observation 1*: Men propose to women in a decreasing order of preference
     on his list and does not skip anyone.

     *Observation 2*: A women says maybe to a man in an increasing order of
     preference.

     So the algorithm terminates.

     Output is a matching $S$. Why is it stable?

     Let $wm \notin S$, let $w_m$ and $m$ be matched. and $w and m_w$ be
     matched. Suppose $m$ prefers w$ to $w_m$. Then why are they not married? At
     some point in the algorithm, $m$ proposed to $w_m$, but had to be rejected
     eventually. Why? $w_m$ rejects a man who is proposing only if there was a
     better man $m'$ who is higher on the $w_m$'s list than $m$. In particular,
     $m_w$ is higher than $m$ in $w$'s list. Thus this cannot be an unstable
     pair.
** Claim
   If $D$ is an orientation of $L(K_{n, n})$, such that $\forall v \in V(K_{n,
   n})$, the set $\{vw \colon w \in N(v) \}$ is oriented transitively, then $D$
   is kernel perfect.
*** Proof
    Define appropriate preferences for the vertices such that any stable
    matching $K \subset E(K_{n, n}) = v(D)$, $K \cap I$ is a kernel. The kernel
    is a vertex set of the line graph and hence a vertex set of the original
    graph.

    Men $m_i \in M$ prefers women $w_j$ to $w_j'$ if $m_i w_j \in I, m_iw_j' \in
    I$ and $m_iw_j \leftarrow m_iw_j$.

    If $m_i w_j \in I$, $m_i w_j' \notin I$, $m_i w_j$, $m_iw_j \notin I$ ($m_i
    w_j \leftarrow m_i w_j$)

    Women $w_j$ prefers man $m_i$ to $m_i'$ if $m_i w_j$, $m_i'w_j \in I$ and
    $m_i w_j \rightarrow m_i w_j$ or $m_i w_j \in I$, $m_iw_j \notin I$ ($m_iw_j
    \leftarrow m_i' w_j$)

    Claim: $K \cap I$ is a kernel of $D[I]$.

    Firstly, $K$ is an independent set, indeed, since $K$ is a matching in
    $K_{n, n}$.

    Secondly, suppose you have a pair $m_iw_j \in I \setminus K$, but has no out
    neighbours in $K \cap I$. We should be able to find an unstable pair. (There
    is some issue with this part.)

    Suppose $w_j$ prefers $m$ to $m_i$, then if $mw_j \in I$, then $mw_i
    \rightarrow m_i w_j$. This cannot occur?
* Lecture 19 <2018-12-19 Wed>
** About network flows
   $(D, s, +, E)$ and $D = (V, E)$ and $s, t \in V$ and $c \colon E \rightarrow
   \R_{\ge 0}$.

   Want certain constraints: a flow $f\colon E \rightarrow \R$. Capacity
   constraints: $0 \le f(e) \le c(e)$.

   The second type of conservation constraints: $\sum_{v \in N^{-1}(v)} (f(uv))
   = \sum_{w \in N^{+}(v)$ for all $v \in V \setminus\{s, t\}$.

   The $\max \sum_{u \in N^{-1}(t)} f(ut) - \sum_{w \in N^{+}(t)} f(tw)$.

   $f \in \R^{\vert E \vert}$. There is a vector $d \in \R^{\vert E \vert}$
   ($\{0, 1, -1\}^{\vert E \vert}$)

   Maximize $d \cdot f$ subject to $\bar{0} \le f \le c$. The bar means it's a
   vector of zeros. One vector is less than or equal to the other one if
   coordinate wise they are the same.

   For every $v \in V \setminus \{s, t\}$, $f \cdot e_v = 0$, $e_v \in \R^{\vert
   E \vert}$

   What we want is $f \in \R^{\vert E \vert}$

   This is the framework of linear programming.
** Linear programming
   In linear programming, we search for a maximum or a minimum of a linear
   objective function, $c^T \cdot x$, $c$ is a constant vector subject to linear
   constraints $x$, where $x \in \R^n$

   $A x \ge b_i$ or $A_x \le b_i$ or $A_x = b_i$.
** Types of problems
*** Word problems
    LP's were developed by Dantzig. And also by Kantorovich.
*** $n = 2$
    Maximize $x_1 + x_2$.

    Subject to $-x1 + x_2 \ge -2$, $-3x_2 + 2x_2 \le 2$, $x_1 + 2x_2 \le 4$,
    $x_1, x_2 \ge 0, x_2 - x_1 = -2$.

    A graph was drawn with the constraints and feasible region was identified.

    Maximize $x_1 + x_2$ such that $-x_1 + x_2 \ge 2$ $-3x_1 + 2x_2 \le 2$
    $x_1 + 2x_2 \le 4$ $x_1, x_2 \ge 0$, the feasible region has no solution.

    We can also have unbounded optimal solution.

    If the feasible region is bounded, then there is definitely an optimum.
*** Book
    Some book by springer.
** Uniformization of the setup
   Maximize or minimize a linear function subject to linear constraints.

   Objective function:

   Given a vector $c \in \R^n$, $c^{T}\cdot x = c_1 x_1 + \cdots + c_nx_n$.
   $\min c^{T} \cdot x = \max (-c^T)\cdot x$.

   We may assume that we have a maximization problem.

   Also we can represent $\le, \ge$ into $\le$.

   From equality you can go to $\le$, by introducing two constraints, both $\le$
   and $\ge$. Then transform the $\ge$ further.

   We can end up with $Ax \le b$, $A \in \R^{m \times n}$ and $b \in \R^m$.
** A dietary example

  | Ingredients          | Requirement | Carrot | White Cabbage | Pickle |
  |----------------------+-------------+--------+---------------+--------|
  | Vitamin A (mg/kg)    | 0.5mg       |    0.5 |           0.5 |    0.5 |
  | Vitamin C (mg/kg)    | 15mg        |    300 |           300 |     10 |
  | Dietary Fiber (g/kg) | 4g          |     30 |            20 |     10 |
  |----------------------+-------------+--------+---------------+--------|
  | Cost                 |             |   0.75 |          0.5  | 0.15   |

  Minimize the cost. $x_c, x_k, x_p$ are the kilograms of Carrow, White cabbage,
  and pickle in one Kebab.

  Minimize $0.75 x_c + 0.5 x_w + 0.15 x_p$ (the price of the pickle.)

  Constraints:

  (Vitamin A) $35 x_c + 0.5 x_w + 0.5 x_p \ge 0.5$

  There is also non-negativity constraints. $x_c, x_w, x_p \ge 0$.

  Similar constrains for Vitamin C and Fiber.

  The optimal solution $0.009 kg$ of carrot, $x_w = 0.038 kg$ of white cabbage.
  From the pickle $0.290 kg$.
** Another example
   We have a data and want to find correlation.

   Here it would be average rating and amount of chocolate offered to students.

   We want $a, b \in \R$. Then we have $\sum (ax_i + b - y_i)^2$. Outliers would
   come very much into this calculation. We can take $\sum \vert ax_i + b -
   y_i\vert$ and minimize this instead. You want $a$ and $b$.

   The function is not a linear function, it contains the absolute value.

   We need to get rid of the absolute value.

   Here we introduce new variables $e_i$ and subject to the constraints.

   $e_i = \vert ax_i + b - y_i\vert \iff e_i \ge ax_i + b - y_i, e_i \ge y_i -
   ax_i - b$.

   Input $(x_i, y_i)$ and we have variables $a, b, e_i$. What you're interested
   is in $a$ and $b$.

   For the minimum, for all i, at least one is equality. For this reason, $e_i$
   indeed expresses, the absolute value.
* Lecture 20 <2019-01-08 Tue>
** Linear programming (LP)
   Given $c \in \R^n$ and $A \in \R^{m \times n}$. We want to maximize, $c^{T}
   \cdot x$ subject to $Ax \le b$.

   LP is in P. Simplex algorithm is an algorithm.
** A machine learning application
   We need to device a trap for Rabbits. There could be other animals that fall
   into the trap, we need to separate Rabbits from Weasels. We have a 2
   dimensional vector corresponding to each animal.

   We try to separate them by a line.

   $a, b \in \R$ such that $a x(R_i) + b < y(R_i) \forall i = 1, \cdots, k$.
   and $a x(w_j) + b > y(w_j)$

   This looks like a linear program, except the inequalities are not strict.

   We do a trick to avoid this situation. We'll have variables $x, b, \delta$.
   The goal will be to maximize $\delta$.

   $\max \delta$, subject to $a x(R_i) + b + \delta \le y(R_i)$ for all $i = 1,
   \cdots, k$.

   $a x(W_j) + b - \delta \ge y(W_j) \forall j = 1, \cdots, l$.

   If $\delta > 0$, then there is an $a, b$ which is good, i.e., there exists a
   separating line.

   If $\delta \ge 0$, then there is no separating line. (In practise, everything
   may be allowed.)
** Another example
   You are a company that sells Wrapping paper, you produce paper rolls of width
   $300$ cm.

   Order

   | Number | Width |
   |--------+-------|
   |     97 |   135 |
   |    610 |   108 |
   |    395 |    93 |
   |    211 |    42 |

   There is a waste when we cut stuff and throw them away. We need to find a way
   to minimize the waste.

   Question: What is the smallest number of rolls of 300 cm that can be cut to
   produce this order?

   How would you set this up?

   Minimize the variable $x_{300}$ subject to the demand $x_{135} \ge 300$,
   $x_{108} \ge 610$, $x_{93} \ge 395$, $x_{42} \ge 211$. This is not a good
   linear program.

   The possible ways to cut a roll of width $300$ cm. This will be a vector of
   length $4$. How many $135, 108, 93, 42$ that you produce.

   Possibilities: $P_1 \colon (2, 0, 0, 0)$, $P_2 \colon (1, 2, 0, 1)$. There
   are eventually finitely many choices.

   $P_3 \colon (1, 0, 1, 1) ,P_4 \colon (1, 0, 0, 3), P_5\colon (0, 2, 0,2), P_6
   \colon (0, 1, 2, 0), P_7 \colon (0, 1, 1, 2), P_8 \colon (0,1, 0, 4), P_9
   \colon (0, 0, 3, 0), P_{10} \colon (0, 0, 2, 2), P_{11} \colon (0, 0, 1, 4),
   P_{12} \colon (0, 0, 0, 7)$.

   $y_i$ is the number corresponding to $P_i$.

   LP is to minimize $\sum y_i$ subject to $2y_1 + y_2 + y_3 + y_4 \ge 97$,
   $y_{2} + 2y_5 + \cdots \ge 610$.

   Optimal solution: $y_1 = 48.5, y_5 = 206.25, y_6 = 197.5$ all others are $0$.
   When we round it above, $450$ rows of width $300$. The sum of these are
   $452.25$, this means that $452$ is not enough.

   $y_1 = 49, y_5 = 207, y_6 = 196, y_9 = 1$.

   There is an integer solution with values $453$ rolls.

** Integer Linear Programming
   $\max c^{T} x$ subject to $Ax \le b$, $x \in \Z^n$
** Max-weight matching
   We solved this by Hungarian algorithm. We have a bipartite graph, $G$ where
   the vertex set is $G = (A \cup B, E)$ and you have a weight function $w
   \colon E \righatrrow \R$.

   Goal is to find a matching of maximum weight $\sum_{e \in M} w(e)$. We solved
   this problem in polynomial time.

   Encode this as an Integer linear program.

   Variable $x_e$, for all $e \in E$, $x_e \in \{0, 1\}$, ($x_e = 1 \iff e \in
   M$ otherwise $x_e = 0$)

   Maximize $\sum_{e \in E} w_e x_e$

   Subject to the condition that $\sum_{e \in E} x_e \le 1$ for all $v \in V$.

   If $x^{*} \in \{0, 1}^{\vert E \vert}$ is optimal solution if and only if
   $\sum w_e x_e^{ *}$ is the weight of the maximal matching.

   We take the LP relaxation, which is the same thing. Maximize, $\sum_{e \in E}
   w_e x_e$ subject to $\sum_{e \in E, v \in e} x_e \le 1, \forall v \in V$.

   $0 \le x_e \le 1 \forall e \in E$.
** Proposition
   For the max-weight problem, the LP-relaxation, has the same value as the
   integer program. (For the complete bipartite graph.)
*** Proof
    Optimum to LP-relaxation is at least as large as optimum for IP.

    Let $x^{*}$ be an optimal solution to LP. If $x^{ *}$ is an integer vector,
    we are done.

    Otherwise, we'll make a change to this vector which will not change the
    value, but it will increase the number of integer coordinates without
    changing the value.

    There is an edge, call it $e_1$ such that the value of $x^{*}$ is neither
    zero, nor one.

    Let's say $e_1 = a_1b_1$. $a_1 \in A$ and $b_1 \in B$, the bipartite sets.

    This implies that there is an edge $e_2$ incident to $b_1$ such that the
    value of $0 < x^{+}_{e_2} < 1$ since $\sum x^k_e = 1$. At some point I must
    return to the vertex. The idea is that we'll grab the cycle and change the
    weight of the edges a bit. We'll add $\varepsilon$ and subtract
    $-\varepsilon$ alternately.

    There is a cycle $e_1, e_2, \cdots, e_{2t - 1}, e_{2t}$ (an even cycle since
    $G$ is bipartite)

    $0 < x^{*}(e_i) < 1$, we'll change $\tilde{x}(\varepsilon)_e$ is $x^{ *} +
    \varepsilon$ if $e = e_{2i - 1}$ and $x^{ *} - \varepsilon$ when $e =
    e_{2i}$ and $x_e^{ *}$ otherwise.

    $\varepsilon$ is going to be $\min(\{1 - x_e^{*} \colon e = e_{2i - 1}\}
    \cup \{x_e^{ *} \colon e = e_{2i}\})$.

    We know that $\varepsilon_0 > 0$.

    The easy claim is that $\tilde(\varepsilon_0)$ is a feasible solution for
    the LP.

    $\sum_{e \in E} w_e \cdot \tilde{x}(\varepsilon_0)_e = \sum w_e x^{*} +
    \varepsilon_0 \sum_{j = 1}^{2} (-1)^{j+1} w_{ej} \ge \val(x^{ *})$.

    There is at least one more integer coordinate.
* Lecture 21 <2019-01-09 Wed>
** Repetition
   We have an even cycle with value along with edges that is between 0 and 1.
   And we make a modification.

   $M_1 = \{e_1, \cdots, e_{2 t - 1}\}$ and $M_2 = \{e_2, \cdots,
   e_{2t}\}$. $\sum_{e \in M_1} w_e \ge \sum_{e \in M_2} w_e$ (without loss of
   generality.)

   You choose the $+\varepsilon$ in this case. The value is the same as the
   optimum value.

   $x(\varepsilon_0)$ has at least one more integer coordinate than $x^{*}$. It
   is also an optimal solution. This way we get an algorithm that gives us,
   integer solution.
** Minimum vertex cover problem
   We have a graph $G$ and $\beta(G) = \min\{ \vert C \vert \colon C \subset V,
   \forall e \in E(G), e \cap c \neq \emptyset\}$, we know that $G$ is
   bipartite, then $\alpha'(G) = \beta(G)$.

   In general this is not true, and it is know to be NP-Hard to decide whether
   $\beta$ is larger than $\alpha'$.

   An integer linear program formulation for $\beta$.

   Variables, $\forall v \in V$, variable $x_v \in \{0, 1\}$.

   We'll have a constraint, $\forall e \in E$, $\sum_{v \in e} x_v \ge 1$.

   The goal is to minimize the $\sum_{v \in V} x_v$.

   The claim is that the optimal solution is equal to $\beta'$.

   (More general statement) Claim: $x \in \{0, 1\}^{\vert V \vert}$ is a
   feasible solution to IP, if and only if $S_x \{v \in V \vert x_v = 1\}$ is a
   cover. In particular, $x^{*}$ is optimal solution if and only if $S_{x^{
   * }}$, hence $\sum x^{ * }_v = \beta(G)$.
*** Proof
    Let's prove that $S_x$ is a cover.

    Take arbitrarily edges $e$. We need to show that $S_x \cap e \neq
    \emptyset$.

    Since $x$ is feasible, $\sum_{v \in e} x_v \ge 1 \implies x_{z_1}$ or
    $x_{z_2} \ge 1 \implies$, $z_1 \in S_x$ or $z_2 \in S_x$, $e \cap S_x \neq
    \emptyset$.
** LP relaxation of the problem
   $\min \sum x_v$ subject to

   $\sum_{v \in e} x_v \ge 1$ and $0 \le x_v \le 1$. This is the LP relaxation.

   One could solve it.

   If the linear program could give integer solution, then we could, solve an NP
   hard problem in P, which would mean more.

   But we can use the LP to form an approximation algorithm to the problem.

   Let $x^{*} \in [0, 1]^{\vert V \vert}$ an optimal solution to the LP.
   $S_{x^{ * }}$

   $S_x = \{v \in V \colon x_v \ge 1/2\}$

   Claim: $S_x$ is a cover.

   Take $e \in E$, $x$ is feasible, implies $x_{z_1} + x_{z_2} \ge 1$, implies
   that $x_{z_1}$ or $x_{z_2}$, $\ge 1/2$ implies $z_1$ or $z_2 \in S_x$.

   Take $x^{ * }$ be optimum, then $\vert S_{x^{ * }} \vert = \sum_{v, x_v \ge
   1/2} 1 \le 2\sum x^{ * }_v \le 2 \vert_{v \in V} x^{*}_{v} \le 2 \beta(G)$

   Where $\beta(G)$ is the optimal solution to the integer program.

   Here we are using the that optimal solution to the LP is $\le$ the optimum
   for the IP.
** Remark
   If you take a Maximal matching $M$, then the set of vertices $S = \{v \in
   V\colon v \textup{ is saturated by M}\}$ is a cover of size $2n$.

   $2 \vert M \vert \le 2 \beta(G)$. This is a trivial observation. We can find
   this using a greedy algorithm.

   Finding a 1/2 approximation is not a big deal.
** Max incl set problem
   Given $G$, What is $\alpha(G)$? This is $\max \{ \vert I \vert \colon I
   \subset V \forall e, e \notin I\}$

   ILP: $\forall v \in V$, there is a variable $x_v \in \{0, 1\}$, $e \in E$,
   constraint $\sum_{v \in e} x_v \le 1$.

   The maximum $\sum_{v \in V} x_v$.
*** Claim
    $x$ is a feasible solution to the problem if and only if $I_x = \{v \in
    V\colon x_v = 1\}$ is included. In particular, the implies that optimum
    problem $= \alpha(G)$.

    The claim wasn't proven.
** LP relaxation
   $x \in [0, 1]^{\vert V \vert}$.

   Can we learn something about $\alpha(G)$ from the optimum value of the
   objection from the LP relaxation.

   For a clique, $G = K_n, \alpha(K_n) = 1$. (The independence number.)

   The vector $x = 1/2$ is a feasible solution for any graph. And $\sum x_v =
   n/2$. This means that, the LP relaxation is useless.

   We know that $I$ is independent if and only if $V \setminus I$ is a vertex
   cover. Even though that we know about a $1/2$ approximation for the vertex
   cover.
*** A result by Hastead.
    NP-complete to approximate $\alpha(G)$ upto the factor $n^{0.99999}$
** LP in general
   We'll first get rid of the inequalities make it into an equation form.

   Every LP is equivalent to an LP of the following form:

   Max $cx$ subject to the constraint $Ax = b$ and $x \ge 0$. $A$ is a matrix,
   $c$ and $b$ are vectors.

   $a_{11}x_1 + \cdots + a_{1n}x \le b_i$, then we have a new variable, slack
   variable, $z_i \ge 0$, and replace constraint with $a_{i1}x_ + \cdots +
   a_{in}x_n + z_i = b_i$.

   New variables, $y_1, y_i', \forall i 1, \cdots, n$ and $z_j, \forall j = 1,
   \cdots, m$.

   $y_i = \max\{0, x_i\}$ and $y'_i = \max\{0, -x_i\}$.

   The number of variables is $n$ and the number of constrains is $m$.
*** Some assumptions
    1. $Ax = b$ has a solution. (We can do Gauss-elimination to decide it fast),
       if not the LP has no feasible solution.
    2. $\rank(A) = m$. Because if not, we can delete the row. The assumption
       says that the rows are linearly independent.

    With the assumption, the typical solution space would look like a simplex.
** About the linear system
   $Ax = b$

   $A_B$ is the matrix is the matrix where columns of $A$ indexed by $[n]
   \setminus B$ are indexed. Choose values arbitrarily, it gives you a solution
   to $Ax = b$. We have a solution space which is the right size.
** Definition
   $B \subset [n]$ is called a basis if rank $A_B = m$. In fact, columns of $A_B
   are a basis in column space of $A$.
** Definition
   A feasible solution to $*$ is called a basic feasible solution for a basis $B
   \subset \binom{[n]}{m}$ if $x_i = 0$ for every $i \in [i] \setminus B$.
** Lemma
   For every basis $B$, corresponds to at most $1$ basic feasible solution.
*** Proof
    If $x$ is a feasible feasible solution for $B$, then $Ax = A_Bx_B + A_N
    x_n = A_B x_B$.

    We denote the complement of $B$ by $N$.

    Now $A_B$ is an invertible matrix, because it is a basis. This means that
    $x_B = A_B^{-1} b$ is determined (and of course $x_N = 0$)

    (Next week we'll show that if we have a bounded linear program, then it will
    have a basic feasible solution that is optimal. This means that to find the
    $x_B$ for each $m$ element subset, check if it a non-negative vector, go
    through them and calculate the maximum value. For each $B$, calculate $c
    x_B$. Look at all of them find the maximum.)
* Lecture 22 Skipped
* Lecture 23
** A review of simplex method
   Given $A \subset \R^{m\times n}, b\in \R^m, c \in \R^n$. An LP in equational
   form. LP: $\max c^{T} x$ subject to $Ax = b, x \ge 0$.

   *Step 0*: Check whether $Ax = b$ has a solution. Make sure that
   $\operatorname{rank}A = m$. You can assume this two things.

   If we have a basis, then we know how to create a basic feasible solution.
** How to find a basis
   Add extra variables $w_1, \cdots, w_n$. Your new system will look like
   $a_{i1}x_1 + \cdots + a_{in}x_n = b_i$. Make sure $b_i \ge 0$. New
   constraint, $i = 1, \cdots, m$, $a_{i1}x_1 + \cdots + a_{in} x_n + w_i =
   b_i$.

   The objective function is to maximize the new variables. $\max -w_1 - w_2 -
   \cdots - w_m$.

   Matrix

   #+BEGIN_SRC artist
          +-------------------+----------+
          |                   |          |
          |                   |          |
          |       A           |   I_n    |
          |                   |          |
          |                   |          |
          |                   |          |
          +-------------------+----------+
   #+END_SRC

   Solve auxiliary LP with Simplex.
*** Claim
    Value of auxiliary LP $=0$ if and only if the original LP is feasible.

    This claim is easy to see.

    If you get a solution, then we got a solution, which end with a basis.

    In the second case, the basis would be $I_m$.
** Step 1 of Simplex
   If there is an obvious feasible solution from the slack variables when
   original $b > 0$, then take it. Otherwise, solve the Auxiliary LP and if the
   maximum is $< 0$, then stop and return infeasible and if h
** Step 2 (Simplex tableau)
   Compute a simplex tableau. Belongs to a feasible basis.

   $x_B = P + Q \times x_N$. $(\iff Ax = b)$

   The last row was the objective function, $z = z_0 + r^{T} x_N$ ($\iff z =
   c^{T}x$)

   $P \in \R^m, Q \in \R^{m \times (n-m)}$ and $r \in \R^{n + m}$ and $z_0 \in
   \R$ are all expressed from $A, b, c$.
** Step 3 (Check for optimality)
   If the whole vector $r \le 0$, then we stop and return the current basic
   feasible as optimal. (We proved this last time.)

   Otherwise
** Step 4
   Select index $i \in N$ entering the basis among $\{i \in N \colon r_i > 0\}$.

   How? pivot Rule.
** Step 5
   $j \in B$ and $x_j = P_j + \sum_{i \in \N} q_{jl}x_l$ if $a_{ji} \ge 0$, then
   there is no restriction.

   If $a_{ji} < 0$, then there is a restriction. Because you don't want $x_j$ to
   go below $0$. Then $0 \le x_j \le p_j + q_{ji}x_i$. (Was done yesterday)

   First we test for unboundedness. If for every $j \in B$, $q_{ji} \ge 0$,
   then, stop and return that the LP is unbounded. Because $x_i$ can be
   increased indefinitely.
** Step 6
   Otherwise, select variable $j \in B$ for which these restrictions are the
   strictest. $j \in B - \frac{p_j}{q_{ji}} = \min \{ - \frac{p_l}{q_{jl}} \vert
   \colon l \in B q_{l, i} < 0\}$ to leave the basis.

   How? Pivot rule.
** Step 7 (The last step)
   Perform the pivot, move to a new basis $B' = B \setminus (\{j\} \cup \{i \})$

   Update the Tableau. Go back to step $2$ and do it again.

   Use equation to replace to replace $x_i$ with $x_j$ everywhere else on the
   RHS. Apparently all these were done yesterday.
** Pivot Rules
*** The largest coefficient rule
    Take the $\operatorname{argmax} \{r_i \colon r_j > 0 \}$
*** Largest increase
    For a pair $i, j$, what is the one with largest increase. This can be expensive.
*** Steepest edge
    Great in practise.
*** Bland's rule
    The entering variable, the smallest index is entering among $r_i > 0$, $i
    \in \N$. It has no concern in the value of $r_i$. The leaving variable is
    picked as the smallest for which $-p_{i}{q_{j,i}} = \min \{ \cdots \}$.
*** Random edge
    Take a random.
** Theorem
   The simplex algorithm with Bland's rule will terminate in finitely many step.
** Proof
   If the algorithm does not terminate, then it follows that there is a sequence
   of basis. $B_1, B_2, \cdots, B_s = B_1$ following each other.

   Let $F = \{x_i \colon \cup_{d = 1}^{s} B_d\}$, these are the variables
   participating in a basis of the cycle. These are called "fickle" variables.
   These are the variables which are entering and leaving the basis.

   All $B_i$ have the same basic feasible solution and all fickle variables stay
   $0$ throughout. (This should be clear, is it so clear?)

   Because we're in a cycle, the value cannot improve because the value is the
   same at the end and at the beginning. Thus the value cannot improve in
   between.

   We'll identify the $x_\beta \in F$, where $\beta$ is the largest and now
   we'll look at $x_\beta$ enters and when it left. $B$ is the basis just before
   $\beta$ enters and $B'$ is the basis just before $\beta$ leaves. Doesn't
   matter if it repeats, pick any two.

   Now, we apply the Bland's rule. The first deduction is that in the simplex
   tableau. $x_B = p + Q x_N$.

   $z = z_0 + r x_N$. In the simplex Tableau, $r_\beta > 0$. On the other hand,
   for every $i \in F \setminus \{beta\} \cap N, \gamma_\beta = 0$.

   The second deduction is that in the simplex tableau corresponding to
   $\tau(\beta')$, $\beta'$

   $x_B = P + Q x_N$ and $z = z_0 + r x_N$ and $q_{\beta, \alpha} < 0$. This
   should be true, furthermore, $q_{i, \alpha} \ge 0, \forall i \in B' \cap F
   \setminus \{\beta\}$

   We write down an Auxiliary linear program (it looks very similar to the
   previous one.)

   $Ax = b$, $x_B \le 0$, $x_{F \setminus \{\beta\}} \ge 0$ and $x_{N \setminus
   F} = 0$.

   Let $x^{ * }$ be the basic feasible solution corresponding to the $B$ and $B'$
   and everybody else. Then, Deduction 1 implies that $x^{ * }$ is an optimal
   solution to the Auxiliary LP. (Because of the fact that $\beta$ was the largest)

   The deduction $2$ will imply the value of $x^{ * }(t)$ is $t$ if $i =
   \alpha$, $0$ if $i \in N \setminus \{\alpha\}$ and $x_{i}^{ * } + q_{\alpha}$
   if $i \in B'$.

   $x^{*}(t)$ is feasible for all $t > 0$. $c^{t} \colon x^{ * }(t) \righatrrow
   \infty$. This would end up implying that the LP is unbounded, which is a
   contradiction.
* Lecture 24 <2019-01-22 Tue>
** Example
   The example from the beginning of 6th chapter.
** Weak duality
   Other examples: Matching, flows, covers.
** Corollary of weak duality
   1. If both $P$ and $D$ are feasible and bounded, then optimum of $P$ is less
      than the optimum of $D$.
   2. For every feasible $x^{*} \in \R^n$ of $P$ and feasible $y \in \R^n$ of
      $D$, then $c^{t} x^{ * } = b^t y^{ * } \implies$, $x^{ * }$ and $y^{ * }$
      are optimal for $P$ and $D$ respectively

      The strong duality theorem tells that the $\Leftarrow$ is also true.
   3. If $P$ is unbounded from the above then $D$ is infeasible. If $D$ is
      unbounded from below, then $P$ is infeasible.
** Definition of Primal-Dual for general LP
   Suppose that $P$ is a an LP with LP with $n$ variables $x_1, \cdots, x_n$ and
   $m$ constrains.
** Proposition
   If $P$ is feasible and bounded, then $D$ is feasible and bounded and the
   respective optimum exists and are equal.

   (The main part of the proof of the strong duality theorem.)
*** Proof
    (I think this is from section 6.3)

    If $D$ is feasible, then it is bounded by weak duality.

    Want: $D$ is feasible

    $P$, $\max c^{t}x$ subject to $Ax \le b$ and $x \ge 0$.

    $P'$ introduces slack variable, maximize $c^{t}x$ subject to $Ax = b$, $x
    \ge 0$.
    
    Simplex terminates on $P'$ with a basic feasible solution $x^{*} \in \R^{n
    \times m}$ corresponding to a feasible basis $B \subset [n + m]$.

    Implies that the vector $(x_1^{ * }, \cdots, x_n^{ * } = x^{*} \in \R^n$ is
    an optimal solution of $P$.

    We know that $x^{*}$ multiplied by $c$ is the same the original one.

    The final tableau looks like 

    $\bar{x}_B = P + Q \bar{x}_N$
    $z =z_0 + r^{T}\bar{x}_N$.

    Where $r \in \R^n$ and $r \le 0$. (This is when the simplex algorithm halts.)

    We can use this information. 
** Claim
   The vector $y^{*} = (\bar{c}_B^{T} \bar{A}_B^{-1})^T$ is a feasible set of
   $D$ and $c^{T}x^{ * } = b^{ * }$
*** Proof
    $D$ is minimize $b^{T} y$ subject to $A^{T}x \ge c$ and $y \ge 0$.

    $c^{T}x^{ * } = \bar{c}^T \bar{x}^{ * } = \bar{c}_B^T \bar{x}_B^{*} =
    \bar{c}_B^T (A_B^{-1} b) = (\bar{c}_B^{T} \bar{A}_B^{-1}) b$.

    What we want: $y^{*}$ is feasible for $D$.

    $A^{T}y^{ * } \ge c$ and $y^{ * } \ge 0$.

    If and only if

    #+BEGIN_SRC artist
             +--------------+
             |              |
             |     A^T      |
             |              | (y^{*}) \ge \bar{c}
             |              |
             +--------------+
             |1             |
             | 1            |
             |           1  |
             +--------------+
    #+END_SRC

    Szabo's proof looked much longer than the proof in the book. Maybe he did
    something different?
* Lecture 25
** Duality recipe
   Page 85 of the book or section 8.2
** Maximum matching problem
   $G = (V, E)$, with variables $x_e$ for all edge $e \in E$, $x_e \in \Z$.

   Constraints: $\forall v$, $\sum_{e \in v} x_e \le 1$.

   Sign-constrains: $\forall e \in E, x_e \ge 0$.

   We have shown that the maximum of the objective function: $\sum_{e \in E} x_e
   = \alpha'(G)$

   We'll try to dualize this problem.

   In dual variable $y_v$, for all $v \in V$.

   Constrains $\forall e$, $\min \sum_{v \in V} y_v \le \beta(G)$.

   If $y \in \Z^n$, then $\le$ becomes $=$.

   For Bipartite graphs, we will show that the LP becomes IP. This and the
   duality leads to Konig's theorem
** Konig's theorem
   We can use Linear programming to show Konig's thoerem.
   
   If $G$ is bipartite, then $\alpha'(G) = \beta(G)$
** Plan
   1. Prove that $G$ is bipartite, then the optimal solution of $P$ are integers,
   hence $\max x_e = \alpha'(G)$.
   2. Optimal solution for $D$ (LP) is also integers, hence $\min y_v = \beta(G)$

   Now, using Duality theorem, $\alpha'(G) = \max x_e = \min y_v = \beta(G)$.
** Proposition
   For LP, max $c^{t} x$, subject to $Ax \le b$ and $x \ge 0$.

   This is from chapter 8. Section 2.
* Lecture 26
** Zero sum two player games
*** Payoff matrix
*** Strategy
*** Mixed strategy
*** Definition of $\beta$
*** Mixed Nash equilibrium
*** Remarks on mixed Nash Equlibrium
    1. They are the best possible responses against each other.
    2. Neither player can do better by switching strategies.
*** Question?
    1. Do we always have a mixed Nash equilibrium?
    2. Which one are the best? 
    3. How do we find them?
    4. For the rock paper scissors, the Nash equilibrium is $(1/3, 1/3, 1/3)$.
       The expected payoff will be $0$. But if we change, then the opponent will
       have a better strategy (provided they know about your distribution.)
* Lecture 27
** Helly's theorem
   Section 8.6

   He proved helly's theorem for 1 dimension.
* Lecture 28 <2019-02-05 Tue>
** Randomized algorithms
   Decision problems, problems with answers yes or no.

   Two types of randomized algorithms.

  | Las Vegas                       | Monte Carlo                 |
  |---------------------------------+-----------------------------|
  | "House always wins"             | Usually correct             |
  | Always correct                  | Always fast                 |
  | Usually fast                    |                             |
  | running time is random variable | solution is random variable |
** Monte Carlo: types of errors
   1. False positive (algorithm says yes, answer is no)
   2. False negative (algorithm says no, answer is yes)

   *Goal*: Bound the probability of an incorrect answer.

   Sometimes we can ask for algorithms with one-sided errors, for example, never
   give a false negative. In this case, we can repeat the test several times, if
   we ever see a NO, the answer is NO. If we only see Yes, return Yes. Notice
   that if the algorithm is repeated $k$ different times, then
   $P(\textup{error}) = P(\textup{false positive})^k \rightarrow 0$.
** Example (Matrix multiplication verification)
   Problem: Given two $n\times m$ matrices, $A, B$ our matrix multiplying.
   Machine claims $C = A \times B$.

   Question how can we verify this?

   If we compute $AB$ ourselves classically we need $\Omega(n^3)$ operations.
*** Randomized verification
    Pick a random entry $C$, compute what it should be. If the answers agree,
    then accept $C$, else reject. To compute a single entry it take linear time.
    
    No false negative.

    $P(\textup{false positive}) \le 1 - \frac{1}{n^2}$.
*** A more precise verification
    Question $AB = C$.

    Reformulate the question: is $AB - C = 0$.

    So the general question is the matrix $M = 0$.

    *Problem* we don't have access to the entries. Instead we can multiply $M$
    by vectors.

    $Mx = (AB - C)x = (AB)x - Cx$. We can compute $Cx$.

    $ABx = A(Bx)$ This requires quadratic operations.

    *Key observation* $M = 0 \iff Mx = 0 \forall x$.
*** Randomized algorithm
    1. Choose a random $x \in \{0, 1\}^n$
    2. Compute $Mx$ if $\neq 0$, return $M \neq 0$. If $= 0$, return $M = 0$.

    We never get a false negative.
*** Claim
    If $M \neq 0$, and $x \in \{0, 1\}^n$ uniformly random then $P(Mx = 0) \le
    \frac{1}{2}$, i.e., the probability of false positive is $\le \frac12$.
**** Proof
     Without loss of generality let us say that $M_{11} \neq 0$.

     Consider $(Mx) = \sum_{i=1}^{n} M_{1i} x_i = M_{11} x_1 + \sum_{i=2}^{n}
     M_{1i}x_i$.

     For every choice of $x_2, x_3, \cdots, x_n$. So the choice is zero for at
     most one choice of $x_1$.

     Therefore $P(Mx = 0) \le P((Mx)_1 = 0) \le \frac{1}{2}$.
*** About the algorithm
    Repeat the algorithm $k$ times and get that the probability of error $\le
    \frac{1}{2^k}$.
** Definition
   $\omega$ exponent for matrix multiplication, i.e., two matrices can be
   multiplied in $\theta(n^\omega)$ time.
** Bounds
   $2 \le \omega \le 2.3728629$.
** Conjecture
   $\omega = 2 + o(1)$.
* Lecture 29 <2019-02-07 Thu>
  Didn't have charge in my laptop. Did stuff about matching etc.
* Lecture 29 <2019-02-12 Tue>
** Hypergraphs
   A pair $(V, \mathscr{F})$, where $V$ is a set of vertices and $\mathscr{F}$
   is a family of subsets. $F \subset 2^V = \{X \subset V\}$ set family
   (elements are called Hyperedges)

   Hypergraph is $k$ uniform.

   If $\forall F \subset \mathscr{F}$, $\vert F \vert = k$.

   Example: $2$ uniform Hypergraph $\iff$ graph.

   A $3$ uniform hypergraph Fano Plane. $7$ vertices and $7$ hyperedges.

   #+BEGIN_SRC artist
                                           |
                                          /|\
                                         / | \
                                        /  |  \
                                       /   |   \
                                     /-----+----\-
                                    /-/    |    \/-
                                   //---   | /--- \\
                                  /(    \--+-      )\
                                 /  \/---  | \--- /  \
                                //----\    |    /\----\
                               ------------+----------\--
   #+END_SRC

   Finite Geometry. We talked about Hypergrpahs in Baranyai's theorem. A
   complete $k$ uniform on $n$ edges. This can be partitioned into a Perfect
   matching if $k \vert m$.

   Hypergraph $(v, \mathscr{F})$ is called $2$ colorable if $\exists c \colon V
   \rightarrow \{R, B\}$ that $\forall F \in \mathscr{F}$, $\vert c(F) \vert =
   2$
   
   It can be shown that the Fano plane is not $2$ colorable.
** Problem
   $m(k)$ is the minimum $\min \{ \vert \mathscr{F} \vert \textup{non 2
   colorable k uniform hypergraph \}$.

   $m(2) = 3$. For $2$ edges, it is definitely two colorable.

   $m(3) = 7$

   Geometric rate of $m(k)$ as $k \rightarrow \infty$.

   *Upper bound on m(k)*

   $\binom{[2k-1]}{k} = K^k_{2k-1}$ is not $2$ colorable. For any two coloring
   you color the vertices with $\{R, B\}$, the bigger color class has at least
   $k$ vertices. But in our hypergraph, every $k$ edge is present. Thus inside
   this edge there is a monochromatic hyperedge.

   $m(k) \le \binom{2k-1}{k}\approx 4^k$
** Lower bound
   If every $k$ uniform hypergraph with $f$ hyperedges can be $2$ colored.
** Erdos
   $m(k) \ge 2^{k-1}$
*** Proof
    Let $F$ be $k$ uniform and $\vert F \vert < 2^{k-1}$, we need to $2$ color
    $F$. 

    Strategy $2$ color $V$ randomly and prove that with non-zero probability
    that there is no monochromatic hyperedge. What we have shown is that there
    exists a proper $2$ coloring.

    Color each $v$ red with probability $\frac{1}{2}$ blue with the same
    probability. Choices of vertices are uniformly independent.

    For $F \in \mathscr{F}$, let $E_F$ be event that $F$ is m.c. $Pr(E_r) =
    \frac{2}{2^k} = \frac{1}{2^{k-1}}$.

    $P(\exists F \in \mathscr{F}, F \textup{is m.c.}) \le \sum P(E) <
    2^{k-1}\frac{1}{2^{k-1} = 1$ Weird proof.
*** Next winter
    In Extremal combinatorics, $\frac{2}{\log k} 2^s \le m(k) < k^2 2^k$.
** Algorithm
   From the Erdos's proof, we get a Monte Carlo algorithm. Then a random
   coloring succeeds in finding a proper $2$ coloring with a probability
   $\ge 1 - \frac{1}{2} \ge \frac12$.

   This means that we can repeat the algorithm several times to get a result. At
   the end we produce a proper $2$ coloring $\frac{1}{2^i}$.
** Goal
   *Derandomization* get rid of the randomization.
** Positional games
   A game Bridge-It
** Theorem
   Player 2 does not have a winning strategy.
*** Proof
    Suppose player $2$ has a winning strategy $S$. Player $1$ has access to this
    strategy, player 1 plays an arbitrary edge $e_1$, then follows $S$. Ignoring
    $e_1$. If $S$ calls for $e_1$, take another arbitrary $e_2$ etc. At the end,
    we claim that Player 1 wins since, he always owns the moves according to $S$
    plus one more. At the end you win because $S$ is a winning strategy. But
    player $2$ plays according to $S$, so they cannot win.

    The proof doesn't really use the game.

    This doesn't show that player 1 doesn't have a winning strategy. We need to
    show that the game cannot end in a draw.
** Winning strategy
   How to win?

   #+BEGIN_SRC artist

                            -+---------+--------+-------+-
                          -/ |         |        |       | \--
                       --/   |         |        |       |    \--
                    --/      |         |        |       |       \-
                  -/   ------+---------+--------+-------+-----    \--
             x --\\---/      |         |        |       |     \---------
                  -\--\      |         |        |       |            /--/- y
                    -\ ---\  |         |        |       |    /-------/--
                      -\   --+---------+--------+-------+----    /---
                        -\   |         |        |       |     /--
                          -\ |         |        |       |  /--
                            -+---------+--------+-------+--
   #+END_SRC

   $P_1$ and $P_2$ color edges red and blue. Player wins if and only if $x$ and
   $y$ are colored in the red group
** Theorem (Lehmann)
   If connected and disconnected occupying edges of a graph $G$ (alternatively)
   the connector is able to occupy a spanning tree of $G$ provided $G$ provided
   $G$ contains two spanning tree (edge-disjoint)
*** Proof
    We assume that $E(G) = E(T_1^{ * }) \cup E(T_2^{ * })$ in the beginning. In the
    algorithm, the connector maintains two spanning trees $T_1$ and $T_2$ such
    that the following properties hold.

    1. $E(T_1) \cap E(T_2)$ is the edges of the connector.
    2. $E(T_1) \triangle E(T_2)$ is the unclaimed edges.

    If succeeds then after $n-1$ moves $\vert E(T_1) \cap E(T_2) \vert = n -1$
    and $T_1 = T_2$ is a spanning tree occupied by connector.

    Start: $T_1 = T^{ * }$, $T_2 = T_2^{ * }$

    Now, after $i$ moves, we have $T_1$ and $T_2$. In the next move, the
    disconnector takes an edge $e \in E(T_1) \triangle E(T_2)$. Without loss of
    generality, $e \in E(T_1) \setminus E(T_2)$. This disconnects $T_1$ into two
    components $U_1$ and $U_2$. $T_2$ is connected, so $\exists$ an edge $e^{**}
    \in E(T_2)$ between $U_1$ and $U_2$. $e^{ *} \notin E(T_1)$ otherwise $U_1$
    will be connected in $U_2$ in $T_1 - e$.

    So connector takes $e^{*} \in E(T_2) \setminus E(T_1)$ and updates $T_2 =
    T_1$ and $T_1$ will be losing the edge $e$ and adding the edge $e^{ * }$.
* Tutorial
  [[http://discretemath.imp.fu-berlin.de/DMII-2018-19/][link]]
** Tutorial 1 (Sheet 1)
*** Problem 2
    Each step reduces the number of components by at most $4$. After $5$ steps, at least $5$ components are left.
** Tutorial 2 (Sheet 2)
*** SAT
**** Example of un-satisfiable instance of SAT
     $f(x_1) = x_1 \wedge \neg x_1$

     No matter what the instance is, this will evaluate to zero.
**** About $2^k$ clauses
     We start by proving that the statement is true for exactly $k$ variables.

     Now we induct on the number of variables, starting from $n$. If it is true
     for $m$, then it is also true for $m+1$ because we can replace the $m+1$th
     variable by $x_1$ and bang.
**** Bound being strict
     For $k$ literals, and for $2^k$, we take all possible combinations of $x_1,
     \cdots, x_k$ such that no two literals are the same. This is not
     satisfiable. (This should evaluate to $1$ all the time.) Because no matter
     what is the value of $x_1, \cdots, x_k$, there is a literal where the or is
     zero and that literal is present in it.
*** Problem 1 bipartite
    We do a BFS. We have an array and it would be the distance. Now the claim is
    that $A(u) = A(v) \mod 2$ and if there is an edge that connects these two
    vertices, then the graph is not bipartite.

    We did prove that for BFS, the distances from the root are preserved.

    The proof was a bit complex. But it turned out to be something about
    applying BFS.
*** Problem 3 Greedy algorithm can fail
**** part a
     Algorithm: Sort the edges according to the weight in ascending order. $E =
     \emptyset$.
     1. No vertex of degree $3$
     2. No cycle of length $<$ n.

     #+BEGIN_SRC artist
                                     1
                            +-------------------+
                            | \-             -/ |
                            |   \- 2     2 -/   |
                            |     \-    --/     |
                            |       \--/        |1
                            |1      -/ \-       |
                            |    --/     \-     |
                            |  -/          \-   |
                            |-/     1        \- |
                           -/------------------\+
     #+END_SRC

     Another algorithm: start with an edge, something like a lightest edge. It's
     vague.


**** part b
*** TODO Problem 2
    The first $m$ edges are the smallest weight forest.

    Induction: First edge is true. Assume it's true for $m-1$ edges are minimal
    weight forest. Now kruskal adds an edge (it is the edge with smallest
    weight). It does not make a cycle. We still have a forest. Now, the weight
    the new is smaller than or equal to every other $e_i$.

    Apparently it doesn't work.

    But we can do the induction backwards. Suppose that it is true for $n-1$
    upwards.

    $K_{m+1}$ is the forest that it construct in $m+1$ steps. (This doesn't work
    either.)

    Apparently, we could just repeat the proof for the Kruskal.

    The problem is that there could be an edge that we didn't add because it
    created a cycle before. This edge can create problems later.
**** TODO Go again through the argument of Kruskal
*** Exercise 4
**** Counter example
     Apparently any algorithm would fail on it because the shortest path does
     not make any sense.
     #+BEGIN_SRC artist
                                         x
                                         /-\
                                        /   \
                                       /     \
                                      /       -\
                                     /          \
                                  1 /            -\  1
                                    |              \
                                   /                -\
                                  /                   \
                                 /                     \
                                /        -2             -\
                              b---------------------------\c
     #+END_SRC
**** TODO Algorithm
     *Claim*: Right after $i$ th step of second part, $i$ th step of second
      part, $v \in V, \dist(v) \le \min \{ \vert w\vert \vert w \textup{
      contains } \le i \textup{ edges} \}$.

      1. For $i=1$, it is trivially true.
      2. For $i\ge 2$, $v \in V$, $w = $ shortest path $ \le i$ edges. There are
         parts of the argument that I skipped. [fn:17]

      It is not clear how the last part of the algorithm is able to detect the
      negative weighted cycle. The claim more or less does it.

      The time complexity is $O(mn)$
*** Exercise 5 (SAT)
**** Part a
     It's easy
**** Part b
     Write $f(x_1, \dots, x_m) = c_1 \wedge c_2 \cdots c_m$, $m < 2^k$.

     $c_i = \tilde{x_{i_1}} \and \tilde{x_{i_2}} \cdots$

     for $e$ or $e_i$, define the set $D_i = \{v\colon \{T, F\}^n \vert c_i(v) =
     false\}$. $\vert D_i \vert = 2^{n-k}$, $\sum D_i = \{v \vert f(v) = F\}$.[fn:18]
** Tutorial 3 (Sheet 3)
*** Problem 1
    We draw the graph, and remove the edges 2, 4, 8, 12. Now there are 6 odd
    number of components. There is a characterization about the maximal matching
    being the minimum of $\{n - o(G\setminus S) + \vert S\vert\colon \forall S
    \subset V(G)\}$. Now, this is $n-2$, which means if there is a matching with
    $n-2$, it will be maximal.

    Easy solution: Show that there is no maximal matching. The number of
    vertices is 18. There cannot be a matching on 17, because odd. Thus, the
    matching on 16 has to be the maximal matching.
*** Problem 2
**** part a
     It is clear that $\vert V \vert = \sum \textup{vertices in even components
     of G\S} + \sum \textup{vertices in odd components of }G\setminus S + \vert S \vert$

     The above sum $\mod 2$ evaluates to $\vert V \vert \mod 2= o(G\setminus S) +
     \vert S \vert$. This is same as $\vert S \vert - o(G \setminus S \mod 2$.
**** part b
     The idea is something like this.

     For each $S$ such that the number of odd components in the complement of
     $S$ is greater than $S$, introduce a $K_d$, where it's the clique on $d$
     vertices. What should be the value chosen for $d$? Each value of $d$ should
     be equal to the the difference for that set $S$.

     Now this should satisfy Tutte's theorem (maybe we can do this inductively
     as well.) There is a maximal matching. We need to show that in the maximal
     matching contains no edge in the clique. Now, we can arrive at a matching
     on the original graph $G$ and things should be okay?

     I've omitted details in the steps.
**** part b (alternate solution)
     Observation:
     1. For all even components, there is a matching
     2. For each vertex in the odd components, then $C_o \setminus \{v\}$ has a
        perfect matching.
     3. If $M$ is a maximal matching on $G[s]$.

     Proof
     1. Using Tutte's theorem: $S' \subset V(e)$, $\tilde S= S \cap S'$, then
        $\vert \tilde S\vert - o(G\setminus \tilde S)\ge \vert S \vert - o(G
        \setminus S)$ Now the LHS is equal to $\vert S \vert + \vert S'\vert -
        o(G\setminus S) - o(Ce\setminus S) \ge 0$, now we can infer that Tutte's
        theorem is satisfied inside the even components.
     2. $\vert S \vert + \vert S'' \vert - o(G \setminus S) - o(C_o \setminus
        S'') + 1$ (I missed some stuff here.)

*** Problem 3
**** Part a
     This is kinda similar to the proof of the theorem about $3$ regular graph
     without any cut edges having a perfect matching.

     First notice that every three regular graph must have even number of
     vertices.

     Given $S$ and $G\setminus S$, we think about the the degrees of elements
     inside any disconnected component of $G \setminus S$. They cannot be $0$
     because then, it means that there are three cut edges. They may be $1$ and
     they maybe $2$ at most $2$ times.

     If it is $2$ exactly 0 times, there are no cut edges and we are done (the
     idea is that the sum of the degrees of each vertex have to be even, but the
     degrees are either 1 or 3, so there has to be an even number of vertices.)

     If it is $2$ exactly 1 times, then there can be at most $1$ odd component,
     and $\vert S\vert > 0$, now, Tutte's theorem!

     If it is $2$ exactly $2$ times, then we know that $\vert S \vert -
     o(G\setminus S)$ has to have parity same as $\vert V\vert$, but this means
     that $\vert S\vert$ is a non-zero multiple of $2$, and $o(G \setminus S)$
     is exactly equal to $2$. Now, Tutte's theorem.
**** Part b
     The graph from the graph theory book
**** part c
     If $k$ is even, we can think about a clique on $k+1$ vertices.

     If $k$ is odd, then we can do a similar construction as $b$.
*** Problem 4
    Induction on the number of vertices of the tree.

    For $2$ vertices. It has a unique perfect matching.

    For $n \ge 2$, $n$ has to be even. There has to be at least one vertex that
    is a leaf (Handshake lemma?) we picked a leaf because the edge must be in
    the perfect matching. We delete the vertex right next to the edge. And on
    each of the even component, we can use the induction argument? We need to
    prove that if we delete a vertex inside the even component, there has to be
    exactly one component is odd.

    $\vert T' \vert = \vert T' \cap C \vert (\mod 2)$

    We can prove this. And apply induction hypothesis.
*** Problem 4 (Alternate proof)
    *Theorem*: In any tree there exist at most $1$ perfect matching. (the idea
     is that if there are two matching, then their symmetric difference must
     have a cycle.)
*** Problem 5
    Let $M$ be a maximal matching on $G$. Let $2\alpha$ be the number of
    vertices here. Let $2\alpha'$ be the number of vertices that gets connected
    in $2\alpha'$, we are supposed to show that $\alpha' > 1/2 \alpha$.

    Let the edges in the matching be $\{e_{i_1}, \cdots, e_{i_n}\}$. Where $i_1
    \le \cdots, i_n$. So when the algorithm is at the step $i_k$, either it gets
    added to $M_{t-1}$ or gets rejected. Let $x$ number of them get's rejected
    and $y$ number of them gets added.

    When does an edge gets rejected? If one of the vertex in $e_{i_k}$ is
    already in $M_{t}$. Let us now count the number of vertices in $M_t$ at the
    end.

    $M_t \ge x + 2y \ge x+y = (\alpha)$.

    Strictness: Consider the following graph and ordering of vertices:
    #+BEGIN_SRC artist
      o----------------o----------------o----------------o
             e2              e1                 e3
    #+END_SRC

    The maximal matching is $\{e2, e_3\}$, whereas the algorithm generates
    $\{e_1\}$.
** Tutorial 4 (Sheet 4?)
*** Problem 1
    The inequality is super easy and just follows from the definition.

    The proof is equality is also very easy. If no permutation satisfy the
    condition, then we can easily prove that there cannot be an equality.
*** Problem 2
    We can take the negative of all weights. And apply Hungarian algorithm.
*** TODO Problem 2
    I've not completed this.

    I was wondering if the values in the equality matrix can increase. In one of
    my iteration, the value increased.

    Also I messed up since the problem talks about minimal matching. I computed
    the maximal instead. So the first step should have been to negate the
    matrix.
*** Problem 3
    Step $i$ of the algorithm: let $M$ be the maximum matching in the equality
    graph $G_{u, v}$ and $\vert r \vert < n$ and let $Q$ be the vertex cover
    provided by the algorithm, then $T = Q \cap Y$ and $R = Q \cap X$ and
    $\varepsilon = \min \{u_i + v_j - w_{ij}\vert x_i \in X \setminus R, y \in Y
    \setminus T\}$

    Let $(\bar u, \bar v)$ be the weighted cover after calculating with
    $\varepsilon$.

    We need to show that $\vert X \setminus R \vert > \vert T \vert$. This is
    true, because if it is not true, then we have that $n = \vert R \vert +
    \vert X \setminus R \vert \le \vert R \vert + \vert T \vert = \vert Q \vert
    = \vert r \vert < n$.

    Indeed at each step, the cost function decreases by some number.

    Why does the algorithm terminate?

    $w_{ij} = \frac{p_{ij}}{q_{ij}}$, and $q = LCM(p_{ij}, q_{ij})$, then
    $\epsilon \ge 1/\varepsilon$, then we can always finish the algorithm.

    Or we can make all the weights as integers. The rational case follows
    easily.
*** Problem 4
    Questions:
    1. Is this the unique extremal examples? The answer is yes.
    2. What are the extremal examples?
    3. Hypergraphs: $k$ uniform Hypergraphs. We can define the notion of perfect
       matching in Hypergraphs. What is the maximum number? A paper in 2011
       solved it.
*** Problem 5
    Isn't this kinda simple? Like it is trivial when $n \ge k+2$, when $n= k+1$,
    the sum on the RHS evaluates to $k -1/2$, since $k$ is an integer,
    $\delta(G)$ has to be at least $k$.
*** Problem 5 (proof)
    What is the definition of $k$ connected?

    $\kappa(G) \ge k \iff G$ is $k$ connected.

    $G, n \ge k+1$, $\delta(G?) \ge \frac{n+k-2}{2}$. It is sufficient to show
    that if we subtract a set of size $k-1$, then the graph is connected.
    $G[V(G)\setminus S]$ are either neighbours (in this case it's trivial.)

    $u, v \in V(G')$.

    1. $u \in N(v)$, then we are done.
    2. $N(v) \cap N(u) \neq \emptyset$. Suppose $N_G'(v) \cap N_G'(u) = \emptyset$.
       $n+k-2 - 2(k-1) = n-k$, we should have so much vertices in $G'$ for the
       condition to be true. But $G'$ has $n-k+1$ vertices.
    3. Remark: $\delta \ge (n-1)/2$, then the graph is 1-connected or connected.
       Whereas, if $\delta \ge n/2$, then the graph has a hamiltonian cycle.
*** Problem 5 (Bonus)
    An example where the bound in sharp.
*** Problem 5 (my solution)
    I had a proof that counted the number or edges and arrived at a
    contradiction. I forgot how it worked. It was something like if the degree
    of each edge is $\frac{n+k-2}{2}$, then we have a lower bound on the number
    of edges because of handshake lemma. But the if we assume that there is a
    $k-1$ set that disconnects it, we can end up with an upper bound on the
    number of edges. Turns our that this upper bound is lower than the lower
    bound and hence the contradiction.
** Tutorial 5 (Sheet 4)
*** Problem 4
    We can do this in cases.

    1. $\kappa(G) = 3$, then because $\kappa \le \kappa' \le 3$, $\kappa' =
       \kappa = 3$.
    2. $\kappa = 1$, then disconnecting one vertex would disconnect the graph.
       Let $U_1$, $U_2$, $U_3$, be the at most three components that would
** Tutorial 6 (Sheet 5)
*** Problem 1
    It is easy to see that $\kappa = 2$. It is easy to see that $\kappa' \le 4$.
    We need to see how to ignore the other parts.

    We find two hamiltonian cycles and then two of the edges have to be on the
    hamiltonian cycle. And the third edge cannot really disconnect the graph.
    Remove the Hamiltonian cycles, then graph still has $\kappa' 2$. Thus the
    original graph has $\kappa' = 4$.

    *Hint*: Let $S$ be an edge cut of size $\le 3$. Then we need to show $S \ge
    5$, and $\vert \bar{S}\vert \ge 5$. Then the induced graph has to be $K_5$. But
    there is no $K_5$.
*** Problem 3
    $K_n$, $n$ is a natural number that is very large. There were two $K_n$ at
    both the end and in the center a graph of size $k$.
*** Problem 3
    1. $k=d=k'$, $K_{d+1}$ would do
    2. $k < d$ and $k'$ two $K_{d+1}$ graphs and do connections between them
*** Problem 4
    Something like if $\kappa' = r$, then there are $r$ edge disjoint paths. If
    $\kappa < \kappa'$, then there is a point in the edge disjoint path that
    doesn't blah blah.

    For proving for Peterson graph: there is a theorem by Brouwer that proves
    this.

    A simple way to do it is that remove a vertex and we get a graph that has a
    hamiltonian cycle.

    $\vert S, \vert{S} \vert = \sum d(v) - 2e(G[S])$[fn:34]
*** Problem 5
    You make a line graph and then argue that the $\kappa'$ of the graph is same
    as the $\kappa$ of the line graph. (There is an intermediate graph which
    should be formed by adding extra vertices $u$ and $v$, and the edge $ux$ and
    $yx$.)

    "Vertex transitive" graph. Any two vertices are the same in Peterson graph.

    Called the Harrary graph.
*** Problem 6
    $\val(f) = f^-(t) - f^{+}(t) = \sum_{u \in \bar{S}} (f^{-1}(w) - f^{+}(w)) =
    \sum_{u\in \bar{S}}(\sum_{v \in V} f(vu) - \sum_{v \in V} f(uv))$ some
    calculations and we are done.
** Tutorial 7 (Sheet 6)
*** Problem 1
    Given a bipartite Graph $G$ with $X$ and $Y$ be two partitions. Add two
    vertices $x$, $y$ and edges from $x$ to every element of $X$ and edges from
    $y$ to every element of $Y$.

    The theorem now follows from local Vertex Menger's theorem.

    How does one prove Local Vertex Menger from Local edge menger? The other
    side was an exercise.
*** Problem 1 (after LP)
    This is a consequence of duality. We already know that Ford Fulkerson is a
    consequence of duality. Not surprising.
*** Problem 2
    I think I proved this for the grade. I think this was from the book.
*** Problem 4
    $I$ is the set of all spanning trees and $J$ is the set of all edge cuts.
** TODO Tutorial 8 (Sheet 7)
*** Exercise 3
    $G$ be a graph with the property that every two odd cycles share a vertex.
    Then prove that the chromatic number of the graph is at most $5$

    Proof is from a math stackexchange answer

    Consider the smallest odd cycle of the graph. There are no chords in the odd
    cycle, since if there were, we could take a detour and make a smaller odd
    cycle. Thus we can color this cycle $C$ with $3$ colors.
    
    Consider the cycle $G \setminus C$. This graph has no odd cycles, this is
    because if $G \setminus C$ has an odd cycle, then it is also an odd cycle in
    $G$, but is disjoint from $C$. Thus $G \setminus C$ is bipartite. Thus we
    can color this with $2$ colors. Thus we can color the whole with thing with
    at most $5$ colors.

    Also I'm not completely sure about the framing of the question. Do they mean
    that every odd cycle intersect in exactly one point or at least one point? I
    guess the proof will work in both cases. There was a math stackexchange
    answer saying we can do even better by saying that the chromatic number is
    at most $4$, but it doesn't work when $K_5$ is used.
*** Exercise 4
    Chromatic number $k$ means that at least ${k \choose 2}$ edges.

    My proof to this was wrong. My mistake was that I ended up assuming that
    every coloring should come from a greedy coloring. Is this true? Probably
    not.

    [[https://math.stackexchange.com/questions/1135688/prove-that-every-k-chromatic-graph-has-size-m-geq-binom-k2][Math.SE]] question.

    Consider the colors $1, \cdots, k$. For each colors, there should be at
    least one edge from the color $i$ to $j$, where $i \le j$. If not, we could
    just remove one of the color class to get a $k-1$ coloring of the graph.

    This means that there are at least $0 + 1 + 2 + \cdots + k - 1 =
    \binom{k}{2}$ edges.
*** Exercise 5
    For triangle free graphs, the trivial upper bound $\chi(G) \le \Delta + 1$
    can be strengthened into

    $$\chi(G) \le \frac{3}{4} \left\lceil \frac{\Delta(G) + 1}{4}\right\rceil$$

    There is a solution [[http://www-sop.inria.fr/members/Frederic.Havet/Cours/coloration.pdf][This pdf]] proposition 8.16 pp. 119 (7 in pdf)

    *Proof* let $k = \left\lceil \frac{\Delta(G) + 1}{4}\right\rceil$, then let
     $V_1, \cdots, V_k$ be a partition of the vertices such that the sum of
     internal vertices for each $V_i$ is minimum (the minimum exists, although
     this computation may not be possible in polynomial time.)

     Now the idea is that each component $V_i$ can be colored with at most $3$
     colors. We first not that for the subgraph $V_i$, $\Delta \le 3$. This is
     because if there is a vertex $v$ in $V_1$ (WLOG) that has degree $4$ or
     more, then there must be another partition where it's degree is $3$, if
     not, there will be $4k \ge \Delta + 1$ edges.

     Now, from brook's theorem each $V_i$ can be colored in $\le 3$ colors,
     since there are no cliques of size $3$ and odd cycles can anyway be colored
     in $3$ colors.

     Thus each components can be colored in $3$ colors and hence the whole thing
     can be colored in $3k$ colors.
** Tutorial 9 (Sheet 8)
*** Exercise 1
    If $X(G) = 4$, then it has a subgraph $H$, that is $4$ critical. But then
    $\delta(H) \ge 3$. This is sort of a contradiction. There will always be a
    vertex such that $\delta(G) = 2$.
*** Exercise 2
    For odd, $X = 3$ and $X \le 3$. We are done.

    When $n$ is even. We can assume that all lists are the same.
*** Exercise 3
    $F - E + V = 2$ and $4F \le 2E$. Suppose $\delta(G) \ge 4$. $4V \le \sum
    d(v) = 2E$.

    Now, $2F \le E$.

    $2V \le E \implies V \le E/2$.

    $V = 2 + E - F \ge 2 + E - E/2 = 2 + E/2$. A contradiction.

    Now, do this by induction.

    Also the notion of $0$ free planar implies $3$ colorable. Some version of
    Brook's theorem.
*** Exercise 4
    Similar to how the 5 list colorability of graphs were proven in the class.
*** About exercise 4
    Prove that every outer plane has a vertex of degree $2$. Now one can repeat
    the above proof.

    Something about dual graph. Something about triangulation as well.
*** Mirzakhani graph (Problem 5)
    The idea is the following: we take a subgraph, which is the square with the
    center element. To color this with $4$ colors, at least two elements of the
    outer loop has to be the same. But two adjacent vertices can't have the same
    color, hence one diagonal has to be the same color.

    Let us, without loss of generality assume that $2$ and $4$ have the same
    color. The color has to be $1$ or $3$. Now, notice that $1$ and $3$ are
    diagonally opposite to $2$ and $4$. Regardless of what is the color for $2$
    and $4$, these diagonals, can't be same. Hence the other diagonal has to
    have the same color.

    For simplicity assume that the color chosen was $1$. Thus, there is a line
    $\bar{4}, \bar{3}, \bar{2}$ which needs to have the same color. And this
    color cannot be $1$, but $1$ is the only possibility.

    To show the part (b), let us say that the color of the infinite vertex is
    $5$, then $5$ cannot appear on the boundary of the first square. Now, it can
    be reduced to the problem (a), that this graph can't be list colored.

    The chromatic number cannot be $2$ because there is $K_3$ or a triangle in
    the graph.

    We color the outer sides with two colors $1, 2$. The interior points with
    color $3$, and the exterior point with $3$. This is now $3$ colorable. The
    chromatic number is $3$.
*** TODO New bonus
    If you have a triangle free planar graph, it's chromatic number is at most $3$.
*** Random stuff
    $C_{2n}$, $v_1, \cdots, v_{2n}$.

    $f_G = (x_1 -x_2)(x_2 - x_3) \cdots (x_{2n-1} - x_{2n})(x_{2n} - x_1)$.

    The list is a box $L_1 \times \cdots \times L_{2n} \subset \R^{2n}$.

    $G$ is list colorable if and only if $\exists x \in$ the box such that the
    polynomial does not vanish.

    The monomial $x_1x_2\cdots x_{2n}$ does not get cancelled out implies
    Combinatorial Constellation.
** Tutorial 10 (Sheet 9)
*** Problem 1
**** Part 1
     $C(x_i, y_j) = j - i (\mod n)$
**** Part 2
     $G$ be broken down into two components $X$ and $Y$

     We make a network by adding $s$ and $t$. Now from $X$, add $\Delta(G) -
     \deg x$ and from $X$ to $Y$ it should be $1$.

     Similarly for $\Delta - \deg y$ to the sink.

     Minimum capacity is $\sum \Delta - \deg x$.
**** Part 2
     We double the graph. The graph $\tilde{G}$, then $\Delta(\tilde G) =
     \Delta(G)$ and $\delta(\tilde{G}) = \delta(G) + 1$. We keep repeating the
     construction.

     Basically, we add edges between vertices that are not saturated.

     This would lead to an exponentially large graph.
**** Bonus
     What is the best bound on size of the new graph.
*** Problem 3
** Tutorial 11 (Sheet 10)
*** TODO Exercise 1
*** Exercise 2
    Preference

    |   | 1 | 2 | 3 | 4 |
    |---+---+---+---+---|
    | 1 | x | 2 | 1 | 3 |
    | 2 | 3 | x | 2 | 1 |
    | 3 | 3 | 2 | x | 1 |
    | 4 | 2 | 1 | 3 | x |

    | Matching    | Unstable pair | Reason           |
    |-------------+---------------+------------------|
    | (1,2) (3,4) | (2,3)         | 34 < 32; 21 < 23 |
    | (1,3) (2,4) | (4,3)         | 24 < 21; 13 < 12 |
    | (1,4) (2,3) | (4,2)         | 23 < 24; 41 < 42 |


    $ab < ac$ means a prefers $c$ over $a$. Here the numbers means ranking,

    smaller rank is preferred over a larger rank.

    |   | 1 | 2 | 3 | 4 |
    |---+---+---+---+---|
    | 1 | x | 1 | 2 | 3 |
    | 2 | 2 | x | 1 | 3 |
    | 3 | 1 | 2 | x | 3 |
    | 4 | 1 | 2 | 3 | x |

    | Matching    | Unstable pair | Reason                                 |
    |-------------+---------------+----------------------------------------|
    | (1,2) (3,4) | (2,3)         | 2 prefers 3 over 1; 3 prefers 2 over 4 |
    | (1,3) (2,4) | (2,1)         | 2 prefers 1 over 4; 1 prefers 2 over 3 |
    | (1,4) (2,3) | (1, 3)        | 1 prefers 3 over 4; 3 prefers 1 over 2 |





*** Exercise 3
    The weights be $w_1, \cdots, w_n$ and for each student, $S_i$, let $e_{1,i},
    \cdots, e_{12,i}$ be their grade in a homework, now the average for the
    student is $w_1e_{1, i} + \cdots + w_ne_{n, i}$ and $0 \le w_i \le 1$.

    For the students, that he wants to fail, it'll be $w_1e_{1, i} + \cdots +
    w_ne_{n, i} \le 12$, and for the ones he has to pass, $w_1e_{1, i} +
    \cdots + w_ne_{n, i}$. We also have a constraint $w_1 + \cdots + w_n$.

    We don't really have to minimize a cost function here, or do we? We could
    just say $\min w_1$, just to make it an LP. But in fact, all we need to know
    is if there is a solution inside the feasible region.


    There might be another way to formulate the problem.

    The cost function will be the sum of $12 - (w_1 e_{1, i} + \cdots ) + (w_1
    e_{1, j} + \cdots + w_{12} e_{12, j})$. We maximize this, under the
    constraint $w_1 + \cdots + w_n = 1$. and $0 \le w_i \le 1$.

    This would ensure that the most number of people the teacher hate will fail
    (kinda) and the maximum number of people that the teacher like will pass.
*** TODO Exercise 4
    This is an example from the book.
*** Exercise 5
    What does it mean by decision version of a problem?

    Given a IP with $n$ variables and $m$ equations, (Can we assume $n \ge m$)
    given value $x$, is there a solution such that $\max c^t x \le k$?

    Here $k$ should also be part of the problem. The decision version is clearly
    in NP. This is because if we know such a solution exist, you can verify it
    in polynomial time.
**** Reduction of 3 sat to Decision-IP
     If there is a clause like $(x_1 \wedge x_2 \wedge x_3)$, then we can form
     the equation $x_1 + x_2 + x_3 \ge 1$. Similarly for each of the clauses.

     Also add extra clauses $x_1 + \bar{x_1} = 1$ for each symbol. Here $x_i \in \{0, 1\}$

     What should be the maximization or minimization problem?

     We know that it is decidable if there exists a solution to the problem. So
     we can kinda pick an arbitrary, equation to maximize or minimize.
**** Decision version
     I saw this question in CS Stackexchange about what is the decision version
     of the IP. One answer said the decision version of any problem is that "is
     there a solution", I found it a bit weird because, in our case, it would
     just mean whether the feasible region is empty or not, and we're not
     concerned about maximizing or minimizing a function.

     But for the reduction for $3$ sat to Decision IP, we just care about this,
     again.
*** TODO Exercise 6
    $x_i$ should correspond to vertices.

    $y_{ij}$ should correspond to edges, but it is not known if an edge actually
    exist.

    The value of $x_k$ means if a vertex is picked or not.
** Tutorial 12 (Sheet 11)
*** Problem 1
    The following code does the calculation

    #+BEGIN_SRC R :results output
      x1 <- matrix(c(1, 2), nrow=2)
      x2 <- matrix(c(-1, 2), nrow=2)
      x3 <- matrix(c(-3, 1), nrow=2)
      x4 <- matrix(c(1, -2), nrow=2)

      b <- matrix(c(5, 4), nrow=2)

      ls <- list(x1, x2, x3, x4)

      for(i in 1:3)
      {
          for(j in (i+1):4)
          {
              print(c(i, j))
              xb <- tryCatch({solve(matrix(c(ls[[i]], ls[[j]]), nrow=2), b)},
                             error=function(e){return(c(NaN, NaN))})

              print(xb)
          }
      }

      print(matrix(c(x1, x2, x3, x4), nrow=2))
    #+END_SRC

    #+RESULTS:
    #+begin_example
    [1] 1 2
         [,1]
    [1,]  3.5
    [2,] -1.5
    [1] 1 3
               [,1]
    [1,]  2.4285714
    [2,] -0.8571429
    [1] 1 4
         [,1]
    [1,]  3.5
    [2,]  1.5
    [1] 2 3
         [,1]
    [1,]  3.4
    [2,] -2.8
    [1] 2 4
    [1] NaN NaN
    [1] 3 4
         [,1]
    [1,] -2.8
    [2,] -3.4
         [,1] [,2] [,3] [,4]
    [1,]    1   -1   -3    1
    [2,]    2    2    1   -2
    #+end_example

    There is only one feasible basis $(1, 4)$ and $x_B = (3.5, 1.5)$. The
    maximum value is $3.5 + 1.5 = 5$.
*** Problem 2
    #+BEGIN_SRC R :results output :session problem2
      x2 <- matrix(c(-2, 1), nrow=2)
      y1 <- matrix(c(1, 0), nrow=2)
      y2 <- matrix(c(0, 1), nrow=2)
      y3 <- matrix(c(2, 2), nrow=2)
      y4 <- matrix(c(-2, -2), nrow=2)

      ls <- list(x2, y1, y2, y3, y4)
      b <- matrix(c(-2, 2), nrow=2)

      for(i in 1:4)
      {
          for(j in (i + 1):5)
          {
              print(c(i, j))
              xb <- tryCatch({solve(matrix(c(ls[[i]], ls[[j]]), nrow=2), b)},
                             error=function(e){return(c(NaN, NaN))})
              print(xb)
          }
      }
    #+END_SRC

    #+RESULTS:
    #+begin_example
    [1] 1 2
         [,1]
    [1,]    2
    [2,]    2
    [1] 1 3
         [,1]
    [1,]    1
    [2,]    1
    [1] 1 4
              [,1]
    [1,] 1.3333333
    [2,] 0.3333333
    [1] 1 5
               [,1]
    [1,]  1.3333333
    [2,] -0.3333333
    [1] 2 3
         [,1]
    [1,]   -2
    [2,]    2
    [1] 2 4
         [,1]
    [1,]   -4
    [2,]    1
    [1] 2 5
         [,1]
    [1,]   -4
    [2,]   -1
    [1] 3 4
         [,1]
    [1,]    4
    [2,]   -1
    [1] 3 5
         [,1]
    [1,]    4
    [2,]    1
    [1] 4 5
    [1] NaN NaN
    #+end_example
    
    Simplex, $A_B^{-1}A_N$

    #+BEGIN_SRC R :results output :session problem2
      AB <- matrix(c(-2, 1, 1, 0), nrow = 2)

      AN <- matrix(c(0, 1, 2, 2, -2, -2), nrow = 2)

      xb <- matrix(c(2, 2), nrow=1)

      print(solve(AB) %*% AN)

      cb <- matrix(c(2, 0), nrow=2)

      ## Calculation of r
      matrix(c(0, 1, -1), nrow=3) -  t(t(cb) %*% solve(AB) %*% AN)
    #+END_SRC

    #+RESULTS:
    :      [,1] [,2] [,3]
    : [1,]    1    2   -2
    : [2,]    2    6   -6
    :      [,1]
    : [1,]   -2
    : [2,]   -3
    : [3,]    3
    
    After a pivot we'll see that the problem is unbounded. This can be seen by
    plugging in $x_1 = t$ and $x_2 = t +1$ and $t \rightarrow \infty$.
*** Problem 3
    #+BEGIN_SRC R :session problem3
    cn <- matrix(c(46, 43, -271, 8))
    #+END_SRC

    #+RESULTS:
    |   46 |
    |   43 |
    | -271 |
    |    8 |

    Basis z1, z2
    |  x1 |  x2 |  x3 | x4 | z1 | z2 | b |
    |-----+-----+-----+----+----+----+---|
    |   2 |   1 |  -7 | -1 |  1 |  0 | 0 |
    | -39 |  -7 |  39 |  2 |  0 |  1 | 0 |
    |-----+-----+-----+----+----+----+---|
    | -46 | -43 | 271 | -8 |  0 |  0 | 0 |

    Basis x1, z2
    | x1 |    x2 |   x3 |   x4 |   z1 | z2 | b |
    |----+-------+------+------+------+----+---|
    |  1 |   0.5 | -3.5 | -0.5 |  0.5 |  0 | 0 |
    |  0 | -12.5 | 19.5 | 21.5 | 19.5 |  1 | 0 |
    |  0 |   -20 |  110 |  -15 |   23 | 0  | 0 |


    
*** Problem 4
    This is a section from the book. I also solved this for the tutorial
*** Problem 5
    From the last equation it is clear that $x_n \le 5^n$. 

    The last equation can be rewritten as $2(obj) \le x^n + 5^n$. Thus the
    objective value is bounded above by $5^n$. Clearly $(0, \cdot, 5^n)$ is an
    optimal solution.
** Tutorial 13 (Sheet 12)
*** Problem 1
    #+BEGIN_SRC R :session problem1 :results output
      x1 <- matrix(c(15, -1, -3), nrow=3)
      x2 <- matrix(c(-1, 1, -1), nrow=3)
      x3 <- matrix(c(-1, 0, 0), nrow=3)
      x4 <- matrix(c(0, -1, 0), nrow=3)
      x5 <- matrix(c(0, 0, -1), nrow=3)

      ls <- list(x1, x2, x3, x4, x5)
      b <- matrix(c(4, 100, 120), nrow=3)

      for(i in 1:3)
      {
          for(j in (i+1):4)
          {
              for(k in (j+1):5)
              {
                  xb <- solve(matrix(c(ls[[i]], ls[[j]], ls[[k]]), nrow=3), b)
                  if(all(xb >= 0))
                  {
                      print(c(i, j, k))
                      print(xb)
                  }
              }
          }
      }
    #+END_SRC

    #+RESULTS:

    Turns out that the feasible region of the problem is empty.

    Thus the original problem must be unbounded.
*** Problem 2
    Optimum Optimum. Primal: maximize $x$ subject to $x \le 1$ and $x \ge 0$.
    Dual: minimize $y$ subject to $y \ge 1$ and $y \ge 0$.

    Unbounded, infeasible. Primal: maximize $x$ subject to $-x \le 1$ and $x \ge
    0$. Dual: minimize $x$ subject to $-x \ge 1$ and $x \ge 0$.

    Infeasible, unbounded. Use the above problem again.

    Infeasible infeasible. Primal maximize $x + y$ subject to $-x \le 1$ and $y
    \le -1$ and $x, y \ge 0$ Dual: minimize $x - y$ subject to $-z \ge 1$, $a
    \ge 1$ and $z, a \ge 0$.
*** Problem 4
    I solved this for the grade.

    One can solve this directly exactly as we proved the theorem for optimal
    solution. One can also solve it using the fact that basic feasible solution
    of a linear program is a vertex of the polyhedra. So there exists a linear
    program which has a unique optimum equal to the basic feasible solution, but
    we know that unique optimal solution has integer coordinates. Thus, the
    basic feasible solution has integer coordinates.
*** Exercise 5
    The primal solution is the following:

    maximize $\sum e_i - sum \tilde{e}_i$. Where $e_i$ is the set of vertices
    entering $t$ and $\tilde{e}_i$ is the set of vertices that leave $t$.

    Subject to: $\sum e_i - \sum \tilde{e}_i = 0$ for each vertex $v$ that is
    not $s$ or $t$. $e_i$ and $\tide{e}_i$ have the same meaning. This is the
    conservation constraint.

    Also subject to $e_i \le f(e_i)$ (this is the capacity constraint; some
    abuse of notation)

    And $e_i \ge 0$.

    The dual of the program is the following:

    $\minimize \sum f(e_i) e_i$ subject to

    $x_j + e_i - x_j \ge 0$ where $e_i \ge 0$ and $x_i \in \R$.

    Meaning of the variables. $e_j$ should denote a value that means that it is
    an edge between the cut. $e_j$ should be $1$ if it is between the cut. Now
    the value of $x_j$ means that it should be $1$ if it in the cut with $s$.
    Now if $x_j$ and $x_i$ have the same value, $e_j$ can be anything, either
    $0$ or $1$, but then, because of the minimization problem $e_j$ will be
    zero hence the equations represent the problem for min-cut.
    
    *About the matrix being totally unimodular*.
    
    Notice that we don't need to consider the bottom $I_n$ matrix, because of
    the lemma that adding a canonical basis won't change the unimodularity.
    
    We prove this by induction of $\vert V \Vert$. For $\vert V \vert = 1$, we
    are done. Let us assume that it is true till $\vert V \vert \le n$. For
    $n+1$, we think about the matrix in terms of the columns.

    Suppose there is a column with all zeros, we are done. Suppose there is a
    column with exactly one non-zero, we are again done.

    Notice that each column can have at most two nonzeros. So the only case left
    if the case where every column has exactly two nonzeros. On the other hand,
    there can only be at most one $1$ and at most one $-1$. In this case the
    matrix is singular since we can just add all the rows and get the row $(0,
    0, \cdots, 0)$ Thus it is unimodular in every case. By induction, we are
    done.
*** Exercise 6
    Original problem: $Ax \le c$ subject to $x \ge 0$.
    
    Given a arbitrary linear program, construct another linear program $P'$ such
    that $P$ has an optimal solution if and only if $P'$ has a feasible solution.

    Our new optimal solution is the following:
    
    Minimize (doesn't matter)
    
    Subject to 
    
    | A^t |  0 | . | y | >= | c  |
    |   0 | -A | . | x | >= | -b |

    greater than or equal to

    and $y \ge 0$, $x \ge 0$.

    *Proof* Notice that the dual of the original program is $\min c^t y$
    subject to $A^t \ge c$ and $y \ge 0$. The dual of this program is the
    original program.

    If the program is optimal, both the dual and the dual of the dual is optimal
    and hence also feasible. Thus $A^t y \ge c$ and $-A^tx \ge -b$. Thus the new
    program is feasible.

    If the new program is feasible, this means that both the dual of the
    original program and the original program is feasible. Assume that the
    program is not optimal, then it has to be either unbounded or infeasible. It
    cannot be unbounded because the dual of the program is feasible. It cannot
    be infeasible, because we already know it is feasible. Thus the problem has
    to have an optimal solution.
** Tutorial 14 (Sheet 13)
*** Problem 1
    | F\D     | Rock | Paper |
    |---------+------+-------|
    | Rock    |    0 |    -1 |
    | Paper   |    1 |     0 |
    | Scissor |   -1 |     1 |

    When they tell me to analyse the strategy, I'm not sure what exactly should
    be done.

    For the father, the row with paper has no losses. The father probably
    shouldn't pick Rock, Scissor is even. For the Daughter both the columns
    seems to do okay on average, but she should pick paper, since the father is
    more likely to pick paper.
*** Problem 2
    | A\B |  1 |  2 | 3 |
    |-----+----+----+---|
    |   1 |  1 | -1 | 1 |
    |   2 | -1 |  1 | 1 |
    |   3 |  1 | -1 | 1 |

    Let $(x_1, x_2, x_3)$ and $(y_1, y_2, y_3)$ be probabilities, then $x^t M y
    = (1-2y_2)(1-2x_2)$.

    $\beta(x) = \min_{y} x^{t} M y$ and $\alpha(y) = \max_{x} x^{t} M y$.

    $\beta(x)$ is if $1 - 2x_2 > 0$, $-(1-2x_2)$ for $y_2 = 1$ and if $1 - 2x_2
    < 0$ is $-1(1-2x_2)$ for $y_2 = -1$ and is zero when $1-2x_2 = 0$

    $\alpha(y)$ is if $1-2y_2 > 0$ then $1-2y_2$ for $x_2 = 0$ and if $1-2y_2 <
    0$ is $-(1-2y_2)$ for $x_2= 1$.

    The mixed equilibrium should be for $x_2 = y_2 = 1/2$ and the choice of
    $x_1$ and $x_2$ are the same.

    The value of the game should be $0$.

    *Problem 2* If B plays $(1/3, 1/3, 1/3)$, then $x^{t} M y^{t} =
    (1-2x_2)(1/3)$ For $x_2 = 0$, this becomes $1/3$. It jumps from $0$ to
    $1/3$.

    *Problem 3* It seems that the number of ways to win doesn't really matter
    in the general case. This is just the intuition.

    So if B is playing uniformly, then there is a way for A to make profit.

    $x^t M x$ will turn out to be $6x_1 -12x_2 + 6x_3$. Now, $(1, 0, 0)$ will be
    a good strategy for A. So A should accept it.
*** Problem 5
    I only verified the first one. I hope the rest is true.

    We can get the following relation. If $T(k)$ denotes the number of
    operations. Then there are a total of $18$ addition and $7$ multiplications
    of size $T(k-1)$.

    $T(k) = 7T(k-1) + 18 2^{k-1}$ and $T(0) = 1$.

    Now, $T(k) = 7^k + 18/13 (14^k - 1)$.

    For general matrices, we can make it a matrix on $2^k$ by padding it with
    zeros. It is similar to how we compute the complexity of the merge sort.
* Footnotes

[fn:38] We've not done the full proof of this claim, I think.

[fn:37] Now, it should follow from the greedy vertex coloring of the graph,
    provided we prove that $\Delta$ of the line graph is same as the original
    graph.

[fn:36] I don't understand this. Shouldn't we multiply this by some factorial?
*Answer* the key is pairwise disjoint.

[fn:35] Repetition is allowed only for $\emptyset$. Because sets are disjoint.

[fn:34] If there is an edge cut of size $2$, then there is a triangle in the
peterson graph.

[fn:33] An $f$ augmenting path by definition means from source to sink. Our
definition here means the same definition except that the end vertex need not be
the sink.

[fn:32] I think there is a typo in the last $\sigma$ part.

[fn:31] This can be generalized in a very straightforward way.

[fn:30] It is true that for $k$ connectedness, any $k$ vertices are on a cycle,
but the other direction is not true.

[fn:29] We use the convention that the prime is the corresponding to edges.

[fn:28] In a good connected graph, you don't want a vertex set to be small.

[fn:27] Practical example: There is a company which has factories and corn
farms, the edges represent the profit the factory makes by producing corn from a
certain farm. There is more to this story. But I missed it.

[fn:26] I missed a lot of details here.

[fn:25] Apparently the problem for general graph is also in P. The algorithm for
this graph was what lead to the definition of $P$.

[fn:24] If the graph satisfies tutt's condition, then $\alpha$ should evaluate
to $n/2$.

[fn:23] What is a handshake lemma?

[fn:22] An example of such a graph is Peterson graph.

[fn:21] What is a cut edge? A cut edge should mean that you remove an edge and
the graph gets disconnected.

[fn:20] What is regular graph? Every vertex has the same number of neighbours.

[fn:19] Is the $K_2$ here an edge?

[fn:18] A lower bound for Ramsey numbers was also proved in same way. Also you
can do a probabilistic argument.

[fn:17] Apparently Bellman-ford is an algorithm that is better than Djistra when
it comes to negative edges. But it works for digraphs and not for directed
graphs.

[fn:16] I think I missed some parts to the explanation.

[fn:15] Here $xy$ is an edge that is not already in $G$.

[fn:14] Why can we do this? We fix the number of vertices, so this it actually
makes sense.

[fn:13] [[https://en.wikipedia.org/wiki/Tutte_theorem][Tutte's theorem]]

[fn:12] My condition

[fn:11] Here $o$ is the number of components of odd size.

[fn:10] Peterson graphs can be used to make a lot of counter examples. This was
taught in discrete math 1.

[fn:9] Here $C$ is the number of connected components in the graph.

[fn:8] Apparently there is a characterization that a graph is 2 colorable if and
only if it has no cycle.

[fn:7] I think it's about going through each edge once.

[fn:6] The belief is that this is not true. This is one of the Millennium problems.

[fn:5] Input is a graph $G$ and the question is whether there is a perfect matching.

[fn:4] In general we don't know how to approximate the TSP, but we can do it with some extra conditions

[fn:3] "and the decision problem version ("given the costs and a number x, decide
whether there is a round-trip route cheaper than x") is NP-complete."-Wikipedia

[fn:2] Otherwise you can explore the components.

[fn:1] MST would be city-side and the fastest possible way would be consumer side.

* TODO Things to study
** TODO Proof of Kruskal
   There was also an exercise talking about how the first $n$ edges of the
   kruskal algorithm forms a minimum weight forest. Anurag said this was a
   tricky problem.
*** TODO Problem in proof about complexity
    In the second step, there is this argument on if $C_u = C_w$ takes $O(n^2)$.
    Shouldn't it be $O(mn)$?
** TODO Proof of lower bound for sorting
** TODO What is the lightest network that remains connected after the breakdown of any edge?
   What does lightest mean here? 

   In a clique, you really have to remove a lot of edges to make it disconnected.

   Perhaps, he's talking about a graph on $n$ vertices with minimum number of
   edges that will remain connected, even after removing any edge from it.
** TODO Proof of Hall's theorem
   He didn't do this in current course. But was done in the previous course.
   West has a proof.
** TODO Finish learning the proof of Tutte's theorem
** TODO Minimum number of edges in a k connected graph?
** TODO Study Baryani theorem
** TODO For edge coloring consider when we are working with multigraphs
   And when we are not.

   For example, for Vizing's theorem, we are working with simple graphs.
** TODO Learn some properties of Peterson graph
** TODO Matchings
** TODO I don't understand the deal about M augmenting path and maximal matchings
   
   #+BEGIN_SRC artist
                      +-----
                      |     \---------
                      |               \---------
                      |                         X-------------------
                      |                  /------
                      |           /------
                      |    /------
                      |----
   #+END_SRC

   Consider the above graph, A triangle and a protruding edge. Consider my
   matching which only consists of the protruding edge, it is clearly not a
   maximal matching, but I can't seem to find any m augmenting paths
** TODO Study the proof of Hungarian algorithm
   One can model this using linear programming, the argument for the matrix $A$
   being unimodular follows from a theorem similar to
